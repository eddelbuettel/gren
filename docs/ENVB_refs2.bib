
@phdthesis{beal_variational_2003,
	address = {London},
	title = {Variational algorithms for approximate {Bayesian} inference},
	url = {https://www.researchgate.net/publication/34000771_Variational_algorithms_for_approximate_Bayesian_inference},
	abstract = {Thesis (Ph.D.)--University of London, 2003.},
	urldate = {2016-09-26},
	school = {University College},
	author = {Beal, Matthew James},
	month = jan,
	year = {2003},
	file = {Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\CAIHJJFB\\34000771_Variational_algorithms_for_approximate_Bayesian_inference.html:text/html;VariationalAlgorithmsApproximateBayesianInference.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\VQC6GFJG\\VariationalAlgorithmsApproximateBayesianInference.pdf:application/pdf}
}

@article{beal_variational_2003-1,
	title = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}: with {Application} to {Scoring} {Graphical} {Model} {Structures}},
	volume = {7},
	shorttitle = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}},
	journal = {Bayesian Statistics},
	author = {Beal, MJ and Ghahramani, Z},
	year = {2003},
	file = {Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\CUH5QCHC\\224881759_The_Variational_Bayesian_EM_Algorithm_for_Incomplete_Data_with_Application_to_Scoring.html:text/html;valencia02.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\74JX9H8E\\valencia02.pdf:application/pdf}
}

@article{bergersen_weighted_2011,
	title = {Weighted {Lasso} with {Data} {Integration}},
	volume = {10},
	issn = {1544-6115},
	url = {http://www.degruyter.com/view/j/sagmb.2011.10.issue-1/sagmb.2011.10.1.1703/sagmb.2011.10.1.1703.xml},
	doi = {10.2202/1544-6115.1703},
	number = {1},
	urldate = {2015-09-14},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Bergersen, Linn Cecilie and Glad, Ingrid K. and Lyng, Heidi},
	month = jan,
	year = {2011},
	pages = {1--29},
	file = {WeightedLassoDataIntegration.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\WeightedLassoDataIntegration.pdf:application/pdf;WeightedLassoDataIntegration.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\MBI5ZXCI\\WeightedLassoDataIntegration.pdf:application/pdf;WeightedLassoDataIntegration.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\THXSADP8\\WeightedLassoDataIntegration.pdf:application/pdf}
}

@article{biane_probability_2001,
	title = {Probability laws related to the {Jacobi} theta and {Riemann} zeta functions, and {Brownian} excursions},
	volume = {38},
	issn = {0273-0979, 1088-9485},
	url = {http://www.ams.org/bull/2001-38-04/S0273-0979-01-00912-0/},
	doi = {10.1090/S0273-0979-01-00912-0},
	abstract = {Abstract:},
	number = {4},
	urldate = {2016-10-07},
	journal = {Bull. Amer. Math. Soc.},
	author = {Biane, Philippe and Pitman, Jim and Yor, Marc},
	year = {2001},
	keywords = {Bessel process, functional equation, Infinitely divisible laws, sums of independent exponential variables},
	pages = {435--465},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\E3DHE52G\\Biane et al. - 2001 - Probability laws related to the Jacobi theta and R.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\IQPCE2QJ\\S0273-0979-01-00912-0.html:text/html}
}

@article{blei_variational_2016,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior distribution. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	urldate = {2016-09-27},
	journal = {arXiv:1601.00670 [cs, stat]},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1601.00670 PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\JPZ2BADG\\Blei et al. - 2016 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\Z9PF9AIP\\1601.html:text/html}
}

@article{carbonetto_scalable_2012,
	title = {Scalable {Variational} {Inference} for {Bayesian} {Variable} {Selection} in {Regression}, and {Its} {Accuracy} in {Genetic} {Association} {Studies}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {http://projecteuclid.org/euclid.ba/1339616726},
	doi = {10.1214/12-BA703},
	abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities{\textbar}including posterior distributions of the hyperparameters{\textbar}are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables.},
	language = {EN},
	number = {1},
	urldate = {2016-09-27},
	journal = {Bayesian Anal.},
	author = {Carbonetto, Peter and Stephens, Matthew},
	month = mar,
	year = {2012},
	mrnumber = {MR2896713},
	keywords = {genetic association studies, Monte Carlo, Variable selection, variational inference},
	pages = {73--108},
	file = {euclid.ba.1339616726.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\NWEDS26P\\euclid.ba.1339616726.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\JAQG6PR2\\1339616726.html:text/html}
}

@article{casella_empirical_2001,
	title = {Empirical {Bayes} {Gibbs} sampling},
	volume = {2},
	number = {4},
	journal = {Biostatistics},
	author = {Casella, George},
	year = {2001},
	pages = {485--500},
	file = {Biostat-2001-Casella-485-500.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\38IRWHCP\\Biostat-2001-Casella-485-500.pdf:application/pdf}
}

@article{chib_marginal_1995,
	title = {Marginal {Likelihood} from the {Gibbs} {Output}},
	volume = {90},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2291521},
	doi = {10.2307/2291521},
	abstract = {In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.},
	number = {432},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Chib, Siddhartha},
	year = {1995},
	pages = {1313--1321},
	file = {JSTOR Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\FVFJICM7\\Chib - 1995 - Marginal Likelihood from the Gibbs Output.pdf:application/pdf}
}

@article{choi_polya-gamma_2013,
	title = {The {Polya}-{Gamma} {Gibbs} sampler for {Bayesian} logistic regression is uniformly ergodic},
	volume = {7},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1377005819},
	doi = {10.1214/13-EJS837},
	abstract = {One of the most widely used data augmentation algorithms is Albert and Chib’s (1993) algorithm for Bayesian probit regression. Polson, Scott, and Windle (2013) recently introduced an analogous algorithm for Bayesian logistic regression. The main difference between the two is that Albert and Chib’s (1993) truncated normals are replaced by so-called Polya-Gamma random variables. In this note, we establish that the Markov chain underlying Polson, Scott, and Windle’s (2013) algorithm is uniformly ergodic. This theoretical result has important practical benefits. In particular, it guarantees the existence of central limit theorems that can be used to make an informed decision about how long the simulation should be run.},
	language = {EN},
	urldate = {2016-10-19},
	journal = {Electron. J. Statist.},
	author = {Choi, Hee Min and Hobert, James P.},
	year = {2013},
	mrnumber = {MR3091616},
	keywords = {data augmentation algorithm, Markov chain, minorization condition, Monte Carlo, Polya-Gamma distribution},
	pages = {2054--2064},
	file = {euclid.ejs.1377005819.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\GXHX58IR\\euclid.ejs.1377005819.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\8MI6UI5B\\1377005819.html:text/html}
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	pages = {1--38},
	file = {JSTOR Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\2NBCF56N\\Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf:application/pdf}
}

@inproceedings{fan_statistical_2007,
	title = {Statistical challenges with high dimensionality: feature selection in knowledge discovery},
	volume = {III},
	shorttitle = {Statistical challenges with high dimensionality},
	booktitle = {Proceedings of the {International} {Congress} of {Mathematicians} {Madrid}, {August} 22-30, 2006},
	author = {Fan, J and Li, R},
	editor = {Sanz Solé, Marta and Soria, Javier and Varona, Juan Luis and Verdera, Joan},
	year = {2007},
	keywords = {dimensionality, high, learning, machine},
	pages = {595--622},
	file = {StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\E92B4WS6\\StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:application/pdf}
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and its {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
	doi = {10.1198/016214501753382273},
	language = {en},
	number = {456},
	urldate = {2015-09-14},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	month = dec,
	year = {2001},
	pages = {1348--1360},
	file = {VariableSelectionNonconcavePenalizedLLOracleProp.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\VariableSelectionNonconcavePenalizedLLOracleProp.pdf:application/pdf;VariableSelectionNonconcavePenalizedLLOracleProp.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\UUAENWSN\\VariableSelectionNonconcavePenalizedLLOracleProp.pdf:application/pdf}
}

@article{fan_selective_2010,
	title = {A selective overview of variable selection in high dimensional feature space},
	volume = {20},
	journal = {Statistica Sinica},
	author = {Fan, Jianqing and Lv, Jinchi},
	month = jan,
	year = {2010},
	note = {invited review article},
	pages = {101--148},
	file = {SelectiveOverviewVarSelectionHighDimSpace.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\SelectiveOverviewVarSelectionHighDimSpace.pdf:application/pdf;SelectiveOverviewVarSelectionHighDimSpace.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\JPDS74VV\\SelectiveOverviewVarSelectionHighDimSpace.pdf:application/pdf}
}

@article{futreal_census_2004,
	title = {A census of human cancer genes},
	volume = {4},
	issn = {1474-175X, 1474-1768},
	url = {http://www.nature.com/doifinder/10.1038/nrc1299},
	doi = {10.1038/nrc1299},
	number = {3},
	urldate = {2016-09-13},
	journal = {Nature Reviews Cancer},
	author = {Futreal, P. Andrew and Coin, Lachlan and Marshall, Mhairi and Down, Thomas and Hubbard, Timothy and Wooster, Richard and Rahman, Nazneen and Stratton, Michael R.},
	month = mar,
	year = {2004},
	pages = {177--183},
	file = {CensusHumanCancerGenes.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\RDX5MCHD\\CensusHumanCancerGenes.pdf:application/pdf;CensusHumanCancerGenes.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\G5TCUQJ2\\CensusHumanCancerGenes.pdf:application/pdf}
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Bioinformatics, Computational intelligence, Data mining, Forecasting, Inference, Machine learning, Methodology, Statistics},
	file = {ESLII_print10.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Statistical learning\\ESLII_print10.pdf:application/pdf}
}

@article{hcine_highly_2014,
	title = {Highly {Accurate} {Log} {Skew} {Normal} {Approximation} to the {Sum} of {Correlated} {Lognormals}},
	url = {http://arxiv.org/abs/1501.02347},
	doi = {10.5121/csit.2014.41304},
	abstract = {Several methods have been proposed to approximate the sum of correlated lognormal RVs. However the accuracy of each method relies highly on the region of the resulting distribution being examined, and the individual lognormal parameters, i.e., mean and variance. There is no such method which can provide the needed accuracy for all cases. This paper propose a universal yet very simple approximation method for the sum of correlated lognormals based on log skew normal approximation. The main contribution on this work is to propose an analytical method for log skew normal parameters estimation. The proposed method provides highly accurate approximation to the sum of correlated lognormal distributions over the whole range of dB spreads for any correlation coefficient. Simulation results show that our method outperforms all previously proposed methods and provides an accuracy within 0.01 dB for all cases.},
	urldate = {2016-09-29},
	journal = {arXiv:1501.02347 [cs, math]},
	author = {Hcine, Marwane Ben and Bouallegue, Ridha},
	month = dec,
	year = {2014},
	note = {arXiv: 1501.02347},
	keywords = {Computer Science - Information Theory},
	pages = {41--52},
	file = {arXiv\:1501.02347 PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\K7Z6AZU5\\Hcine and Bouallegue - 2014 - Highly Accurate Log Skew Normal Approximation to t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\95CMQG8U\\1501.html:text/html}
}

@inproceedings{hinton_keeping_1993,
	address = {New York, NY, USA},
	series = {{COLT} '93},
	title = {Keeping the {Neural} {Networks} {Simple} by {Minimizing} the {Description} {Length} of the {Weights}},
	isbn = {978-0-89791-611-0},
	url = {http://doi.acm.org/10.1145/168304.168306},
	doi = {10.1145/168304.168306},
	urldate = {2016-09-27},
	booktitle = {Proceedings of the {Sixth} {Annual} {Conference} on {Computational} {Learning} {Theory}},
	publisher = {ACM},
	author = {Hinton, Geoffrey E. and van Camp, Drew},
	year = {1993},
	pages = {5--13},
	file = {ACM Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\JZ2AQK6D\\Hinton and van Camp - 1993 - Keeping the Neural Networks Simple by Minimizing t.pdf:application/pdf}
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {00401706},
	shorttitle = {Ridge {Regression}},
	url = {http://www.jstor.org/stable/1267351?origin=crossref},
	doi = {10.2307/1267351},
	number = {1},
	urldate = {2015-09-14},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	month = feb,
	year = {1970},
	pages = {55},
	file = {RidgeRegressionBiasedEstimationNonorthogonalProblems.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\RidgeRegressionBiasedEstimationNonorthogonalProblems.pdf:application/pdf}
}

@article{holmes_bayesian_2006,
	title = {Bayesian auxiliary variable models for binary and multinomial regression},
	volume = {1},
	url = {http://projecteuclid.org/euclid.ba/1340371078},
	number = {1},
	urldate = {2016-09-13},
	journal = {Bayesian analysis},
	author = {Holmes, Chris C. and Held, Leonhard and {others}},
	year = {2006},
	pages = {145--168},
	file = {BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\5BEFI3RP\\BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:application/pdf}
}

@article{huang_adaptive_2008,
	title = {Adaptive {Lasso} for sparse high-dimensional regression models},
	url = {http://www.jstor.org/stable/24308572},
	urldate = {2016-09-13},
	journal = {Statistica Sinica},
	author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
	year = {2008},
	pages = {1603--1618},
	file = {AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\SWA3V5BR\\AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:application/pdf}
}

@article{jaakkola_bayesian_2000,
	title = {Bayesian parameter estimation via variational methods},
	volume = {10},
	url = {http://link.springer.com/article/10.1023/A:1008932416310},
	number = {1},
	urldate = {2016-09-13},
	journal = {Statistics and Computing},
	author = {Jaakkola, Tommi S. and Jordan, Michael I.},
	year = {2000},
	pages = {25--37},
	file = {art%3A10.1023%2FA%3A1008932416310.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\VIBNVWR5\\art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf;art%3A10.1023%2FA%3A1008932416310.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\UZFCTMZR\\art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf}
}

@book{jorgensen_statistical_1982,
	address = {New York ; Heidelberg ; Berlin},
	series = {Lecture notes in statistics ; 9. 830800018},
	title = {Statistical properties of the generalized inverse {Gaussian} distribution},
	isbn = {978-0-387-90665-2},
	abstract = {\$\$C note\$\$V Originally presented as the author's thesis (M.Sc.), Aarhus University.},
	language = {eng},
	publisher = {Springer},
	author = {Jørgensen, Bent},
	year = {1982},
	keywords = {31.73 mathematical statistics, QA276.7, Statistiek, Verallgemeinerte inverse Gaussverteilung.},
	file = {bok%3A978-1-4612-5698-4.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\XU6WJNTH\\bok%3A978-1-4612-5698-4.pdf:application/pdf}
}

@article{leday_gene_2015,
	title = {Gene network reconstruction using global-local shrinkage priors},
	url = {http://arxiv.org/abs/1510.03771},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1510.03771},
	author = {Leday, Gwenaël GR and de Gunst, Mathisca and Kpogbezan, Gino B. and Van der Vaart, Aad W. and Van Wieringen, Wessel N. and Van de Wiel, Mark A.},
	year = {2015},
	file = {BSEM_draft_AOAS.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\P9J6PMJC\\BSEM_draft_AOAS.pdf:application/pdf;BSEM_supp.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\S8DJXMWQ\\BSEM_supp.pdf:application/pdf}
}

@article{li_bayesian_2010,
	title = {The {Bayesian} elastic net},
	volume = {5},
	issn = {1936-0975},
	url = {http://projecteuclid.org/euclid.ba/1340369796},
	doi = {10.1214/10-BA506},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Bayesian Analysis},
	author = {Li, Qing and Lin, Nan},
	month = mar,
	year = {2010},
	pages = {151--170},
	file = {BayesianElasticNet.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\BayesianElasticNet.pdf:application/pdf;BayesianElasticNet.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\X6AEU9NC\\BayesianElasticNet.pdf:application/pdf}
}

@incollection{mackay_developments_1995,
	title = {Developments in {Probabilistic} {Modelling} with {Neural} {Networks} — {Ensemble} {Learning}},
	copyright = {©1995 Springer-Verlag London Limited},
	isbn = {978-3-540-19992-2 978-1-4471-3087-1},
	url = {http://link.springer.com/chapter/10.1007/978-1-4471-3087-1_37},
	abstract = {Ensemble learning by variational free energy minimization is a framework for statistical inference in which an ensemble of parameter vectors is optimized rather than a single parameter vector. The ensemble approximates the posterior probability distribution of the parameters. In this paper I give a review of ensemble learning using a simple example.},
	language = {en},
	urldate = {2016-09-26},
	booktitle = {Neural {Networks}: {Artificial} {Intelligence} and {Industrial} {Applications}},
	publisher = {Springer London},
	author = {MacKay, David J. C.},
	editor = {Kappen, Bert and Gielen, Stan},
	year = {1995},
	note = {DOI: 10.1007/978-1-4471-3087-1\_37},
	keywords = {Artificial Intelligence (incl. Robotics), Neurosciences, Pattern Recognition},
	pages = {191--198},
	file = {10.1.1.49.3128.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\DN6UKEP3\\10.1.1.49.3128.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\8TJFWCUW\\10.html:text/html;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\99ENFQE3\\10.html:text/html}
}

@incollection{mackay_hyperparameters:_1996,
	address = {Dordrecht},
	title = {Hyperparameters: {Optimize}, or integrate out?},
	isbn = {978-94-015-8729-7},
	shorttitle = {Hyperparameters},
	url = {http://link.springer.com/chapter/10.1007/978-94-015-8729-7_2},
	urldate = {2016-09-13},
	booktitle = {Maximum entropy and {Bayesian} methods: {Santa} {Barbara}, {California}, {U}.{S}.{A}., 1993},
	publisher = {Springer Netherlands},
	author = {MacKay, David JC},
	editor = {Heidbreder, Glenn R.},
	year = {1996},
	pages = {43--59},
	file = {HyperparametersOptimizeIntegrateOut.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\8K6W266F\\HyperparametersOptimizeIntegrateOut.pdf:application/pdf}
}

@techreport{mackay_ensemble_1997,
	title = {Ensemble learning for hidden {Markov} models},
	author = {MacKay, David J. C.},
	year = {1997},
	file = {10.1.1.52.9627.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\8XXF9PW5\\10.1.1.52.9627.pdf:application/pdf}
}

@article{meier_group_2008,
	title = {The group lasso for logistic regression: {Group} {Lasso} for {Logistic} {Regression}},
	volume = {70},
	issn = {13697412},
	shorttitle = {The group lasso for logistic regression},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
	doi = {10.1111/j.1467-9868.2007.00627.x},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meier, Lukas and Van De Geer, Sara and Bühlmann, Peter},
	month = jan,
	year = {2008},
	pages = {53--71},
	file = {GroupLassoLogisticRegression.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\GroupLassoLogisticRegression.pdf:application/pdf;GroupLassoLogisticRegression.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\IUPASA9T\\GroupLassoLogisticRegression.pdf:application/pdf}
}

@article{michael_generating_1976,
	title = {Generating {Random} {Variates} {Using} {Transformations} with {Multiple} {Roots}},
	volume = {30},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2683801},
	doi = {10.2307/2683801},
	abstract = {The general approach to generating random variates through transformations with multiple roots is discussed. Multinomial probabilities are determined for the selection of the different roots. An application of the general result yields a new and simple technique for the generation of variates from the inverse Gaussian distribution.},
	number = {2},
	urldate = {2016-11-07},
	journal = {The American Statistician},
	author = {Michael, John R. and Schucany, William R. and Haas, Roy W.},
	year = {1976},
	pages = {88--90},
	file = {2683801.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\UJU6WK7Q\\2683801.pdf:application/pdf}
}

@incollection{neal_view_1998,
	series = {{NATO} {ASI} {Series}},
	title = {A {View} of the {Em} {Algorithm} that {Justifies} {Incremental}, {Sparse}, and other {Variants}},
	copyright = {©1998 Springer Science+Business Media Dordrecht},
	isbn = {978-94-010-6104-9 978-94-011-5014-9},
	url = {http://link.springer.com/chapter/10.1007/978-94-011-5014-9_12},
	abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
	language = {en},
	number = {89},
	urldate = {2016-09-27},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Neal, Radford M. and Hinton, Geoffrey E.},
	editor = {Jordan, Michael I.},
	year = {1998},
	note = {DOI: 10.1007/978-94-011-5014-9\_12},
	keywords = {Artificial Intelligence (incl. Robotics), Statistical Physics, Dynamical Systems and Complexity, Statistics, general},
	pages = {355--368},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\R7ZTS7EZ\\Neal and Hinton - 1998 - A View of the Em Algorithm that Justifies Incremen.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\RWXGTBBN\\10.html:text/html}
}

@article{neuenschwander_summarizing_2010,
	title = {Summarizing historical information on controls in clinical trials},
	volume = {7},
	issn = {1740-7745},
	url = {http://ctj.sagepub.com/cgi/doi/10.1177/1740774509356002},
	doi = {10.1177/1740774509356002},
	language = {en},
	number = {5},
	urldate = {2016-09-13},
	journal = {Clinical Trials},
	author = {Neuenschwander, B. and Capkun-Niggli, G. and Branson, M. and Spiegelhalter, D. J.},
	month = jan,
	year = {2010},
	pages = {5--18},
	file = {SummarizingHistoricalInformationControlsClinicalTrials.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\PZDAHZPA\\SummarizingHistoricalInformationControlsClinicalTrials.pdf:application/pdf}
}

@article{polson_bayesian_2013,
	title = {Bayesian {Inference} for {Logistic} {Models} {Using} {Pólya}–{Gamma} {Latent} {Variables}},
	volume = {108},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2013.829001},
	doi = {10.1080/01621459.2013.829001},
	abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
	number = {504},
	urldate = {2016-09-15},
	journal = {Journal of the American Statistical Association},
	author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
	month = dec,
	year = {2013},
	pages = {1339--1349},
	file = {BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\VDK8S2H4\\BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:application/pdf;Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\PDKP9JFH\\Polson et al. - 2013 - Bayesian Inference for Logistic Models Using Pólya.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\HWBHFG72\\01621459.2013.html:text/html;techsuppR1.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\SVUQ2BMZ\\techsuppR1.pdf:application/pdf}
}

@incollection{bernardo_shrink_2011,
	title = {Shrink {Globally}, {Act} {Locally}: {Sparse} {Bayesian} {Regularization} and {Prediction}*},
	isbn = {978-0-19-969458-7},
	shorttitle = {Shrink {Globally}, {Act} {Locally}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199694587.001.0001/acprof-9780199694587-chapter-17},
	urldate = {2016-10-25},
	booktitle = {Bayesian {Statistics} 9},
	publisher = {Oxford University Press},
	author = {Polson, Nicholas G. and Scott, James G.},
	editor = {Bernardo, José M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
	month = oct,
	year = {2011},
	pages = {501--538},
	file = {Bayes1.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\HQPRWEJJ\\Bayes1.pdf:application/pdf}
}

@article{polson_local_2012,
	title = {Local shrinkage rules, {Lévy} processes and regularized regression},
	volume = {74},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.01015.x/abstract},
	doi = {10.1111/j.1467-9868.2011.01015.x},
	abstract = {Summary.  We use Lévy processes to generate joint prior distributions, and therefore penalty functions, for a location parameter  as p grows large. This generalizes the class of local–global shrinkage rules based on scale mixtures of normals, illuminates new connections between disparate methods and leads to new results for computing posterior means and modes under a wide class of priors. We extend this framework to large-scale regularized regression problems where p{\textgreater}n, and we provide comparisons with other methodologies.},
	language = {en},
	number = {2},
	urldate = {2016-10-25},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Polson, Nicholas G. and Scott, James G.},
	month = mar,
	year = {2012},
	keywords = {Lévy processes, Normal scale mixtures, Partial least squares, Principal components regression, Shrinkage, Sparsity},
	pages = {287--311},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\TGZI96T5\\Polson and Scott - 2012 - Local shrinkage rules, Lévy processes and regulari.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\8HTX7V9K\\abstract.html:text/html}
}

@article{rajaratnam_lasso_2016,
	title = {Lasso regression: estimation and shrinkage via the limit of {Gibbs} sampling},
	volume = {78},
	issn = {1467-9868},
	shorttitle = {Lasso regression},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12106/abstract},
	doi = {10.1111/rssb.12106},
	abstract = {The application of the lasso is espoused in high dimensional settings where only a small number of the regression coefficients are believed to be non-zero (i.e. the solution is sparse). Moreover, statistical properties of high dimensional lasso estimators are often proved under the assumption that the correlation between the predictors is bounded. In this vein, co-ordinatewise methods, which are the most common means of computing the lasso solution, naturally work well in the presence of low-to-moderate multicollinearity. The computational speed of co-ordinatewise algorithms, although excellent for sparse and low-to-moderate multicollinearity settings, degrades as sparsity decreases and multicollinearity increases. Though lack of sparsity and high multicollinearity can be quite common in contemporary applications, model selection is still a necessity in such settings. Motivated by the limitations of co-ordinatewise algorithms in such ‘non-sparse’ and ‘high multicollinearity’ settings, we propose the novel ‘deterministic Bayesian lasso’ algorithm for computing the lasso solution. This algorithm is developed by considering a limiting version of the Bayesian lasso. In contrast with co-ordinatewise algorithms, the performance of the deterministic Bayesian lasso improves as sparsity decreases and multicollinearity increases. Importantly, in non-sparse and high multicollinearity settings the algorithm proposed can offer substantial increases in computational speed over co-ordinatewise algorithms. A rigorous theoretical analysis demonstrates that the deterministic Bayesian lasso algorithm converges to the lasso solution and it leads to a representation of the lasso estimator which shows how it achieves both l1- and l2-types of shrinkage simultaneously. Connections between the deterministic Bayesian lasso and other algorithms are also provided. The benefits of the deterministic Bayesian lasso algorithm are then illustrated on simulated and real data.},
	language = {en},
	number = {1},
	urldate = {2016-09-13},
	journal = {J. R. Stat. Soc. B},
	author = {Rajaratnam, Bala and Roberts, Steven and Sparks, Doug and Dalal, Onkar},
	month = jan,
	year = {2016},
	keywords = {Bayesian lasso, Lasso regression, Limit of Gibbs sampler, Multicollinearity},
	pages = {153--174},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\BATTWFT2\\Rajaratnam et al. - 2016 - Lasso regression estimation and shrinkage via the.pdf:application/pdf;Rajaratnam_et_al-2016_Supplements.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\4T8NK9D6\\Rajaratnam_et_al-2016_Supplements.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\4N9FZQGW\\abstract.html:text/html}
}

@article{scott_expectation-maximization_2013,
	title = {Expectation-maximization for logistic regression},
	url = {http://arxiv.org/abs/1306.0040},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1306.0040},
	author = {Scott, James G. and Sun, Liang},
	year = {2013},
	file = {EMLogisticRegression.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\ZXVP32KX\\EMLogisticRegression.pdf:application/pdf}
}

@article{simon_sparse-group_2013,
	title = {A {Sparse}-{Group} {Lasso}},
	volume = {22},
	issn = {1061-8600},
	url = {http://dx.doi.org/10.1080/10618600.2012.681250},
	doi = {10.1080/10618600.2012.681250},
	abstract = {For high-dimensional supervised learning problems, often using problem-specific assumptions can lead to greater accuracy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with ℓ1 and ℓ2 penalties. We discuss the sparsity and other regularization properties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradient descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data. This article has online supplementary material.},
	number = {2},
	urldate = {2016-11-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2013},
	keywords = {Model, Nesterov, Penalize, Regression, Regularize},
	pages = {231--245},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\BJS8SUZI\\Simon et al. - 2013 - A Sparse-Group Lasso.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\3FNIU6JK\\10618600.2012.html:text/html}
}

@article{tai_incorporating_2007,
	title = {Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms},
	volume = {23},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btm234},
	doi = {10.1093/bioinformatics/btm234},
	language = {en},
	number = {14},
	urldate = {2015-09-14},
	journal = {Bioinformatics},
	author = {Tai, F. and Pan, W.},
	month = jul,
	year = {2007},
	pages = {1775--1782},
	file = {IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:application/pdf;IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\C9M9HWTG\\IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the lasso},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society, Series B},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {RegressionShrinkageSelectionLasso.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\RegressionShrinkageSelectionLasso.pdf:application/pdf}
}

@incollection{turner_online_2013,
	title = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}: {With} {Application} to {Radar} {Tracking}},
	shorttitle = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}},
	url = {http://papers.nips.cc/paper/5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-application-to-radar-tracking.pdf},
	urldate = {2016-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Turner, Ryan D and Bottone, Steven and Stanek, Clay J},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {306--314},
	file = {NIPS Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\852DAFWI\\Turner et al. - 2013 - Online Variational Approximations to non-Exponenti.pdf:application/pdf;NIPS Snapshort:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\JK4B6R4U\\5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-appli.html:text/html}
}

@article{van_de_geer_adaptive_2011,
	title = {The adaptive and the thresholded {Lasso} for potentially misspecified models (and a lower bound for the {Lasso})},
	volume = {5},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1311600467},
	doi = {10.1214/11-EJS624},
	language = {en},
	number = {0},
	urldate = {2016-09-13},
	journal = {Electronic Journal of Statistics},
	author = {van de Geer, Sara and Bühlmann, Peter and Zhou, Shuheng},
	year = {2011},
	pages = {688--749},
	file = {AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\GZP99W7R\\AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:application/pdf}
}

@article{van_de_wiel_better_2016,
	title = {Better prediction by use of co-data: adaptive group-regularized ridge regression},
	volume = {35},
	issn = {02776715},
	shorttitle = {Better prediction by use of co-data},
	url = {http://doi.wiley.com/10.1002/sim.6732},
	doi = {10.1002/sim.6732},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Statistics in Medicine},
	author = {van de Wiel, Mark A. and Lien, Tonje G. and Verlaat, Wina and van Wieringen, Wessel N. and Wilting, Saskia M.},
	month = feb,
	year = {2016},
	pages = {368--381},
	file = {BetterPredictionCodataAdaptiveGroupRegularizedRidgeRegression.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\N7W75WGI\\BetterPredictionCodataAdaptiveGroupRegularizedRidgeRegression.pdf:application/pdf;van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\WQFH375F\\van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf}
}

@article{waldron_optimized_2011,
	title = {Optimized application of penalized regression methods to diverse genomic data},
	volume = {27},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/27/24/3399},
	doi = {10.1093/bioinformatics/btr591},
	abstract = {Motivation: Penalized regression methods have been adopted widely for high-dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well-understood, specific methodology for their optimal application to genomic data has not been determined.
Results: Through simulation of contrasting scenarios of correlated high-dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net behave more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real-life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from metagenomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data.
Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pensim R package, available at http://cran.r-project.org/web/packages/pensim/index.html.
Contact: chuttenh@hsph.harvard.edu; juris@ai.utoronto.ca
Supplementary Information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {24},
	urldate = {2016-09-13},
	journal = {Bioinformatics},
	author = {Waldron, Levi and Pintilie, Melania and Tsao, Ming-Sound and Shepherd, Frances A. and Huttenhower, Curtis and Jurisica, Igor},
	month = dec,
	year = {2011},
	pmid = {22156367},
	pages = {3399--3406},
	file = {Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\CBSRGTTF\\Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\ET8MSI3A\\Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\2JR92KZ9\\3399.html:text/html;Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\P6Z6GZWC\\3399.html:text/html}
}

@incollection{watanabe_application_2003,
	title = {Application of {Variational} {Bayesian} {Approach} to {Speech} {Recognition}},
	url = {http://papers.nips.cc/paper/2174-application-of-variational-bayesian-approach-to-speech-recognition.pdf},
	urldate = {2016-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
	publisher = {MIT Press},
	author = {Watanabe, Shinji and Minami, Yasuhiro and Nakamura, Atsushi and Ueda, Naonori},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	year = {2003},
	pages = {1261--1268},
	file = {NIPS Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\4R8RXBJG\\Watanabe et al. - 2003 - Application of Variational Bayesian Approach to Sp.pdf:application/pdf;NIPS Snapshort:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\J339DID9\\2174-application-of-variational-bayesian-approach-to-speech-recognition.html:text/html}
}

@article{yuan_model_2006,
	title = {Model {Selection} and {Estimation} in {Regression} with {Grouped} {Variables}},
	volume = {68},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/3647556},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	year = {2006},
	pages = {49--67},
	file = {JSTOR Full Text PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\CCXX233Q\\Yuan and Lin - 2006 - Model Selection and Estimation in Regression with .pdf:application/pdf}
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s proportional hazards model},
	volume = {94},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asm037},
	doi = {10.1093/biomet/asm037},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Biometrika},
	author = {Zhang, H. H. and Lu, W.},
	month = aug,
	year = {2007},
	pages = {691--703},
	file = {AdaptiveLassoCoxProportionalHazardsModel.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\ABQT8GA5\\AdaptiveLassoCoxProportionalHazardsModel.pdf:application/pdf}
}

@article{zhao_diagnostics_2013,
	title = {Diagnostics for {Variational} {Bayes} approximations},
	url = {http://arxiv.org/abs/1309.5117},
	abstract = {Variational Bayes (VB) has shown itself to be a powerful approximation method in many application areas. This paper describes some diagnostics methods which can assess how well the VB approximates the true posterior, particularly with regards to its covariance structure. The methods proposed also allow us to generate simple corrections when the approximation error is large. It looks at joint, marginal and conditional aspects of the approximate posterior and shows how to apply these techniques in both simulated and real data examples.},
	urldate = {2016-11-01},
	journal = {arXiv:1309.5117 [stat]},
	author = {Zhao, Hui and Marriott, Paul},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.5117},
	keywords = {Statistics - Computation},
	file = {arXiv\:1309.5117 PDF:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\FE3BJZ2P\\Zhao and Marriott - 2013 - Diagnostics for Variational Bayes approximations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\H54KB5JD\\1309.html:text/html}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1369-7412, 1467-9868},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	language = {en},
	number = {2},
	urldate = {2015-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	pages = {301--320},
	file = {RegularizationVariableSelectionElasticNet.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\RegularizationVariableSelectionElasticNet.pdf:application/pdf}
}

@article{zou_adaptive_2009,
	title = {On the adaptive elastic-net with a diverging number of parameters},
	volume = {37},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1245332831},
	doi = {10.1214/08-AOS625},
	language = {en},
	number = {4},
	urldate = {2015-09-14},
	journal = {The Annals of Statistics},
	author = {Zou, Hui and Zhang, Hao Helen},
	month = aug,
	year = {2009},
	pages = {1733--1751},
	file = {AdaptiveElasticNetDivergingNumberParameters.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf;AdaptiveElasticNetDivergingNumberParameters.pdf:C\:\\Users\\Magnus\\Documents\\Statistical Science\\Jaar 2\\Thesis\\Papers\\AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf;AdaptiveElasticNetDivergingNumberParameters.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\WU4JGNGW\\AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf}
}

@article{zou_adaptive_2006,
	title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000735},
	doi = {10.1198/016214506000000735},
	language = {en},
	number = {476},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Zou, Hui},
	month = dec,
	year = {2006},
	pages = {1418--1429},
	file = {AdaptiveLassoOracleProperties.pdf:C\:\\Users\\Magnus\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\tatqb8d4.default\\zotero\\storage\\Z6CT3PMT\\AdaptiveLassoOracleProperties.pdf:application/pdf}
}