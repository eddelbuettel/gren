
@article{choi_polya-gamma_2013,
	title = {The {Polya}-{Gamma} {Gibbs} sampler for {Bayesian} logistic regression is uniformly ergodic},
	volume = {7},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1377005819},
	doi = {10.1214/13-EJS837},
	abstract = {One of the most widely used data augmentation algorithms is Albert and Chib’s (1993) algorithm for Bayesian probit regression. Polson, Scott, and Windle (2013) recently introduced an analogous algorithm for Bayesian logistic regression. The main difference between the two is that Albert and Chib’s (1993) truncated normals are replaced by so-called Polya-Gamma random variables. In this note, we establish that the Markov chain underlying Polson, Scott, and Windle’s (2013) algorithm is uniformly ergodic. This theoretical result has important practical benefits. In particular, it guarantees the existence of central limit theorems that can be used to make an informed decision about how long the simulation should be run.},
	language = {EN},
	urldate = {2016-10-19},
	journal = {Electron. J. Statist.},
	author = {Choi, Hee Min and Hobert, James P.},
	year = {2013},
	mrnumber = {MR3091616},
	keywords = {data augmentation algorithm, Markov chain, minorization condition, Monte Carlo, Polya-Gamma distribution},
	pages = {2054--2064},
	file = {euclid.ejs.1377005819.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/GXHX58IR/euclid.ejs.1377005819.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/8MI6UI5B/1377005819.html:text/html}
}

@techreport{mackay_ensemble_1997,
	title = {Ensemble learning for hidden {Markov} models},
	author = {MacKay, David J. C.},
	year = {1997},
	file = {10.1.1.52.9627.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/8XXF9PW5/10.1.1.52.9627.pdf:application/pdf}
}

@article{chib_marginal_1995,
	title = {Marginal {Likelihood} from the {Gibbs} {Output}},
	volume = {90},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2291521},
	doi = {10.2307/2291521},
	abstract = {In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.},
	number = {432},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Chib, Siddhartha},
	year = {1995},
	pages = {1313--1321},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/FVFJICM7/Chib - 1995 - Marginal Likelihood from the Gibbs Output.pdf:application/pdf}
}

@article{polson_bayesian_2013,
	title = {Bayesian {Inference} for {Logistic} {Models} {Using} {Pólya}–{Gamma} {Latent} {Variables}},
	volume = {108},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2013.829001},
	doi = {10.1080/01621459.2013.829001},
	abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
	number = {504},
	urldate = {2016-09-15},
	journal = {Journal of the American Statistical Association},
	author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
	month = dec,
	year = {2013},
	pages = {1339--1349},
	file = {BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/VDK8S2H4/BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/PDKP9JFH/Polson et al. - 2013 - Bayesian Inference for Logistic Models Using Pólya.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/HWBHFG72/01621459.2013.html:text/html;techsuppR1.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/SVUQ2BMZ/techsuppR1.pdf:application/pdf}
}

@article{rajaratnam_lasso_2016,
	title = {Lasso regression: estimation and shrinkage via the limit of {Gibbs} sampling},
	volume = {78},
	issn = {1467-9868},
	shorttitle = {Lasso regression},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12106/abstract},
	doi = {10.1111/rssb.12106},
	abstract = {The application of the lasso is espoused in high dimensional settings where only a small number of the regression coefficients are believed to be non-zero (i.e. the solution is sparse). Moreover, statistical properties of high dimensional lasso estimators are often proved under the assumption that the correlation between the predictors is bounded. In this vein, co-ordinatewise methods, which are the most common means of computing the lasso solution, naturally work well in the presence of low-to-moderate multicollinearity. The computational speed of co-ordinatewise algorithms, although excellent for sparse and low-to-moderate multicollinearity settings, degrades as sparsity decreases and multicollinearity increases. Though lack of sparsity and high multicollinearity can be quite common in contemporary applications, model selection is still a necessity in such settings. Motivated by the limitations of co-ordinatewise algorithms in such ‘non-sparse’ and ‘high multicollinearity’ settings, we propose the novel ‘deterministic Bayesian lasso’ algorithm for computing the lasso solution. This algorithm is developed by considering a limiting version of the Bayesian lasso. In contrast with co-ordinatewise algorithms, the performance of the deterministic Bayesian lasso improves as sparsity decreases and multicollinearity increases. Importantly, in non-sparse and high multicollinearity settings the algorithm proposed can offer substantial increases in computational speed over co-ordinatewise algorithms. A rigorous theoretical analysis demonstrates that the deterministic Bayesian lasso algorithm converges to the lasso solution and it leads to a representation of the lasso estimator which shows how it achieves both l1- and l2-types of shrinkage simultaneously. Connections between the deterministic Bayesian lasso and other algorithms are also provided. The benefits of the deterministic Bayesian lasso algorithm are then illustrated on simulated and real data.},
	language = {en},
	number = {1},
	urldate = {2016-09-13},
	journal = {J. R. Stat. Soc. B},
	author = {Rajaratnam, Bala and Roberts, Steven and Sparks, Doug and Dalal, Onkar},
	month = jan,
	year = {2016},
	keywords = {Bayesian lasso, Lasso regression, Limit of Gibbs sampler, Multicollinearity},
	pages = {153--174},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/BATTWFT2/Rajaratnam et al. - 2016 - Lasso regression estimation and shrinkage via the.pdf:application/pdf;Rajaratnam_et_al-2016_Supplements.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/4T8NK9D6/Rajaratnam_et_al-2016_Supplements.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/4N9FZQGW/abstract.html:text/html}
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and its {Oracle} {Properties}},
	volume = {96},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
	doi = {10.1198/016214501753382273},
	language = {en},
	number = {456},
	urldate = {2015-09-14},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	month = dec,
	year = {2001},
	pages = {1348--1360},
	file = {VariableSelectionNonconcavePenalizedLLOracleProp.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/UUAENWSN/VariableSelectionNonconcavePenalizedLLOracleProp.pdf:application/pdf}
}

@incollection{turner_online_2013,
	title = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}: {With} {Application} to {Radar} {Tracking}},
	shorttitle = {Online {Variational} {Approximations} to non-{Exponential} {Family} {Change} {Point} {Models}},
	url = {http://papers.nips.cc/paper/5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-application-to-radar-tracking.pdf},
	urldate = {2016-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Turner, Ryan D and Bottone, Steven and Stanek, Clay J},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {306--314},
	file = {NIPS Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/852DAFWI/Turner et al. - 2013 - Online Variational Approximations to non-Exponenti.pdf:application/pdf;NIPS Snapshort:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/JK4B6R4U/5124-online-variational-approximations-to-non-exponential-family-change-point-models-with-appli.html:text/html}
}

@phdthesis{beal_variational_2003,
	address = {London},
	title = {Variational algorithms for approximate {Bayesian} inference},
	url = {https://www.researchgate.net/publication/34000771_Variational_algorithms_for_approximate_Bayesian_inference},
	abstract = {Thesis (Ph.D.)--University of London, 2003.},
	urldate = {2016-09-26},
	school = {University College},
	author = {Beal, Matthew James},
	month = jan,
	year = {2003},
	file = {Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/CAIHJJFB/34000771_Variational_algorithms_for_approximate_Bayesian_inference.html:text/html;VariationalAlgorithmsApproximateBayesianInference.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/VQC6GFJG/VariationalAlgorithmsApproximateBayesianInference.pdf:application/pdf}
}

@inproceedings{fan_statistical_2007,
	title = {Statistical challenges with high dimensionality: feature selection in knowledge discovery},
	volume = {III},
	shorttitle = {Statistical challenges with high dimensionality},
	booktitle = {Proceedings of the {International} {Congress} of {Mathematicians} {Madrid}, {August} 22-30, 2006},
	author = {Fan, J and Li, R},
	editor = {Sanz Solé, Marta and Soria, Javier and Varona, Juan Luis and Verdera, Joan},
	year = {2007},
	keywords = {dimensionality, high, learning, machine},
	pages = {595--622},
	file = {StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/E92B4WS6/StatisticalChallengesHighDimensionalityFeatureSelectionKnowledgeDiscovery.pdf:application/pdf}
}

@article{zhao_diagnostics_2013,
	title = {Diagnostics for {Variational} {Bayes} approximations},
	url = {http://arxiv.org/abs/1309.5117},
	abstract = {Variational Bayes (VB) has shown itself to be a powerful approximation method in many application areas. This paper describes some diagnostics methods which can assess how well the VB approximates the true posterior, particularly with regards to its covariance structure. The methods proposed also allow us to generate simple corrections when the approximation error is large. It looks at joint, marginal and conditional aspects of the approximate posterior and shows how to apply these techniques in both simulated and real data examples.},
	urldate = {2016-11-01},
	journal = {arXiv:1309.5117 [stat]},
	author = {Zhao, Hui and Marriott, Paul},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.5117},
	keywords = {Statistics - Computation},
	file = {arXiv\:1309.5117 PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/FE3BJZ2P/Zhao and Marriott - 2013 - Diagnostics for Variational Bayes approximations.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/H54KB5JD/1309.html:text/html}
}

@article{carbonetto_scalable_2012,
	title = {Scalable {Variational} {Inference} for {Bayesian} {Variable} {Selection} in {Regression}, and {Its} {Accuracy} in {Genetic} {Association} {Studies}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {http://projecteuclid.org/euclid.ba/1339616726},
	doi = {10.1214/12-BA703},
	abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities{\textbar}including posterior distributions of the hyperparameters{\textbar}are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables.},
	language = {EN},
	number = {1},
	urldate = {2016-09-27},
	journal = {Bayesian Anal.},
	author = {Carbonetto, Peter and Stephens, Matthew},
	month = mar,
	year = {2012},
	mrnumber = {MR2896713},
	keywords = {genetic association studies, Monte Carlo, Variable selection, variational inference},
	pages = {73--108},
	file = {euclid.ba.1339616726.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/NWEDS26P/euclid.ba.1339616726.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/JAQG6PR2/1339616726.html:text/html}
}

@article{meier_group_2008,
	title = {The group lasso for logistic regression: {Group} {Lasso} for {Logistic} {Regression}},
	volume = {70},
	issn = {13697412},
	shorttitle = {The group lasso for logistic regression},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
	doi = {10.1111/j.1467-9868.2007.00627.x},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meier, Lukas and Van De Geer, Sara and Bühlmann, Peter},
	month = jan,
	year = {2008},
	pages = {53--71},
	file = {GroupLassoLogisticRegression.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/IUPASA9T/GroupLassoLogisticRegression.pdf:application/pdf}
}

@article{blei_variational_2016,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior distribution. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	urldate = {2016-09-27},
	journal = {arXiv:1601.00670 [cs, stat]},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1601.00670 PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/JPZ2BADG/Blei et al. - 2016 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/Z9PF9AIP/1601.html:text/html}
}

@article{fan_selective_2010,
	title = {A selective overview of variable selection in high dimensional feature space},
	volume = {20},
	journal = {Statistica Sinica},
	author = {Fan, Jianqing and Lv, Jinchi},
	month = jan,
	year = {2010},
	note = {invited review article},
	pages = {101--148},
	file = {SelectiveOverviewVarSelectionHighDimSpace.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/JPDS74VV/SelectiveOverviewVarSelectionHighDimSpace.pdf:application/pdf}
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Bioinformatics, Computational intelligence, Data mining, Forecasting, Inference, Machine learning, Methodology, Statistics}
}

@book{jorgensen_statistical_1982,
	address = {New York ; Heidelberg ; Berlin},
	series = {Lecture notes in statistics ; 9. 830800018},
	title = {Statistical properties of the generalized inverse {Gaussian} distribution},
	isbn = {978-0-387-90665-2},
	abstract = {\$\$C note\$\$V Originally presented as the author's thesis (M.Sc.), Aarhus University.},
	language = {eng},
	publisher = {Springer},
	author = {Jørgensen, Bent},
	year = {1982},
	keywords = {31.73 mathematical statistics, QA276.7, Statistiek, Verallgemeinerte inverse Gaussverteilung.},
	file = {bok%3A978-1-4612-5698-4.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/XU6WJNTH/bok%3A978-1-4612-5698-4.pdf:application/pdf}
}

@article{van_de_wiel_better_2016,
	title = {Better prediction by use of co-data: adaptive group-regularized ridge regression},
	volume = {35},
	issn = {02776715},
	shorttitle = {Better prediction by use of co-data},
	url = {http://doi.wiley.com/10.1002/sim.6732},
	doi = {10.1002/sim.6732},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Statistics in Medicine},
	author = {van de Wiel, Mark A. and Lien, Tonje G. and Verlaat, Wina and van Wieringen, Wessel N. and Wilting, Saskia M.},
	month = feb,
	year = {2016},
	pages = {368--381},
	file = {BetterPredictionCodataAdaptiveGroupRegularizedRidgeRegression.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/N7W75WGI/BetterPredictionCodataAdaptiveGroupRegularizedRidgeRegression.pdf:application/pdf;sim6732-sup-0001-Supplementary1.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/CNPFHP5A/sim6732-sup-0001-Supplementary1.pdf:application/pdf;van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/WQFH375F/van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf}
}

@article{beal_variational_2003-1,
	title = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}: with {Application} to {Scoring} {Graphical} {Model} {Structures}},
	volume = {7},
	shorttitle = {The {Variational} {Bayesian} {EM} {Algorithm} for {Incomplete} {Data}},
	journal = {Bayesian Statistics},
	author = {Beal, MJ and Ghahramani, Z},
	year = {2003},
	file = {Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/CUH5QCHC/224881759_The_Variational_Bayesian_EM_Algorithm_for_Incomplete_Data_with_Application_to_Scoring.html:text/html;valencia02.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/74JX9H8E/valencia02.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the lasso},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society, Series B},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288}
}

@article{polson_local_2012,
	title = {Local shrinkage rules, {Lévy} processes and regularized regression},
	volume = {74},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.01015.x/abstract},
	doi = {10.1111/j.1467-9868.2011.01015.x},
	abstract = {Summary.  We use Lévy processes to generate joint prior distributions, and therefore penalty functions, for a location parameter  as p grows large. This generalizes the class of local–global shrinkage rules based on scale mixtures of normals, illuminates new connections between disparate methods and leads to new results for computing posterior means and modes under a wide class of priors. We extend this framework to large-scale regularized regression problems where p{\textgreater}n, and we provide comparisons with other methodologies.},
	language = {en},
	number = {2},
	urldate = {2016-10-25},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Polson, Nicholas G. and Scott, James G.},
	month = mar,
	year = {2012},
	keywords = {Lévy processes, Normal scale mixtures, Partial least squares, Principal components regression, Shrinkage, Sparsity},
	pages = {287--311},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/TGZI96T5/Polson and Scott - 2012 - Local shrinkage rules, Lévy processes and regulari.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/8HTX7V9K/abstract.html:text/html}
}

@article{biane_probability_2001,
	title = {Probability laws related to the {Jacobi} theta and {Riemann} zeta functions, and {Brownian} excursions},
	volume = {38},
	issn = {0273-0979, 1088-9485},
	url = {http://www.ams.org/bull/2001-38-04/S0273-0979-01-00912-0/},
	doi = {10.1090/S0273-0979-01-00912-0},
	abstract = {Abstract:},
	number = {4},
	urldate = {2016-10-07},
	journal = {Bull. Amer. Math. Soc.},
	author = {Biane, Philippe and Pitman, Jim and Yor, Marc},
	year = {2001},
	keywords = {Bessel process, functional equation, Infinitely divisible laws, sums of independent exponential variables},
	pages = {435--465},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/E3DHE52G/Biane et al. - 2001 - Probability laws related to the Jacobi theta and R.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/IQPCE2QJ/S0273-0979-01-00912-0.html:text/html}
}

@article{tai_incorporating_2007,
	title = {Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms},
	volume = {23},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btm234},
	doi = {10.1093/bioinformatics/btm234},
	language = {en},
	number = {14},
	urldate = {2015-09-14},
	journal = {Bioinformatics},
	author = {Tai, F. and Pan, W.},
	month = jul,
	year = {2007},
	pages = {1775--1782},
	file = {IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/C9M9HWTG/IncorpPriorKnowledgePredsPenClassifiersMultPenaltyTerms.pdf:application/pdf}
}

@article{zou_adaptive_2006,
	title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000735},
	doi = {10.1198/016214506000000735},
	language = {en},
	number = {476},
	urldate = {2016-09-13},
	journal = {Journal of the American Statistical Association},
	author = {Zou, Hui},
	month = dec,
	year = {2006},
	pages = {1418--1429},
	file = {AdaptiveLassoOracleProperties.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/Z6CT3PMT/AdaptiveLassoOracleProperties.pdf:application/pdf}
}

@article{waldron_optimized_2011,
	title = {Optimized application of penalized regression methods to diverse genomic data},
	volume = {27},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/27/24/3399},
	doi = {10.1093/bioinformatics/btr591},
	abstract = {Motivation: Penalized regression methods have been adopted widely for high-dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well-understood, specific methodology for their optimal application to genomic data has not been determined.
Results: Through simulation of contrasting scenarios of correlated high-dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net behave more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real-life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from metagenomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data.
Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pensim R package, available at http://cran.r-project.org/web/packages/pensim/index.html.
Contact: chuttenh@hsph.harvard.edu; juris@ai.utoronto.ca
Supplementary Information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {24},
	urldate = {2016-09-13},
	journal = {Bioinformatics},
	author = {Waldron, Levi and Pintilie, Melania and Tsao, Ming-Sound and Shepherd, Frances A. and Huttenhower, Curtis and Jurisica, Igor},
	month = dec,
	year = {2011},
	pmid = {22156367},
	pages = {3399--3406},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/CBSRGTTF/Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/ET8MSI3A/Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/2JR92KZ9/3399.html:text/html;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/P6Z6GZWC/3399.html:text/html}
}

@article{van_de_geer_adaptive_2011,
	title = {The adaptive and the thresholded {Lasso} for potentially misspecified models (and a lower bound for the {Lasso})},
	volume = {5},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1311600467},
	doi = {10.1214/11-EJS624},
	language = {en},
	number = {0},
	urldate = {2016-09-13},
	journal = {Electronic Journal of Statistics},
	author = {van de Geer, Sara and Bühlmann, Peter and Zhou, Shuheng},
	year = {2011},
	pages = {688--749},
	file = {AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/GZP99W7R/AdaptiveThresholdedLassoPotentiallyMisspecifiedModels.pdf:application/pdf}
}

@article{hcine_highly_2014,
	title = {Highly {Accurate} {Log} {Skew} {Normal} {Approximation} to the {Sum} of {Correlated} {Lognormals}},
	url = {http://arxiv.org/abs/1501.02347},
	doi = {10.5121/csit.2014.41304},
	abstract = {Several methods have been proposed to approximate the sum of correlated lognormal RVs. However the accuracy of each method relies highly on the region of the resulting distribution being examined, and the individual lognormal parameters, i.e., mean and variance. There is no such method which can provide the needed accuracy for all cases. This paper propose a universal yet very simple approximation method for the sum of correlated lognormals based on log skew normal approximation. The main contribution on this work is to propose an analytical method for log skew normal parameters estimation. The proposed method provides highly accurate approximation to the sum of correlated lognormal distributions over the whole range of dB spreads for any correlation coefficient. Simulation results show that our method outperforms all previously proposed methods and provides an accuracy within 0.01 dB for all cases.},
	urldate = {2016-09-29},
	journal = {arXiv:1501.02347 [cs, math]},
	author = {Hcine, Marwane Ben and Bouallegue, Ridha},
	month = dec,
	year = {2014},
	note = {arXiv: 1501.02347},
	pages = {41--52},
	file = {arXiv\:1501.02347 PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/K7Z6AZU5/Hcine and Bouallegue - 2014 - Highly Accurate Log Skew Normal Approximation to t.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/95CMQG8U/1501.html:text/html}
}

@incollection{mackay_hyperparameters:_1996,
	address = {Dordrecht},
	title = {Hyperparameters: {Optimize}, or integrate out?},
	isbn = {978-94-015-8729-7},
	shorttitle = {Hyperparameters},
	url = {http://link.springer.com/chapter/10.1007/978-94-015-8729-7_2},
	urldate = {2016-09-13},
	booktitle = {Maximum entropy and {Bayesian} methods: {Santa} {Barbara}, {California}, {U}.{S}.{A}., 1993},
	publisher = {Springer Netherlands},
	author = {MacKay, David JC},
	editor = {Heidbreder, Glenn R.},
	year = {1996},
	pages = {43--59},
	file = {HyperparametersOptimizeIntegrateOut.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/8K6W266F/HyperparametersOptimizeIntegrateOut.pdf:application/pdf}
}

@incollection{watanabe_application_2003,
	title = {Application of {Variational} {Bayesian} {Approach} to {Speech} {Recognition}},
	url = {http://papers.nips.cc/paper/2174-application-of-variational-bayesian-approach-to-speech-recognition.pdf},
	urldate = {2016-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
	publisher = {MIT Press},
	author = {Watanabe, Shinji and Minami, Yasuhiro and Nakamura, Atsushi and Ueda, Naonori},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	year = {2003},
	pages = {1261--1268},
	file = {NIPS Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/4R8RXBJG/Watanabe et al. - 2003 - Application of Variational Bayesian Approach to Sp.pdf:application/pdf;NIPS Snapshort:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/J339DID9/2174-application-of-variational-bayesian-approach-to-speech-recognition.html:text/html}
}

@article{yuan_model_2006,
	title = {Model {Selection} and {Estimation} in {Regression} with {Grouped} {Variables}},
	volume = {68},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/3647556},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	year = {2006},
	pages = {49--67},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/CCXX233Q/Yuan and Lin - 2006 - Model Selection and Estimation in Regression with .pdf:application/pdf}
}

@article{scott_expectation-maximization_2013,
	title = {Expectation-maximization for logistic regression},
	url = {http://arxiv.org/abs/1306.0040},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1306.0040},
	author = {Scott, James G. and Sun, Liang},
	year = {2013},
	file = {EMLogisticRegression.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/ZXVP32KX/EMLogisticRegression.pdf:application/pdf}
}

@misc{r_development_core_team_r:_2008,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	isbn = {3-900051-07-0},
	url = {http://www.R-project.org},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Development Core Team}},
	year = {2008},
	file = {euclid.aos.1091626180.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/J5KWDNE7/euclid.aos.1091626180.pdf:application/pdf}
}

@article{holmes_bayesian_2006,
	title = {Bayesian auxiliary variable models for binary and multinomial regression},
	volume = {1},
	url = {http://projecteuclid.org/euclid.ba/1340371078},
	number = {1},
	urldate = {2016-09-13},
	journal = {Bayesian analysis},
	author = {Holmes, Chris C. and Held, Leonhard and {others}},
	year = {2006},
	pages = {145--168},
	file = {BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/5BEFI3RP/BayesianAuxiliaryVariableModelsBinaryMultinomialRegression.pdf:application/pdf}
}

@article{michael_generating_1976,
	title = {Generating {Random} {Variates} {Using} {Transformations} with {Multiple} {Roots}},
	volume = {30},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2683801},
	doi = {10.2307/2683801},
	abstract = {The general approach to generating random variates through transformations with multiple roots is discussed. Multinomial probabilities are determined for the selection of the different roots. An application of the general result yields a new and simple technique for the generation of variates from the inverse Gaussian distribution.},
	number = {2},
	urldate = {2016-11-07},
	journal = {The American Statistician},
	author = {Michael, John R. and Schucany, William R. and Haas, Roy W.},
	year = {1976},
	pages = {88--90},
	file = {2683801.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/UJU6WK7Q/2683801.pdf:application/pdf}
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {00401706},
	shorttitle = {Ridge {Regression}},
	url = {http://www.jstor.org/stable/1267351?origin=crossref},
	doi = {10.2307/1267351},
	number = {1},
	urldate = {2015-09-14},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	month = feb,
	year = {1970},
	pages = {55}
}

@article{huang_adaptive_2008,
	title = {Adaptive {Lasso} for sparse high-dimensional regression models},
	url = {http://www.jstor.org/stable/24308572},
	urldate = {2016-09-13},
	journal = {Statistica Sinica},
	author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
	year = {2008},
	pages = {1603--1618},
	file = {AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/SWA3V5BR/AdaptiveLassoSparseHighDimensionalRegressionModels.pdf:application/pdf}
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	number = {1},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	pages = {1--38},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/2NBCF56N/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf:application/pdf}
}

@article{jaakkola_bayesian_2000,
	title = {Bayesian parameter estimation via variational methods},
	volume = {10},
	url = {http://link.springer.com/article/10.1023/A:1008932416310},
	number = {1},
	urldate = {2016-09-13},
	journal = {Statistics and Computing},
	author = {Jaakkola, Tommi S. and Jordan, Michael I.},
	year = {2000},
	pages = {25--37},
	file = {art%3A10.1023%2FA%3A1008932416310.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/UZFCTMZR/art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf;art%3A10.1023%2FA%3A1008932416310.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/VIBNVWR5/art%3A10.1023%2FA%3A1008932416310.pdf:application/pdf}
}

@incollection{bernardo_shrink_2011,
	title = {Shrink {Globally}, {Act} {Locally}: {Sparse} {Bayesian} {Regularization} and {Prediction}*},
	isbn = {978-0-19-969458-7},
	shorttitle = {Shrink {Globally}, {Act} {Locally}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199694587.001.0001/acprof-9780199694587-chapter-17},
	urldate = {2016-10-25},
	booktitle = {Bayesian {Statistics} 9},
	publisher = {Oxford University Press},
	author = {Polson, Nicholas G. and Scott, James G.},
	editor = {Bernardo, José M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
	month = oct,
	year = {2011},
	pages = {501--538},
	file = {Bayes1.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/HQPRWEJJ/Bayes1.pdf:application/pdf}
}

@inproceedings{hinton_keeping_1993,
	address = {New York, NY, USA},
	series = {{COLT} '93},
	title = {Keeping the {Neural} {Networks} {Simple} by {Minimizing} the {Description} {Length} of the {Weights}},
	isbn = {978-0-89791-611-0},
	url = {http://doi.acm.org/10.1145/168304.168306},
	doi = {10.1145/168304.168306},
	urldate = {2016-09-27},
	booktitle = {Proceedings of the {Sixth} {Annual} {Conference} on {Computational} {Learning} {Theory}},
	publisher = {ACM},
	author = {Hinton, Geoffrey E. and van Camp, Drew},
	year = {1993},
	pages = {5--13},
	file = {ACM Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/JZ2AQK6D/Hinton and van Camp - 1993 - Keeping the Neural Networks Simple by Minimizing t.pdf:application/pdf}
}

@article{bergersen_weighted_2011,
	title = {Weighted {Lasso} with {Data} {Integration}},
	volume = {10},
	issn = {1544-6115},
	url = {http://www.degruyter.com/view/j/sagmb.2011.10.issue-1/sagmb.2011.10.1.1703/sagmb.2011.10.1.1703.xml},
	doi = {10.2202/1544-6115.1703},
	number = {1},
	urldate = {2015-09-14},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Bergersen, Linn Cecilie and Glad, Ingrid K. and Lyng, Heidi},
	month = jan,
	year = {2011},
	pages = {1--29},
	file = {WeightedLassoDataIntegration.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/MBI5ZXCI/WeightedLassoDataIntegration.pdf:application/pdf;WeightedLassoDataIntegration.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/THXSADP8/WeightedLassoDataIntegration.pdf:application/pdf}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1369-7412, 1467-9868},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	language = {en},
	number = {2},
	urldate = {2015-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	pages = {301--320}
}

@article{casella_empirical_2001,
	title = {Empirical {Bayes} {Gibbs} sampling},
	volume = {2},
	number = {4},
	journal = {Biostatistics},
	author = {Casella, George},
	year = {2001},
	pages = {485--500},
	file = {Biostat-2001-Casella-485-500.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/38IRWHCP/Biostat-2001-Casella-485-500.pdf:application/pdf}
}

@article{futreal_census_2004,
	title = {A census of human cancer genes},
	volume = {4},
	issn = {1474-175X, 1474-1768},
	url = {http://www.nature.com/doifinder/10.1038/nrc1299},
	doi = {10.1038/nrc1299},
	number = {3},
	urldate = {2016-09-13},
	journal = {Nature Reviews Cancer},
	author = {Futreal, P. Andrew and Coin, Lachlan and Marshall, Mhairi and Down, Thomas and Hubbard, Timothy and Wooster, Richard and Rahman, Nazneen and Stratton, Michael R.},
	month = mar,
	year = {2004},
	pages = {177--183},
	file = {CensusHumanCancerGenes.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/G5TCUQJ2/CensusHumanCancerGenes.pdf:application/pdf;CensusHumanCancerGenes.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/RDX5MCHD/CensusHumanCancerGenes.pdf:application/pdf}
}

@incollection{neal_view_1998,
	series = {{NATO} {ASI} {Series}},
	title = {A {View} of the {Em} {Algorithm} that {Justifies} {Incremental}, {Sparse}, and other {Variants}},
	copyright = {©1998 Springer Science+Business Media Dordrecht},
	isbn = {978-94-010-6104-9 978-94-011-5014-9},
	url = {http://link.springer.com/chapter/10.1007/978-94-011-5014-9_12},
	abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
	language = {en},
	number = {89},
	urldate = {2016-09-27},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Neal, Radford M. and Hinton, Geoffrey E.},
	editor = {Jordan, Michael I.},
	year = {1998},
	note = {DOI: 10.1007/978-94-011-5014-9\_12},
	keywords = {Artificial Intelligence (incl. Robotics), Statistical Physics, Dynamical Systems and Complexity, Statistics, general},
	pages = {355--368},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/R7ZTS7EZ/Neal and Hinton - 1998 - A View of the Em Algorithm that Justifies Incremen.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/RWXGTBBN/10.html:text/html}
}

@article{li_bayesian_2010,
	title = {The {Bayesian} elastic net},
	volume = {5},
	issn = {1936-0975},
	url = {http://projecteuclid.org/euclid.ba/1340369796},
	doi = {10.1214/10-BA506},
	language = {en},
	number = {1},
	urldate = {2015-09-14},
	journal = {Bayesian Analysis},
	author = {Li, Qing and Lin, Nan},
	month = mar,
	year = {2010},
	pages = {151--170},
	file = {BayesianElasticNet.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/X6AEU9NC/BayesianElasticNet.pdf:application/pdf}
}

@incollection{mackay_developments_1995,
	title = {Developments in {Probabilistic} {Modelling} with {Neural} {Networks} — {Ensemble} {Learning}},
	copyright = {©1995 Springer-Verlag London Limited},
	isbn = {978-3-540-19992-2 978-1-4471-3087-1},
	url = {http://link.springer.com/chapter/10.1007/978-1-4471-3087-1_37},
	abstract = {Ensemble learning by variational free energy minimization is a framework for statistical inference in which an ensemble of parameter vectors is optimized rather than a single parameter vector. The ensemble approximates the posterior probability distribution of the parameters. In this paper I give a review of ensemble learning using a simple example.},
	language = {en},
	urldate = {2016-09-26},
	booktitle = {Neural {Networks}: {Artificial} {Intelligence} and {Industrial} {Applications}},
	publisher = {Springer London},
	author = {MacKay, David J. C.},
	editor = {Kappen, Bert and Gielen, Stan},
	year = {1995},
	note = {DOI: 10.1007/978-1-4471-3087-1\_37},
	keywords = {Artificial Intelligence (incl. Robotics), Neurosciences, Pattern Recognition},
	pages = {191--198},
	file = {10.1.1.49.3128.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/DN6UKEP3/10.1.1.49.3128.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/8TJFWCUW/10.html:text/html;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/99ENFQE3/10.html:text/html}
}

@article{zou_adaptive_2009,
	title = {On the adaptive elastic-net with a diverging number of parameters},
	volume = {37},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1245332831},
	doi = {10.1214/08-AOS625},
	language = {en},
	number = {4},
	urldate = {2015-09-14},
	journal = {The Annals of Statistics},
	author = {Zou, Hui and Zhang, Hao Helen},
	month = aug,
	year = {2009},
	pages = {1733--1751},
	file = {AdaptiveElasticNetDivergingNumberParameters.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/WU4JGNGW/AdaptiveElasticNetDivergingNumberParameters.pdf:application/pdf}
}

@article{neuenschwander_summarizing_2010,
	title = {Summarizing historical information on controls in clinical trials},
	volume = {7},
	issn = {1740-7745},
	url = {http://ctj.sagepub.com/cgi/doi/10.1177/1740774509356002},
	doi = {10.1177/1740774509356002},
	language = {en},
	number = {5},
	urldate = {2016-09-13},
	journal = {Clinical Trials},
	author = {Neuenschwander, B. and Capkun-Niggli, G. and Branson, M. and Spiegelhalter, D. J.},
	month = jan,
	year = {2010},
	pages = {5--18},
	file = {SummarizingHistoricalInformationControlsClinicalTrials.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/PZDAHZPA/SummarizingHistoricalInformationControlsClinicalTrials.pdf:application/pdf}
}

@article{leday_gene_2015,
	title = {Gene network reconstruction using global-local shrinkage priors},
	url = {http://arxiv.org/abs/1510.03771},
	urldate = {2016-09-13},
	journal = {arXiv preprint arXiv:1510.03771},
	author = {Leday, Gwenaël GR and de Gunst, Mathisca and Kpogbezan, Gino B. and Van der Vaart, Aad W. and Van Wieringen, Wessel N. and Van de Wiel, Mark A.},
	year = {2015},
	file = {BSEM_draft_AOAS.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/P9J6PMJC/BSEM_draft_AOAS.pdf:application/pdf;BSEM_supp.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/S8DJXMWQ/BSEM_supp.pdf:application/pdf}
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s proportional hazards model},
	volume = {94},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asm037},
	doi = {10.1093/biomet/asm037},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Biometrika},
	author = {Zhang, H. H. and Lu, W.},
	month = aug,
	year = {2007},
	pages = {691--703},
	file = {AdaptiveLassoCoxProportionalHazardsModel.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/ABQT8GA5/AdaptiveLassoCoxProportionalHazardsModel.pdf:application/pdf}
}

@article{shun_laplace_1995,
	title = {Laplace {Approximation} of {High} {Dimensional} {Integrals}},
	volume = {57},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2345941},
	abstract = {It is shown that the usual Laplace approximation is not a valid asymptotic approximation when the dimension of the integral is comparable with the limiting parameter n. The formal Laplace expansion for multidimensional integrals is given and used to construct asymptotic approximations for high dimensional integrals. One example is considered in which the dimension of the integral is O(n$^{\textrm{1/2}}$) and the relative error of the unmodified Laplace approximation is O(1). Nevertheless, it is possible to construct a valid asymptotic expansion by regrouping terms in the formal expansion according to asymptotic order in n.},
	number = {4},
	urldate = {2017-03-17},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Shun, Zhenming and McCullagh, Peter},
	year = {1995},
	pages = {749--760},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/KAI3QZ9V/Shun and McCullagh - 1995 - Laplace Approximation of High Dimensional Integral.pdf:application/pdf}
}

@article{simon_sparse-group_2013,
	title = {A {Sparse}-{Group} {Lasso}},
	volume = {22},
	issn = {1061-8600},
	url = {http://dx.doi.org/10.1080/10618600.2012.681250},
	doi = {10.1080/10618600.2012.681250},
	abstract = {For high-dimensional supervised learning problems, often using problem-specific assumptions can lead to greater accuracy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with ℓ1 and ℓ2 penalties. We discuss the sparsity and other regularization properties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradient descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data. This article has online supplementary material.},
	number = {2},
	urldate = {2016-11-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2013},
	keywords = {Model, Nesterov, Penalize, Regression, Regularize},
	pages = {231--245},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/BJS8SUZI/Simon et al. - 2013 - A Sparse-Group Lasso.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/3FNIU6JK/10618600.2012.html:text/html}
}

@article{atchade_computational_2011,
	title = {A computational framework for empirical {Bayes} inference},
	volume = {21},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/article/10.1007/s11222-010-9182-3},
	doi = {10.1007/s11222-010-9182-3},
	abstract = {In empirical Bayes inference one is typically interested in sampling from the posterior distribution of a parameter with a hyper-parameter set to its maximum likelihood estimate. This is often problematic particularly when the likelihood function of the hyper-parameter is not available in closed form and the posterior distribution is intractable. Previous works have dealt with this problem using a multi-step approach based on the EM algorithm and Markov Chain Monte Carlo (MCMC). We propose a framework based on recent developments in adaptive MCMC, where this problem is addressed more efficiently using a single Monte Carlo run. We discuss the convergence of the algorithm and its connection with the EM algorithm. We apply our algorithm to the Bayesian Lasso of Park and Casella (J. Am. Stat. Assoc. 103:681–686, 2008) and on the empirical Bayes variable selection of George and Foster (J. Am. Stat. Assoc. 87:731–747, 2000).},
	language = {en},
	number = {4},
	urldate = {2017-02-16},
	journal = {Stat Comput},
	author = {Atchadé, Yves F.},
	month = oct,
	year = {2011},
	pages = {463--473},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/NBJKDVNI/Atchadé - 2011 - A computational framework for empirical Bayes infe.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/EBK77VCP/s11222-010-9182-3.html:text/html}
}

@misc{noauthor_rcpp:_nodate,
	title = {Rcpp: {Seamless} {R} and {C}++ {Integration} {\textbar} {Eddelbuettel} {\textbar} {Journal} of {Statistical} {Software}},
	shorttitle = {Rcpp},
	url = {https://www.jstatsoft.org/article/view/v040i08},
	urldate = {2017-03-17},
	file = {Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/SGEHX2T2/v040i08.html:text/html}
}

@book{bishop_pattern_2006,
	address = {New York},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	url = {http://www.springer.com/gp/book/9780387310732},
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the...},
	urldate = {2017-03-14},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	file = {Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/E9DIXD44/9780387310732.html:text/html}
}

@article{rue_approximate_2009,
	title = {Approximate {Bayesian} inference for latent {Gaussian} models by using integrated nested {Laplace} approximations},
	volume = {71},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2008.00700.x/abstract},
	doi = {10.1111/j.1467-9868.2008.00700.x},
	abstract = {Summary.  Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	language = {en},
	number = {2},
	urldate = {2017-03-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	month = apr,
	year = {2009},
	keywords = {Approximate Bayesian inference, Gaussian Markov random fields, Generalized additive mixed models, Laplace approximation, Parallel computing, Sparse matrices, Structured additive regression models},
	pages = {319--392},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/G68767ZE/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/RDU9UF84/abstract.html:text/html}
}

@article{novianti_better_2017,
	title = {Better diagnostic signatures from {RNAseq} data through use of auxiliary co-data},
	volume = {btw837},
	url = {https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btw837/2884246/Better-diagnostic-signatures-from-RNAseq-data},
	doi = {10.1093/bioinformatics/btw837},
	urldate = {2017-03-22},
	journal = {Bioinformatics},
	author = {Novianti, Putri W. and Snoek, Barbara C. and Wilting, Saskia M. and van de Wiel, Mark A.},
	year = {2017},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/G6NX2KVW/Novianti et al. - Better diagnostic signatures from RNAseq data thro.pdf:application/pdf;Novianti_etal_GRridgeApplNote_SuppMat.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/PGZCG5XU/Novianti_etal_GRridgeApplNote_SuppMat.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/GRCRNEMS/btw837.html:text/html}
}

@article{lange_quasi-newton_1995,
	title = {A quasi-{Newton} acceleration of the {EM} algorithm},
	volume = {5},
	url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A5n11.pdf},
	number = {1},
	urldate = {2017-02-07},
	journal = {Statistica Sinica},
	author = {Lange, Kenneth},
	year = {1995},
	pages = {1--18},
	file = {A5n11.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/NX6VV6R2/A5n11.pdf:application/pdf}
}

@inproceedings{wang_inadequacy_2005,
	title = {Inadequacy of interval estimates corresponding to variational {Bayesian} approximations.},
	url = {http://core.ac.uk/download/pdf/22017.pdf#page=382},
	urldate = {2017-03-07},
	booktitle = {{AISTATS}},
	author = {Wang, Bo and Titterington, D. M.},
	year = {2005},
	file = {146.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/Q2IBHA3Q/146.pdf:application/pdf}
}

@article{consonni_mean-field_2007,
	title = {Mean-field variational approximate {Bayesian} inference for latent variable models},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306003951},
	doi = {10.1016/j.csda.2006.10.028},
	abstract = {The ill-posed nature of missing variable models offers a challenging testing ground for new computational techniques. This is the case for the mean-field variational Bayesian inference. The behavior of this approach in the setting of the Bayesian probit model is illustrated. It is shown that the mean-field variational method always underestimates the posterior variance and, that, for small sample sizes, the mean-field variational approximation to the posterior location could be poor.},
	number = {2},
	urldate = {2017-03-07},
	journal = {Computational Statistics \& Data Analysis},
	author = {Consonni, Guido and Marin, Jean-Michel},
	month = oct,
	year = {2007},
	keywords = {Bayesian inference, Bayesian probit model, Gibbs sampling, Latent variable models, Marginal distribution, Mean-field variational methods},
	pages = {790--798},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/9RRQZZAM/Consonni and Marin - 2007 - Mean-field variational approximate Bayesian infere.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/95F884TQ/S0167947306003951.html:text/html}
}

@article{eddelbuettel_rcpp:_2011,
	title = {Rcpp: {Seamless} {R} and {C}++ integration},
	volume = {40},
	shorttitle = {Rcpp},
	url = {http://r.adu.org.za/web/packages/Rcpp/vignettes/Rcpp-introduction.pdf},
	number = {8},
	urldate = {2017-03-17},
	journal = {Journal of Statistical Software},
	author = {Eddelbuettel, Dirk and François, Romain and Allaire, J. and Chambers, John and Bates, Douglas and Ushey, Kevin},
	year = {2011},
	pages = {1--18},
	file = {v40i08.pdf:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/GE5VCFXN/v40i08.pdf:application/pdf}
}

@article{nettleton_convergence_1999,
	title = {Convergence properties of the {EM} algorithm in constrained parameter spaces},
	volume = {27},
	issn = {1708-945X},
	url = {http://onlinelibrary.wiley.com/doi/10.2307/3316118/abstract},
	doi = {10.2307/3316118},
	abstract = {The established general results on convergence properties of the EM algorithm require the sequence of EM parameter estimates to fall in the interior of the parameter space over which the likelihood is being maximized. This paper presents convergence properties of the EM sequence of likelihood values and parameter estimates in constrained parameter spaces for which the sequence of EM parameter estimates may converge to the boundary of the constrained parameter space contained in the interior of the unconstrained parameter space. Examples of the behavior of the EM algorithm applied to such parameter spaces are presented.},
	language = {en},
	number = {3},
	urldate = {2017-06-23},
	journal = {Can J Statistics},
	author = {Nettleton, Dan},
	month = sep,
	year = {1999},
	keywords = {AECM algorithm, ECM algorithm, EM algorithm, GEM algorithm, incomplete data, inequality constraints, order-restricted inference, restricted maximum-likelihood estimate},
	pages = {639--648},
	file = {Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/H4WD87W4/Nettleton - 1999 - Convergence properties of the EM algorithm in cons.pdf:application/pdf;Snapshot:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/483FXF3W/abstract.html:text/html}
}

@article{nettleton_interval_1998,
	title = {Interval {Mapping} of {Quantitative} {Trait} {Loci} through {Order}-{Restricted} {Inference}},
	volume = {54},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2533997},
	doi = {10.2307/2533997},
	abstract = {An order-restricted version of the standard interval mapping procedure is introduced. The usual LOD scores (base 10 logs of likelihood ratios) used in interval mapping are replaced with LOD scores based on restricted likelihood ratio test statistics, which make use of known orderings in the genotype effects at the putative quantitative trait loci (QTL). Simulations demonstrate that individual tests based on the restricted LOD scores can be more powerful than tests based on the standard LOD scores. The new procedure appears to improve QTL detection capability and estimates of both position and genotype effects in certain circumstances. Techniques from order-restricted inference are combined with the EM algorithm to estimate genotype effects and compute the restricted LOD scores.},
	number = {1},
	urldate = {2017-06-23},
	journal = {Biometrics},
	author = {Nettleton, Dan and Praestgaard, Jens},
	year = {1998},
	pages = {74--87},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Library/Application Support/Zotero/Profiles/klfpkvnp.default/zotero/storage/M9TNEMRN/Nettleton and Praestgaard - 1998 - Interval Mapping of Quantitative Trait Loci throug.pdf:application/pdf}
}