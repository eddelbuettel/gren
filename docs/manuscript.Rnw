% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{url,bm,algorithm,algpseudocode,bbm,amsmath,amssymb,amsthm,
amsfonts,graphics,graphicx,epsfig,rotating,caption,subcaption,natbib,appendix,
titlesec,multicol,hyperref,verbatim,threeparttable,booktabs,dsfont,parskip}

% Add history information for the article if required
%\history{Received August 1, 2010;
%	revised October 1, 2010;
%	accepted for publication November 1, 2010}

% objects
\newcommand{\x}{\mathbf{x}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\m}{\mathbf{m}}

% functions and operators
\newcommand{\tr}{^{\text{T}}}
\newcommand{\expit}{\text{expit}}
\newcommand{\argmax}{\text{argmax} \,}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\trace}{\text{tr}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% settings
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Adaptive group-regularized logistic elastic net regression}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Carel F.W. Peeters$^{1}$, 
Aad W. van der Vaart$^{2}$, and \\ Mark A. van de Wiel$^{1,3}$}

\begin{document}
	
	\maketitle

	\noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health
	research institute, Amsterdam University medical centers, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands \\*
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\*
	3. MRC Biostatistics Unit, Cambridge University, Cambridge, United Kingdom

	\begin{abstract}
		{In high-dimensional data settings, additional information on the features 
		is often available. Examples of such external information in omics research
		are: (a) $p$-values from a previous study, (b) a summary of prior 
		information, and (c) omics annotation. The inclusion of this information in
		the analysis may enhance classification performance and feature selection,
		but is not straightforward in the standard regression setting. As a solution
		to this problem, we propose a group-regularized (logistic) elastic net
		regression method, where each penalty parameter corresponds to a group of
		features based on the external information. The method, termed 
		\texttt{gren}, makes use of the Bayesian formulation of logistic elastic net
		regression to estimate both the model and penalty parameters in an 
		approximate empirical-variational Bayes framework. Simulations and an
		application to a colon cancer microRNA study show that, if the partitioning
		of the features is informative, classification performance and feature
		selection are indeed enhanced.}
	\end{abstract}

	\noindent\textbf{Keywords}: Empirical Bayes; High-dimensional data; 
	Prediction; Variational Bayes

	\noindent\textbf{Software available from}: 
	\url{https://CRAN.R-project.org/package=gren}

	\section{Introduction}\label{sec:introduction}
	Prediction from high-dimensional data is a challenge common to many fields of
	research. Examples of such high-dimensional prediction problems arise in
	computer vision, stock market prediction and disease diagnosis from omics 
	data. In this paper, we specifically focus on the latter setting. High
	dimensionality of data introduces several issues in the estimation of 
	prediction models, especially: (a) unidentifiable models, (b) highly 
	correlated predictor variables and (c) non-trivial variable selection.

	Attempts to tackle one or more of these issues have lead to the development of
	many prediction methods \cite[]{fan_selective_2010}. Here we will focus on the
	penalization approach. A well-known and popular penalization method is elastic
	net regression \cite[]{zou_regularization_2005}, with its special cases ridge
	regression \cite[]{hoerl_ridge_1970} and lasso regression 
	\cite[]{tibshirani_regression_1996}. The elastic net approach takes issues 
	(a)-(c) into account and has yielded many successful extensions. Recent work 
	on elastic net regression (and its special cases) has focussed on increasing
	prediction accuracy by the inclusion of prior knowledge on the variables
	\cite[]{lee_variable_2017,van_de_wiel_better_2016,tai_incorporating_2007}.

	In omics research, sources of such prior knowledge on the variables are often
	available. The information can, for example, come in the form of (a) results 
	on the same molecular features obtained in a previous study (e.g. $p$-values),
	(b) information from a publicly available database that summarizes the prior
	information on the molecular features involved (e.g., the Cancer Gene Census
	\cite[]{futreal_census_2004}), and (c) omics annotation (e.g. the location of 
	a gene on the chromosome). Although this information can rarely be directly
	included in the statistical analysis, it may still be useful and informative
	for the study at hand.

	Two such studies are introduced in Sections \ref{sec:colorectal} and
	\ref{sec:lymphnode}. The first study is concerned with treatment 
	response prediction for colorectal cancer patients, based on sequenced
	microRNAs. In the second study we attempt to predict lymph node metastases,
	using RNAseq data. In both settings additional information on the 
	features is available in the form of a partitioning. Furthermore, in both
	examples data is sparse, so any extra included information may help to boost
	predictive performance. Inclusion of such feature partitions in 
	the analysis is not straightforward with standard methods. 
	
	At first sight, such an \textit{a priori} grouping of the features suggests 
	the group lasso
	\cite[]{meier_group_2008, yuan_model_2006} or one of its extensions, because
	the group lasso penalises and selects features at the group level. 
	There are two limitations to this approach: the group lasso (a) selects groups
	as a whole instead of single 
	features and (b) does not penalise adaptively: all groups are penalised 
	equally. Extensions such as the sparse group lasso 
	\cite[]{simon_sparse-group_2013} partly deal with (a), but do not address (b).

	A natural way to deal with (b) is through differential penalization: that is,
	each group of variables receives its own penalty parameter. An apparent issue
	with this differential penalization is the estimation of the penalty 
	parameters. Naive estimation may be done by cross-validation. However, 
	cross-validation requires re-estimation of the model over a grid of penalty
	parameters. The size of this grid increases exponentially with the number of
	penalty parameters. Consequently, it quickly becomes computationally 
	infeasible. We therefore propose an efficient alternative: empirical Bayes
	estimation of the penalty parameters, which corresponds to hyperparameter
	estimation in the Bayesian prior framework. Because of the ubiquity of binary
	outcome data in omics research, we focus on the logistic elastic net. 
	
	Recently, \cite{zhang_novel_2018} introduced VBVS, a method similar to ours in
	spirit. Nevertheless, one clear advantage of our method over VBVS is the
	possibility of adaptive inclusion of external information on the features to
	aid in prediction and feature selection. Bayesian versions of support vector 
	machines have been used in classification problems as well 
	\cite[]{chakraborty_bayesian_2011}, but just as VBVS, these methods lack 
	adaptive inclusion of external information on the features. We emphasize that 
	our method is different from recent developments in grouped penalized 
	regression such as \cite{dondelinger_joint_nodate}, where instead of the 
	features, the observations are partitioned.

	In line with the above, our proposed method is (a) subjective: the 
	hyper-parameters are estimated from the data and not chosen on objective 
	grounds, (b) adaptive: prior information is automatically and subjectively 
	weighed with respect to its informativeness, (c) fast compared to full 
	Bayesian analysis or cross-validation, and (d) requires minimal input by the 
	user: only the data and grouping of the features is required.

	The rest of the paper is structured as follows. We introduce the Bayesian 
	logistic generalised elastic net model in Section
	\ref{sec:model}. In Section \ref{seq:estimation} we derive a variational
	approximation to this model and use this novel approximation in the empirical
	Bayes estimation of multiple, group-specific penalty parameters. In Section
	\ref{sec:simulations} we compare the method in a simulation study. In Sections
	\ref{sec:colorectal} and \ref{sec:lymphnode}, we demonstrate the approach in 
	the two previously introduced data sets. In Section \ref{sec:discussion} 
	we conclude by both discussing some differences and
	parallels between the proposed method, termed \texttt{gren}, and other methods
	in the literature.

	\section{Model}\label{sec:model}
	In logistic regression the outcome variables are assumed to be binary or sums
	of $m_i$ disjoint binary Bernoulli trials ($y_i = \sum_{l=1}^{m_i} k_l, k_l 
	\in \{0,1\}$ for $i=1, \dots, n$). The binomial logistic model relates the
	responses to the $p$-dimensional covariate vectors $\x_i = \begin{bmatrix}
	x_{i1} & \cdots & x_{ip}\end{bmatrix} \tr$ through $y_i \sim \mathcal{B} 
	\( m_i, \expit ( \x_i\tr \bbeta ) \)$, where $\mathcal{B} (m,\upsilon)$ is 
	the binomial distribution with number of trials $m$ and probability 
	$\upsilon$, and $\expit \( \x_i\tr \bbeta \) = \exp(\x\tr_i \bbeta)/[1 + 
	\exp(\x\tr_i \bbeta)]$. Note that if $m_i=1$ for $i=1, \dots, n$, the model
	reduces to a binary logistic regression model. Throughout the rest of the 
	paper we assume that the model matrix $\X = \begin{bmatrix} \x_1 & \cdots &
	\x_n \end{bmatrix}\tr$ is standardized such that $\frac{1}{n}\sum_{i=1}^n
	x_{ij}=0$ and $\frac{1}{n}\sum_{i=1}^n x_{ij}^2=1$ for $j=1, \dots, p$.

	Assume we have a partitioning of the features in $G$ groups, such that each
	feature belongs to one group. Let $\G(g)$ be the feature index set of group 
	$g$ for $g=1, \dots, G$ and let $w_g \in \mathbb{R}_{>0}$ denote a 
	group-specific weight. In generalised elastic net regression, the penalised 
	likelihood is maximised to yield parameter estimates:
	\begin{align}
	  \hat{\bbeta} & := \underset{\bbeta}{\argmax} \log \L(\y ; \bbeta) - 
	  \frac{\lambda_1}{2} \sum_{g=1}^G \sum_{j \in \G(g)} |w_g \cdot \beta_j| - 
	  \frac{\lambda_2}{2} \sum_{g=1}^G \sum_{j \in \G(g)}\( w_g \cdot \beta_j\)^2 
	  \label{eq:weightedlik}\\
	  & = \underset{\bbeta}{\argmax} \log \L(\y ; \bbeta) -  \frac{\lambda_1}{2} 
	  \sum_{g=1}^G \sqrt{\lambda'_g} \sum_{j \in \G(g)} | \beta_j| - 
	  \frac{\lambda_2}{2} \sum_{g=1}^G \lambda'_g \sum_{j \in \G(g)} \beta_j^2, 
	  \nonumber
	\end{align}
	where $\L(\y ; \bbeta)$ denotes the likelihood function of $\bbeta$ given the 
	observed data $\y = \begin{bmatrix} 
	y_1 & \cdots & y_n \end{bmatrix} \tr$, and 
	$\lambda_1, \lambda_2 \in \mathbb{R}_{>0}$ are the `global' penalty 
	parameters. We wrote $w^2_g = \lambda'_g$ to emphasise that these 
	group-specific weights may be interpreted as penalty multipliers. Note that
	the regular elastic net is recovered by setting $\forall g: \lambda'_g=1$.
	
  Throughout the following we assume that the geometric mean of the multipliers, 
	weighted by their 
	respective group sizes, is one, such that the average shrinkage of the model
	parameters is determined by $\lambda_1$ and $\lambda_2$. That is,
	we calibrate the $\lambda'_g$ such that $\prod_{g=1}^G 
	(\lambda'_g)^{|\G(g)|}=1$. The multiplier appears in square root form in the
	$L_1$-norm term to ensure that penalisation on the parameter level is the same
	for the $L_1$ and $L_2$-norm terms, as can be seen from the parametrisation in
	(\ref{eq:weightedlik}). The elastic net combines $L_1$ and $L_2$-norm 
	penalisation such that the model parameters are shrunken towards zero. 
	The $L_1$-norm may set some of the estimates exactly to zero, thus 
	automatically selecting features. The $L_2$-norm ensures that correlated 
	features behave similarly, as one would generally require. 
	
	The maximiser of the penalised likelihood in the elastic net, corresponds 
	to the posterior mode of a 
	Bayesian elastic model \cite[]{li_bayesian_2010,zou_regularization_2005}.
	\cite{li_bayesian_2010} show that the elastic net prior 
	(see SM Section [! section reference here] for details on this prior) may be 
	written as
	a computationally more convenient scale mixture of normals, with mixing 
	parameter $\btau = \begin{bmatrix} \tau_1 & \cdots & \tau_p \end{bmatrix} 
	\tr$. Using this result, we write the generalised elastic net model in its 
	Bayesian form as:
	\begin{subequations}\label{eq:enmodel2}
		\begin{align}
		\y| \bbeta & \sim \prod_{i=1}^n \mathcal{B} \left( m_i, \expit ( \x\tr_i 
		\bbeta ) \right),  \\
		\bbeta | \btau & \sim \prod_{g=1}^G \prod_{j \in \G(g)} \mathcal{N} \left(0
		,\frac{1}{\lambda' _g \lambda_2} \frac{\tau_j - 1}{\tau_j} \right), \\
		\btau & \sim \prod_{j=1}^p \mathcal{TG} \left( \frac{1}{2},\frac{8 \lambda_2
		}{\lambda_1^2}, \left(1,\infty \right) \right).
		\end{align}
	\end{subequations}
	Here, $\mathcal{TG} \( k,\theta,\( x_l,x_u\) \)$ denotes the truncated gamma 
	distribution with shape $k$, scale $\theta$, and domain $\(x_l,x_u\)$. In this
	Bayesian formulation the penalty parameters $\blambda = \begin{bmatrix} 
	\lambda_1 & \lambda_2 & \lambda'_1 & \cdots & \lambda'_G \end{bmatrix} \tr$ 
	play the role of the hyperparameters in a Bayesian hierarchical model.

	\section{Estimation}\label{seq:estimation}
	\subsection{Empirical Bayes}\label{seq:empiricalbayes}
	If the penalty parameters are known, estimation of the frequentist elastic net
	model parameters or finding the posterior of the generalized elastic net model
	is feasible with small adjustments of the available algorithms 
	\cite[]{friedman_regularization_2010,zou_regularization_2005} or MCMC samplers
	\cite[]{li_bayesian_2010}. Determining these penalty parameters, however, is 
	not straightforward.

	In the frequentist elastic net without group-wise penalisation, two main 
	strategies are used: (i) estimate both $\lambda_1$ and $\lambda_2$ by 
	cross-validation over a two-dimensional grid of values or (ii) re-parametrise 
	the problem in terms of penalty parameters $\alpha= \frac{\lambda_1}{2 
	\lambda_2 + \lambda_1}$ and $\lambda = 2 \lambda_2 + \lambda_1$, fix the 
	proportion of $L_1$-norm penalty $\alpha$ and cross-validate the global 
	penalty parameter $\lambda$. Strategy (i) is advised by 
	\cite{waldron_optimized_2011}, while (ii) is proposed by 
	\cite{friedman_regularization_2010}. In the generalised elastic net setting, 
	strategies (i) and (ii) imply $2 + G$ and $1 + G$ penalty parameters, 
	respectively. $K$-fold cross validation over $D$ values then results in $K 
	\cdot D^{2 + G}$ and $K \cdot D^{1 + G}$ models to estimate. Typically $K$ is 
	set to 5, 10, or to the number of samples $n$, while $D$ is in the order of 
	$100$, so that even for small $G$, the number of models to estimate is 
	prohibitively large.

	In the Bayesian framework, estimation of penalty parameters may be avoided by 
	the addition of a hyperprior to the model hierarchy. The hyperprior takes the 
	uncertainty in the penalty parameters into account by integrating over them. 
	This approach introduces two issues. Firstly, the choice of hyperprior is not 
	straightforward. Many authors suggest a hyperprior from the gamma family of 
	distributions 
	\cite[]{alhamzawi_bayesian_2017,mallick_bayesian_2013,kyung_penalized_2010}, 
	but the precise parametrisation of this gamma prior is not so obvious. A 
	second issue is the loss of correspondence between the  Bayesian and 
	frequentist elastic net. If the ultimate goal is feature selection, this 
	correspondence may be exploited through the automatic feature selection 
	property of the frequentist elastic net. Endowing the penalty parameters with 
	a hyperprior obstructs their point estimation and, consequently, impedes 
	automatic feature selection. Therefore, to circumvent the problem of 
	hyperprior choice and allow for feature selection by the frequentist elastic 
	net, we propose to estimate the penalty parameters by empirical Bayes.

	Many forms of empirical Bayes exist, the most formal one being maximisation of
	the marginal likelihood with respect to the hyperparameters. The resulting 
	hyperparameter estimates are then plugged into the prior. The marginal 
	likelihood is often introduced as a measure of model evidence given the 
	observed data and is computed by integrating the product of likelihood and 
	prior with respect to the model parameters. In the case of the elastic net 
	introduced in (\ref{eq:enmodel2}) this results in the following marginal 
	empirical Bayes posterior for $\bbeta$:
	\begin{align}
	  p_{\hat{\blambda}}(\bbeta | \y) & = \frac{\L (\y ; \bbeta) 
	  \pi_{\hat{\blambda}} (\bbeta) }{p_{\hat{\blambda}}(\y)} = 
	  \frac{\int_{\btau} \L (\y ; \bbeta) 
	  \pi_{\hat{\blambda}}(\bbeta | \btau) \pi_{\hat{\blambda}}(\btau) \,
	  d\btau}{p_{\hat{\blambda}}(\y)}, \nonumber \\
	  \hat{\blambda} & := \underset{\blambda}{\argmax} \log p_{\blambda} (\y) = 
	  \underset{\blambda}{\argmax} \int_{\bbeta} \int_{\btau} \L_{\blambda}(\y, 
	  \bbeta, \btau) \, d\bbeta d\btau \nonumber \\
	  & = \underset{\blambda}{\argmax} \int_{\bbeta} \int_{\btau} \L (\y ; \bbeta)
	  \pi_{\blambda}(\bbeta | \btau) \pi_{\blambda}(\btau) \, d\bbeta d\btau. 
	  \label{eq:mmlestimate}
	\end{align}
	The integrals in (\ref{eq:mmlestimate}) are intractable in the case of the 
	elastic net. In the omics setting the integrals are also high-dimensional, in 
	which case numerical and Monte Carlo approximation methods become tedious and 
	computationally expensive. Moreover, Laplace approximation is known to suffer 
	from low accuracy in many high-dimensional settings 
	\cite[]{shun_laplace_1995}. In \cite{casella_empirical_2001} an EM algorithm 
	is described that estimates the hyperparameters. This EM algorithm iteratively
	maximises the expected joint log likelihood, such that the sequence:
	\begin{equation}\label{eq:ebEM}
	\blambda^{(k + 1)} = \underset{\blambda}{\argmax} \E_{\bbeta, \btau | \y} \[ 
	\log \L_{\blambda}(\y, \bbeta, \btau) | \blambda^{(k)} \]
	\end{equation}
	converges to a local maximum of the marginal likelihood. The difficulty herein
	is in the calculation of the expected joint log likelihood. 
	\cite{casella_empirical_2001} suggests to approximate the expectation by its 
	Monte Carlo expectation. Although elegant and simple, this method requires a 
	converged MCMC sample from the posterior for every iteration: a 
	computationally intensive procedure. \cite{roy_selection_2017} introduce
	generalised importance sampling for the Bayesian elastic net, such that we
	just need a limited, pre-specified number of MCMC chains. However, this still
	requires several converged chains, so is not feasible in many high-dimensional
	omics settings. We propose to tackle this problem by approximating the
	expectation in (\ref{eq:ebEM}) using variational Bayes.

	\subsection{Variational Bayes}\label{sec:variationalbayes}
	Variational Bayes is a widely used method to approximate Bayesian posteriors. 
	It has successfully been applied in a wide range of applications, including 
	genetic association studies \cite[]{carbonetto_scalable_2012} and gene network
	reconstruction \cite[]{leday_gene_2017}. In variational Bayes, the posterior 
	is approximated by a tractable form and estimated by optimizing a lower bound 
	on the marginal likelihood of this model (see Section [! section ref here] 
	of the SM for the 
	lower bound of the proposed model). For an extensive introduction and concise 
	review, see \cite{beal_variational_2003} and \cite{blei_variational_2017}, 
	respectively.

	To simplify the computations of our variational approximation, we follow
	\cite{polson_bayesian_2013} and introduce latent variables $\omega_i$, for 
	$i=1, \dots, n$. Conditional on $\bbeta$, the $\omega_i$ are independent of
	the $y_i$ and P\'{o}lya-Gamma distributed (see Section 4 of the SM for more 
	details). We augment Model (\ref{eq:enmodel2}) with:
	\begin{equation}\label{eq:logmodel}
	  \bomega | \bbeta \sim \prod_{i=1}^n \mathcal{PG}\(m_i, |\x_i \tr \bbeta| \).
	\end{equation}
	Our variational Bayes approximation to the posterior distribution of 
	(\ref{eq:enmodel2}) and (\ref{eq:logmodel}) factorizes over blocks of 
	parameters. We choose the blocks such that:
	\begin{equation}\label{eq:varbayesapproximation}
	  p (\bomega, \bbeta, \btau | \y) \approx Q(\bomega, \bbeta, \btau) 
	  = q_{\bomega} (\bomega) q_{\bbeta} (\bbeta) q_{\btau} (\btau).
	\end{equation}
	Writing $\btheta_1 = \bomega$, $\btheta_2 = \bbeta$, $\btheta_3 = \btau$, and 
	$\btheta = \begin{bmatrix} \btheta_1 & \btheta_2 & \btheta_3 \end{bmatrix}$, 
	calculus of variations gives the optimal distributions 
	$q^*_{\btheta_j} (\btheta_j) \propto \exp \{\E_{\btheta \backslash \btheta_j} 
	[\log p (\btheta | \y)]\}$, where optimality is achieved in terms of the 
	Kullback-Leibler divergence of the posterior to the approximate distribution 
	\cite[]{neal_view_1998}. The approximation in (\ref{eq:varbayesapproximation})
	renders both the posterior parameter calculations and the expected joint log 
	likelihood as introduced in (\ref{eq:ebEM}) tractable.

	After a change of variables $\psi_j = \tau_j - 1$, we find the optimal 
	distributions in our variational Bayes implementation for the model parameters
	as:
	\begin{equation}\label{eq:varbayesdistr}
	  q^*_{\bbeta} (\bbeta) \sim \mathcal{N} (\bmu, \bSigma) \text{, } 
	  q^*_{\bomega} (\bomega) \sim \prod_{i=1}^n \mathcal{PG} (m_i, c_i)
	  \text{, and } q^*_{\bpsi}(\bpsi) \sim \prod_{j=1}^p \mathcal{GIG} 
	  \(\frac{1}{2}, \frac{\lambda_1^2}{4 \lambda_2}, \chi_j\),
	\end{equation}
	where $\mathcal{GIG} (\cdot)$ denotes the generalized inverse Gaussian 
	distribution (See SM Section 5 for the derivations). The so-called variational
	parameters in (\ref{eq:varbayesdistr}) contain cyclic dependencies, so we 
	update them by:
	\begin{subequations}\label{eq:VBupdateequations}
		\begin{align}
		  \bSigma^{(t + 1)} &= \( \X \tr \bOmega^{(t)} \X + \lambda_2 \bLambda' + 
		  \frac{\lambda_1 \sqrt{\lambda_2}}{2} \bLambda' \Z^{(t)}\)^{-1}, \\ 
		  & \text{with } \bOmega^{(t)} = \diag\[ \(\frac{m_i}{2 c_i^{(t)}}\) \tanh 
		  \( \frac{c_i^{(t)}}{2} \)\] \text{ and } \Z^{(t)}=
		  \diag\[(\chi_j^{(t)})^{-1/2}\], \nonumber \\
		  \bmu^{(t + 1)} &= \bSigma^{(t + 1)} \X \tr (\y - \m/2), \\
		  c_i^{(t + 1)} &= \sqrt{\x_i \tr \bSigma^{(t + 1)} \x_i + 
		  (\x_i \tr \bmu^{(t + 1)})^2} \text{, for } i=1, \dots, n, \\
		  \chi_j^{(t + 1)} &= \lambda'_{g(j)} \lambda_2 \[\bSigma^{(t + 1)}_{jj} + 
		  (\bmu^{(t + 1)}_j)^2 \] \text{, for } j=1, \dots, p,
		\end{align}
	\end{subequations}
	until convergence. Here, $\bLambda'$ is a diagonal matrix with entries 
	$\lambda'_g$, each repeated $|\G(g)|$ times and $\m = \begin{bmatrix} m_1 & 
	\cdots & m_n \end{bmatrix} \tr$. Furthermore, $\mathbf{A}_{jj}$ and 
	$\mathbf{a}_j$ denote the $j$th diagonal element of a square matrix and $j$th
	element of a column vector, respectively. Naive calculation of the variational
	parameters is computationally expensive. In Section 6 of the SM we show that 
	informed calculation of the parameters results in a significant reduction of 
	computational complexity.

	\subsection{Empirical-variational Bayes}\label{seq:empvarbayes}
	Variational Bayes was shown to underestimate the posterior variance of the 
	parameters, both numerically and theoretically, in several settings 
	\cite[]{rue_approximate_2009,consonni_mean-field_2007,bishop_pattern_2006,
	wang_inadequacy_2005}. This coincides with our experience that the global 
	penalty parameters $\lambda_1$ and $\lambda_2$ tend to be overestimated. To 
	prevent overestimation we use the parametrisation of the elastic net in 
	\cite{friedman_regularization_2010} as introduced in Section 
	\ref{seq:empiricalbayes}: we fix $\alpha$ and estimate $\lambda$ by 
	cross-validation of the regular elastic net model, such that the overall 
	penalisation is determined by cross-validation of only $\lambda$. By combining
	cross-validation of the global penalty parameter $\lambda$ with empirical 
	Bayes estimation of the penalty multipliers $\blambda' = \begin{bmatrix} 
	\lambda'_1 & \cdots & \lambda'_G \end{bmatrix} \tr$, the estimation is more 
	robust to underestimation of the variational posterior variances. The 
	remaining issue is the choice of $\alpha$. \cite{hastie_glmnet_2016} recommend
	to either fix $\alpha$ \textit{a priori}, or compare the results for several 
	choices of $\alpha$. We recommend the latter.

	To estimate the penalty multipliers, the intractable posterior expectation
	in (\ref{eq:ebEM}) is approximated using the variational posterior:
	$$
	\E_{Q} \[ \log \L_{\blambda'}(\y, \bomega, \bbeta, \btau) | \blambda'^{(k)} \]
	= \frac{1}{2} \sum_{g=1}^G |\G(g)| \log (\lambda'_g) - \frac{(1 - \alpha) 
	\lambda}{4} \sum_{g=1}^G \lambda'_g d^{(k)}_g + C,
	$$
	where $C$ is constant in $\blambda'$ (see SM Section 7 for the full 
	derivation). The $d_g^{(k)}$ terms are calculated as: $d^{(k)}_g = \sum_{j 
	\in \mathcal{G} (g)} \[\bSigma^{(k)}_{jj} + (\bmu^{(k)}_j)^2\]\(1 + 
	\alpha \lambda^{1.5} \sqrt{\frac{1 - \alpha}{8 \chi_j^{(k)}}}\)$. An estimate
	of the new penalty multipliers is now given by:
	\begin{subequations}\label{eq:mmlupdateequation}
		\begin{align}
		  \blambda'^{(k+1)} & = \underset{\blambda'}{\argmax} \frac{1}{2} 
		  \sum_{g=1}^G |\G(g)| \log (\lambda'_g) - \frac{(1 - \alpha) \lambda}{4} 
		  \sum_{g=1}^G \lambda'_g d^{(k)}_g \\
		  & \text{subject to } \prod_{g=1}^G (\lambda'_g)^{|\G(g)|} = 1.
		\end{align}
	\end{subequations}
	Although the solution to (\ref{eq:mmlupdateequation}) is not available in
	closed form, this convex problem is easily solved by a numerical optimisation 
	routine. The full procedure is summarized in Section 8 of the SM.

	\subsection{Feature selection}
	Feature selection is often desirable in high-dimensional prediction problems 
	and omics data problems are no exception. For example, a selection of 
	biomarkers may lead 
	to a large cost reduction by supporting targeted assays. Bayesian feature
	selection is often done by inspection of posterior credible intervals. 
	Similarly as in frequentist hypothesis testing, we may select a feature if 
	zero is not contained in the credible interval. However, the Bayesian lasso's
	credible intervals (a special case of the elastic net) are known to suffer 
	from low frequentist coverage in sparse settings 
	\cite[]{castillo_bayesian_2015}. We often assume sparsity of the features in 
	omics research, so selection by credible intervals is generally inconsistent. 
	We therefore propose to select features in the frequentist paradigm.

	As shortly touched upon in Section \ref{seq:empiricalbayes}, frequentist 
	feature selection is trivial after estimation of the penalty multipliers. We 
	simply plug the estimated penalty parameters into some frequentist elastic net
	algorithm that allows for differential penalization. In our own package 
	\texttt{gren}, we use the \texttt{R}-package \texttt{glmnet} 
	\cite[]{friedman_regularization_2010}. In the frequentist elastic net, feature
	selection is then done automatically. Furthermore, to select a specific number
	of features, we simply adjust the global $\lambda$ until we select the desired
	number of features.

	\section{Simulations}\label{sec:simulations}
	To assess variable selection and predictive performance we conducted a 
	simulation study in which we compare \texttt{gren} to the regular elastic 
	net and ridge models, \texttt{GRridge} \cite[]{van_de_wiel_better_2016}, 
	sparse group lasso (\texttt{SGL}) \cite[]{simon_sparse-group_2013}, 
	group mimimax concave penalty (\texttt{grMCP}) 
	\cite[]{breheny_penalized_2009}, and the group exponential lasso 
	(\texttt{gel}) \cite[]{breheny_group_2015}. \texttt{GRridge} is similar to 
	\texttt{gren} in the sense that it estimates group-specific penalty 
	multipliers. The two main differences with \texttt{gren} are (i) the absence 
	of an $L_1$-norm penalty and (ii) the estimation procedure. The other methods 
	are extensions of the group lasso \cite[]{meier_group_2008, yuan_model_2006} 
	and not adaptive on the group level. In contrast to the original group lasso,
	all of them are able to select single features, instead of just on the group 
	level.

	For the regular elastic net, \texttt{gren}, \texttt{SGL}, \texttt{grMCP}, 
	and \texttt{gel}, we fix $\alpha \in \{0.05, 0.5, 0.95 \}$. In \texttt{gren}
	and the elastic net, $\alpha$ determines the proportion of $L_1$-norm penalty,
	such that the first setting closely resembles the ridge setting, 
	where $\alpha=0$, while the third setting is similar to the lasso with 
	$\alpha=1$. In \texttt{SGL}, \texttt{grMCP}, and \texttt{gel}, $\alpha$
	determines the proportion of group-level penalization ($\alpha=0$) compared to
	$L_1$-norm penalty on the feature level ($\alpha=1$). For the methods that do 
	automatic variable selection, we estimate 
	the models for a range of model sizes.

	We simulated data according to five different scenarios:
	\begin{enumerate}
		\item differential signal between the groups and uniformly distributed model
		parameters,
		\item a large number of small groups of features,
		\item no differential signal between the groups, but strong correlations 
		within groups of features,
		\item differential signal between the groups and heavy-tailed distributed 
		model parameters, and
		\item a very sparse setting with no signal in some of the groups.
	\end{enumerate}
	Full descriptions of the scenarios are given in Section [! ref to section] of 
	the SM. In the first scenario, the distribution of the model parameters is far
	off from the assumed ridge/elastic net/lasso distributions, so we assume that 
	all methods will underperform. In the second scenario, we expect \texttt{gren}
	and \texttt{GRridge} to overfit due to the large number of hyperparameters to 
	estimate. The rest of the methods are conjectured to perform better, due to a
	lack of overfitting. In scenario three, we expect all methods to perform badly
	due to the high correlation between the features. The \texttt{gren} fourth 
	scenario is conjectured to give a better \texttt{gren} performance, due to the
	correctly assumed heavier-tailed distribution and differential signal between
	the groups. Scenario five is expected to lead to a bad performance in all 
	models, because of the sparseness of the truth and lack of differential 
	signal.

	In all scenarios we created a training set of $n=100$ samples to estimate the 
	models and a test set of $n_{\text{test}}=1000$ samples on which we compute 
	the performance measures. The features $\x_i$, $i=1, \dots, n$, are taken 
	from a $p$-dimensional zero-centred Gaussian distribution, where we set 
	$p=1000$. We let the covariance matrix of the features vary over the 
	scenarios. Likewise for the model parameters $\beta_j$, $j=1, \dots, p$. We 
	simulate the outcomes $y_i$, $i=1, \dots, n$, from a binary logistic model: 
	$y_i \sim \mathcal{B}(1, \expit (\x_i \tr \bbeta))$. To mitigate the influence
	of random variation in the simulations, we repeated every scenario 100 times 
	and report estimated penalty parameters and predictive performance measures 
	area under the receiver operator curve (AUC) and Brier skill score (BSS). 
	
	[! results here]
	
	Additional results are presented in Section [! section ref here] of the SM.

	\section{Application to microRNAs in colorectal cancer}
	\label{sec:colorectal}
  As shortly touched upon in Section \ref{sec:introduction}, the data is from a 
  deep sequencing analysis on microRNAs 
  \cite[]{neerincx_combination_2018}. This study in 88 treated 
  colorectal cancer patients was conducted with the aim of predicting treatment 
  response, coded as either non-progressive/remission (70 patients) or 
  progressive 
  (18 patients). After pre-processing and normalisation, 2114 microRNAs 
  remained. In addition to the 2114 microRNAs, four clinical unpenalized 
  covariates were included in the analysis: prior use of adjuvant therapy 
  (binary), the type of systemic treatment regimen (ternary), age, and primary 
  tumor differentiation (binary).

  In a preliminary experiment on different subjects, the microRNA expression 
  levels of metastatic colorectal tumour tissue were compared to normal 
  non-colorectal tissue and primary colorectal tumour tissue was compared to
  primary colorectal tumour tissue \cite[]{neerincx_mir_2015}. This yielded 127
  microRNAs that were differentially expressed in both comparisons with
  $\text{FDR} \leq 0.001$, 95 with and $\text{FDR} \leq 0.05$, and 1893 not 
  differentially expressed microRNAs. We expect that incorporation of this 
  partitioning enhances therapy response 
  classification, because tumor-specific miRNAs might be more relevant than 
  non-specific ones.

  We compared the performance of \texttt{gren} to ridge, \texttt{GRridge}, 
  elastic net, and to the best performing
  group lasso extension, the composite minimax
  concave penalty (\texttt{cMCP}) by \cite{breheny_penalized_2009}. 
  For the methods with an extra tuning parameter 
  $\alpha$, we picked the one (or two) that gave the best performance and refer
  the reader to SM Section [! section ref here] for more comparisons with 
  different $\alpha$s and comparisons to the sparse
  group lasso by \cite{simon_sparse-group_2013} and group
  exponential lasso \cite[]{breheny_group_2015}.
  
  To estimate performance, we split the data into 61 training and 27 test 
  instances, stratified by treatment response. We estimated the models on the 
  training data and calculated AUC on the test data. We present AUC for a range 
  of model sizes, together with the estimated 
  penalty multipliers for \texttt{gren} and \texttt{GRridge} in Figure
  \ref{fig:barplot_lines_micrornaseq_colorectal_cancer_res2_auc}. More 
  performance measures are presented in Section [! section ref here]
  of the SM. In addition, we investigated the sensitivity of
  the multiplier estimation using in SM Section 
  [! section ref here]. We experienced numerical issues with 
  \texttt{GRridge} when cross-validating 
  the global penalty parameter on the training data. We therefore estimated this
  global parameter on the full data for \texttt{GRridge}.
  
  The penalty multipliers estimated by 
  \texttt{gren} and \texttt{GRridge} are according to expectation: the highly
  expressed group receives the lowest penalty, followed by the medium expressed
  group, while the non-expressed group receives the strongest penalty 
  (Figure \ref{fig:barplot_lines_micrornaseq_colorectal_cancer_res2_auc}a). 
  \texttt{gren} seems to outperform the other methods for a range of model 
  sizes in terms of AUC 
  (Figure \ref{fig:barplot_lines_micrornaseq_colorectal_cancer_res2_auc}b). 
  This is especially pronounced for the larger models. The performance of
  \texttt{cMCP} is somewhat unstable. We noticed that this is in general the 
  case for the group-lasso and its extensions (see SM [! section ref here]). 
  Overall, it seems as though the inclusion of the extra information on the 
  features benefited predictive performance: both \texttt{GRridge} and 
  \texttt{gren} outperform their respective non-group-regularized versions, 
  albeit it only slightly for \texttt{GRridge}.
<<barplot_lines_micrornaseq_colorectal_cancer_res2_auc, fig.cap="Results for the colorectal cancer example with (a) the estimated penalty multipliers and (b) estimated AUC.", out.width="100%", fig.asp=1/2>>=
@
  
  \section{Application to RNAseq in lymph node cancer}\label{sec:lymphnode}
  The aim of this study is to predict lymph node metastasis in patients 
  with HPV-negative oral cancer using sequenced RNAs from TCGA 
  \cite[]{the_cancer_genome_atlas_network_comprehensive_2015}. The features are 
  3096 transformed and normalised TCGA RNASeqv2 profiles of head-and-neck 
  squamous cell carcinomas on 133 patients with HPV-negative tumours in the oral
  cavity. Of these patients, 76 suffered from lymph node metastasis, while 57 
  did not. For a thorough introduction of these data, see 
  \cite{te_beest_improved_2017}.
  
  As external feature information, we considered two sources: (a) the 
  cis-correlation between the RNASeqv2 data and TCGA DNA copy numbers on 
  the same patients, 
  quantified by Kendall's $\tau$ and binned into five groups. We also considered
  (b) $p$-values from an external microarray data set described in 
  \cite{mes_prognostic_2017}, again binned into five groups. We expect features 
  with a large positive Kendall's $\tau$ to be more important in 
  metastasis prediction \cite[]{masayesva_gene_2004}. Likewise, we expect 
  features with low $p$-values to be more important.
  
  We compared \texttt{gren} to the same methods as in Section 
  \ref{sec:colorectal}. However, because the co-data consists of two partitions 
  with overlapping groups, we use an extension of \texttt{cMCP} that allows for
  overlapping groups \cite[]{zeng_overlapping_2016}.
  We estimated AUC for the methods on an independent validation set of 97 
  HPV-negative oral cancer patients \cite[]{mes_prognostic_2017}. The features 
  in this validation set were
  microarrays, normalised to account for a different scale of the transformed 
  RNAseq data. We present the results on this validation set in Figure 
  \ref{fig:barplot_lines_rnaseq_lymph_node_metastasis_res1_auc}. Other 
  comparisons and performance measures are presented in the SM Section 
  [! section ref here].
  
  The estimated penalty multipliers follow the expected pattern 
  (Figure \ref{fig:barplot_lines_rnaseq_lymph_node_metastasis_res1_auc}a): 
  larger cis-correlations receive smaller penalties. 
  Likewise, smaller $p$-values are penalized less. This pattern is slightly 
  less pronounced in \texttt{gren} with $\alpha=0.95$ compared to the other 
  methods. The predictive performance of
  \texttt{gren} is slighty better than the other methods for a range of model
  sizes. We also note that the overlapping group \texttt{cMCP} is much more
  stable than the regular \texttt{cMCP} (see in SM [! section ref here]), 
  where we create one paritioning by cross-tabulation of the two paritions. 
  In this example, ridge and \texttt{GRridge} perform almost identical.
<<barplot_lines_rnaseq_lymph_node_metastasis_res1_auc, fig.cap="Results for the lymph node metastasis example with (a) the estimated penalty multipliers for the cis-correlations, (b) the estimated penalty multipliers for the p-values, and (c) the estimated AUC.", out.width="100%", fig.asp=1>>=
@
  
	\section{Discussion}\label{sec:discussion}
	In a taxonomy of Bayesian methods, the proposed method may be considered a 
	local shrinkage model, as opposed to the global-local shrinkage priors that 
	\cite{polson_local_2012,bernardo_shrink_2011} discuss. They characterise 
	certain desirable properties of these global-local shrinkage priors in high 
	dimensions, which, for example, the horseshoe possesses 
	\cite[]{carvalho_horseshoe_2010,carvalho_handling_2009}. In our case, global 
	shrinkage would imply adding another hyperprior for the global $\lambda_1$ 
	and $\lambda_2$ (or $\alpha$ and $\lambda$) hyperparameters. We argue however,
	that if the groups are informative, the empirical Bayes estimation of the 
	(semi-)global shrinkage parameters $\lambda'_g$ may be more beneficial than 
	full Bayes shrinkage of the global penalty parameters, because the latter does
	not use any known structure to model the variability in the hyperparameters. 
	Nonetheless, an interesting direction of future research is the extension of 
	the group-regularized elastic net to a group-regularized horseshoe model, 
	since the horseshoe has been shown to handle sparsity well and render better
	coverage of credibility intervals than lasso-type priors 
	\cite[]{van_der_pas_horseshoe_2014}.

	Although our method can be considered weakly adaptive, it is different from 
	the adaptive lasso \cite[]{van_de_geer_adaptive_2011,huang_adaptive_2008,
	zhang_adaptive_2007,zou_adaptive_2006} and adaptive elastic net 
	\cite[]{zou_adaptive_2009} in the sense that it adapts to external information
	rather than to the data. It also differs in the scale of adaptation: in the 
	adaptive lasso and elastic net the adaptive weights are feature specific,
	while in our case they are estimated on the group level, rendering the 
	adaptation more robust. As can be seen from both the simulations in Section 
	\ref{sec:simulations} and the applications to different outcomes 
	(prognostic or diagnostic) and marker types 
	(RNAseq, microRNAs, or metabolomics) in Sections \ref{sec:colorectal}--
	\ref{sec:lymphnode}, and SM Sections [! section refs here], adaptation to 
	external data may be beneficial for prediction and feature selection. 
	We believe that this is due to the `borrowing of information' effect: 
	estimates of feature effects that behave similarly are shrunken similarly, 
	yielding overall, better estimates.
	
	A common criticism of the lasso (and elastic net) is unstable feature 
	selection: different data sets often lead to wildly differing sets of selected
	features. We investigated stability selection on the data from Section
	\ref{sec:colorectal} by taking bootstraps samples from the data, estimating
	\texttt{gren} and looking at the overlap between selected features over 
	different bootstrap samples. We compared it to the regular elastic net and 
	found that inclusion of extra feature information increases selection 
	stability (see SM Section [! section ref here]). In addition, we investigated 
	the stability of penalty multiplier estimation by examining the distribution
	of estimated penalty parameters from 100 random partitionings of the features
  (see SM Section [! section ref here]). We found that the penalty parameters
  tend to cluster around one, as is expected with random groups, and are 
  unbiased.

	As shortly touched upon in Section \ref{sec:introduction}, another obvious 
	comparison is to the group lasso \cite[]{meier_group_2008,
	yuan_model_2006} and its extensions. Although it is similar in the sense that 
	it shrinks on the 
	group level, the group lasso is built upon an entirely different philosophy: 
	its intended application is to small interpretable groups of features, like, 
	for example, dummies of a categorical variable. Another difference between 
	\texttt{gren} and the group lasso is the number of penalty parameters. 
	\texttt{gren} estimates one parameter per group, while the group lasso
	estimates one overall penalty parameter; it is thereby less flexible in 
	differential shrinkage of the parameters. In addition, in the simulations and
	data applications of \cite{van_de_wiel_better_2016}, group lasso prediction 
	performed inferior to \texttt{GRridge} prediction.

	A possible weak point of the proposed method is the double EM loop. The double
	loop increases the chance of ending up in a local optimum. In the application
	discussed above we investigated the occurrence of multiple optima, but never
	encountered them. This does not guarantee that local optima do not occur, but 
	it provides some evidence that local optima are not ubiquitous. Local optima 
	can often be avoided by an informed choice of starting values. In our 
	experience, reasonable starting values are obtained by running a 
	group-regularized ridge regression and using the estimated penalty multipliers
	as starting values for \texttt{gren}. Alternatively, a skeptical user may set 
	the starting values for all penalty multipliers to one, thereby starting from
	the regular elastic net and adaptively adjusting the penalty parameters to a
	group-regularized model.

<<>>=
bench1 <- read.table("results/micrornaseq_colorectal_cancer_bench2.csv", 
                     stringsAsFactors=FALSE)
bench2 <- read.table("results/rnaseq_lymph_node_metastasis_bench1.csv", 
                     stringsAsFactors=FALSE)
times1 <- round(bench1$time*10^(-9), 2)
times2 <- round(bench2$time*10^(-9), 2)
session <- gsub("_", ".", devtools::session_info()$platform$system)
@
  We have investigated the computation time of the algorithm and compared to the
  other methods introduced in Sections \ref{sec:colorectal}--\ref{sec:lymphnode}
  on the respective datasets. All timings were obtained on a Macbook Pro 2016 
  running \Sexpr{session}. 
  The timings of \texttt{gren} are competitive. On the colorectal cancer example
  we achieved the following times:
  \Sexpr{times1[c(1:3)]} seconds on the colorectal cancer example, 
  with $\alpha \in \{ 0.05, 0.5, 0.95 \}$, respectively. \texttt{GRridge} is 
  slightly faster with \Sexpr{times1[4]} seconds, while \texttt{SGL} was slightly
  slower: \Sexpr{times1[c(5:7)]} seconds, with 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$.
  \texttt{cMCP} (\Sexpr{times1[c(8:10)]} seconds) and \texttt{gel} 
  (\Sexpr{times1[c(11:13)]} seconds) are faster than \texttt{gren}. 
  On the lymph node cancer example \texttt{gren} achieved 
  \Sexpr{times2[c(1:3)]}, with $\alpha \in \{ 0.05, 0.5, 0.95 \}$, respectively.
  \texttt{GRridge} was slightly faster with \Sexpr{times2[4]} seconds, 
  while \texttt{SGL} was slightly slower: \Sexpr{times2[c(5:7)]} seconds, with 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$.
  \texttt{cMCP} (\Sexpr{times2[c(8:10)]} seconds) and \texttt{gel} 
  (\Sexpr{times2[c(11:13)]} seconds) are faster than \texttt{gren}. The 
  overlapping group methods achieved \Sexpr{times2[c(14:16)]} seconds
  (\texttt{ocMCP}) and \Sexpr{times2[c(17:19)]} seconds (\texttt{ogel}), for 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$, respectively.
  We note that a considerable speedup may be achieved by 
  doing one EB update for every few VB updates, instead of running the VB EM 
  until convergence. This works in practice, because an EM tends to take a few 
  initial large steps towards the optimum, followed by a series of smaller 
  steps. We argue that The first few iterations of the VB EM render the 
  approximate expectation in [! equation ref here] accurate enough for a 
  reliable EB step. In addition, this is the approach often taken by more
  computationally oriented disciplines. For example, in many machine learning 
  domains, the problem is viewed as purely algorithmic [! references here]. The
  the EB and VB 
  steps optimisation steps are interpreted as steps along the gradient in the 
  larger combined space of model- and hyper-parameters. 

	\section*{Software}
	A (stable) \texttt{R} package is available from 
	\url{https://CRAN.R-project.org/package=gren}. 

	\section*{Supplementary Material}
	Supplementary Material is available online from 
	\url{http://pub.math.leidenuniv.nl/~munchmm/}. All materials needed to   
	recreate this document are available from 
	\url{https://github.com/magnusmunch/gren}.

	\section*{Acknowledgements}
	\textit{Conflict of Interest}: None declared. We thank the editors and 
	referees for their useful suggestions and comments.

	\section*{Funding}
	This research has received funding from the European Research Council under 
	ERC Grant Agreement 320637.

	\bibliographystyle{author_short3.bst}
	\bibliography{refs}
	
	\section*{Session info}
<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@
	
\end{document}