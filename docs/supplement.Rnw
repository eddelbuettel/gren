% set options
<<settings, include=FALSE, echo=FALSE>>=
knitr::opts_knit$set(root.dir="..", base.dir="../figs")
knitr::opts_chunk$set(fig.align='center', fig.path="../figs/", 
                      echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
@

% read figure code
<<figures, include=FALSE>>=
knitr::read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,verbatim,
bbm,algorithm,algpseudocode,threeparttable,booktabs,mathtools,
dsfont,parskip,xr-hyper,hyperref}
\externaldocument[md-]{manuscript}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% settings
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Adaptive group-regularized logistic elastic net regression Supplementary 
material}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@vumc.nl}{m.munch@vumc.nl}}, Carel F.W. Peeters$^{1}$, 
Aad W. van der Vaart$^{2}$, and \\ Mark A. van de Wiel$^{1,3}$}

\begin{document}

  \maketitle
  
  \noindent
	1. Department of Epidemiology \& Biostatistics, Amsterdam Public Health
	research institute, Amsterdam University medical centers, PO Box 7057, 1007 MB
	Amsterdam, The Netherlands \\*
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\*
	3. MRC Biostatistics Unit, Cambridge University, Cambridge, United Kingdom

  \textbf{Keywords}: Empirical Bayes; High-dimensional data; Prediction; 
  Variational Bayes

  \textbf{Software available from}: 
  \url{https://CRAN.R-project.org/package=gren}

  \section{Introduction}\label{sec:introduction}
  This document contains supplementary material to the paper `Adaptive 
  group-regularized logistic elastic net regression'. Section 
  \ref{sec:enetprior} contains extra details on the elastic net prior 
  distribution. Section \ref{sec:objectivefunction} gives the objective 
  function that is maximised in the variational Bayes procedure introduced in
  Section \ref{md-sec:estimation} of the Main Document (MD). Section
  \ref{sec:polyagamma} gives details on
  the P{\'o}lya-Gamma parametrisation introduced in Section 
  \ref{md-sec:estimation} of the MD. The
  derivations of the variational posterior used in the MD are shown in Section
  \ref{sec:variationaldistributions}. Section
  \ref{sec:computationalcomplexity} shows how we may reduce the computational
  complexity of our variational Bayes procedure. Section 
  \ref{sec:expectedloglikelihood} elaborates on the Variational Bayes expected
  joint log likelihood that is maximised to estimate the penalty multipliers as
  explained in Section \ref{md-sec:estimation} of the MD. The full procedure of
  the
  proposed method is shown in Section \ref{sec:fullestimationprocedure}. 
  Extensions of our method are
  presented in Section \ref{sec:extensions}. The simulations in MD Section
  \ref{md-sec:simulations} are elaborated in Section \ref{sec:simulations}. 
  Additional results to the applications in the MD Sections 
  \ref{md-sec:colorectal}--\ref{md-sec:oral} are presented in Sections 
  \ref{sec:colorectal}--\ref{sec:oral}. We
  apply our method to a study on metabolomics in Alzheimer's in Section
  \ref{sec:alzheimer} and to study in microRNAs in cervical cancer data 
  in Section \ref{sec:cervical}. Finally, 
  we introduce the ridge and lasso versions of the proposed method in Section
  \ref{sec:evbridgelasso}. 

  \section{The elastic net prior}\label{sec:enetprior}
  \subsection{Density function}
  The density function of $\beta$, drawn from an elastic net prior distribution,
  is given by:
  $$
  f(\beta) = g(\lambda_1,\lambda_2) \exp\[-\frac{1}{2} (\lambda_1 |\beta| + 
  \lambda_2 \beta^2)\].
  $$
  The normalizing constant here is calculated as:
  $$
  g(\lambda_1,\lambda_2) =\left\{ \int_{-\infty}^{\infty} \exp\[-\frac{1}{2} 
  (\lambda_1 |\beta| + \lambda_2 \beta^2)\] d\beta \right\}^{-1}= 
  \frac{\sqrt{\lambda_2}}{2} \phi \( \frac{\lambda_1}{2 \sqrt{\lambda_2}} \)
  \Phi \(\frac{-\lambda_1}{2 \sqrt{\lambda_2}} \)^{-1},
  $$
  with $\phi(x)$ and $\Phi(x)$ the standard normal density and distribution 
  functions of $x$, respectively. 

  \subsection{Expectation and variance}
  The expectation and variance of $\beta$ are:
  \begin{align*}
    \E[\beta] &= g(\lambda_1,\lambda_2) \int_{-\infty}^{\infty} \beta \cdot 
    \exp\[-\frac{1}{2} (\lambda_1 |\beta| + \lambda_2 \beta^2)\] d\beta=0, \\
    \V[\beta] &= g(\lambda_1,\lambda_2) \int_{-\infty}^{\infty} \beta^2 \cdot
    \exp\[-\frac{1}{2} (\lambda_1 |\beta| + \lambda_2 \beta^2)\] d\beta -
    \E[\beta]^2\\ &= \frac{\lambda_1^2}{4 \lambda_2^2} + \frac{1}{\lambda_2} -
    \frac{\lambda_1}{2 \lambda_2^{3/2}} \phi \( \frac{\lambda_1}{2 
    \sqrt{\lambda_2}} \) \Phi \(\frac{-\lambda_1}{2 \sqrt{\lambda_2}} \)^{-1}.
  \end{align*}

  \subsection{Simulation}
  Generating samples from the elastic net prior is not straightforward. 
  \cite{li_bayesian_2010} show that the elastic net prior may be written as a 
  scale mixture of normals, with mixing parameter 
  $\btau = \begin{bmatrix} \tau_1 & \cdots & \tau_p \end{bmatrix} \tr$: 
  \begin{align*}
    \beta | \tau & \sim \mathcal{N} \left(0,\frac{1}{\lambda_2} 
    \frac{\tau - 1}{\tau} \right), \\
    \tau & \sim \mathcal{TG} \left( \frac{1}{2},\frac{8 \lambda_2}{\lambda_1^2},
    \left(1,\infty \right) \right),
  \end{align*}
  where $\mathcal{TG} \( k,\theta,\( x_l,x_u\) \)$ denotes the truncated gamma 
  distribution with shape $k$, scale $\theta$, and domain $\(x_l,x_u\)$. So with
  an efficient truncated gamma sampler available we may sample from the prior 
  using any standard software package that includes a Gaussian sampling routine.
  More specifically, we require a a sample from the $\mathcal{TG} 
  \left( \frac{1}{2},8 \lambda2/\lambda_1^2, \left(1,\infty \right) \right)$ 
  distribution. The CDF of this distribution is given by:
  $$
  F_{T}(\tau) =	
  \begin{dcases}
    0, & \text{if }\tau < 1 \\
    1 - \frac{\Phi\(-\frac{\lambda_1}{2} \sqrt{\frac{\tau}{\lambda_2}} \)}
    {\Phi\(-\frac{\lambda_1}{2 \sqrt{\lambda_2}} \)}, & \text{if } \tau \geq 1.
  \end{dcases}
  $$

  Since $F_{T}(\tau)$ is continuous and strictly monotonically increasing for 
  $\tau \geq 1$, the quantile function may be written as a function of the 
  probability $p \in (0,1)$:
  $$
  Q_{T}(p) =	F^{-1}_{T}(p) = \frac{4 \lambda_2}{\lambda_1^2} 
  \Phi^{-1} \[ (1 - p) \Phi\(-\frac{\lambda_1}{2 \sqrt{\lambda_2}} \)\]^2,
  $$
  where $\Phi^{-1}(p)$ is the quantile function of the standard normal 
  distribution, i.e, the probit function. Employing inverse transform sampling 
  for the truncated gamma distribution, an elastic net prior sampling scheme is 
  now:
  \begin{algorithmic}
	  \State{Generate $U \sim \mathcal{U} (0,1)$}
	  \State{Set $\tau = \frac{4 \lambda_2}{\lambda_1^2} \Phi^{-1} \[ (1 - u) 
	    \Phi\(-\frac{\lambda_1}{2 \sqrt{\lambda_2}} \)\]^2$}
	  \State{Set $\nu^2= \frac{\tau - 1}{\tau \lambda_2}$}
	  \State{Generate $\beta \sim \mathcal{N}(0,\nu^2)$}
	  \State{\Return{$\beta$}}
  \end{algorithmic}

  \section{Objective function}\label{sec:objectivefunction}
  The lower bound on the marginal likelihood, or evidence lower bound (ELBO), 
  at iteration $t$ is given by:
  \begin{subequations}
    \begin{align}
      \text{ELBO} (Q^{(t)}) & = \E_{Q^{(t)}} [\log \L (\y, \bomega, \bbeta, 
      \btau)] - \E_{Q^{(t)}} [\log Q (\bomega, \bbeta, \btau)] \\
      & = \E_{Q^{(t)}} [\log \L (\y ; \bbeta)] + \E_{Q^{(t)}} 
      [\log \pi(\bomega | \bbeta)] - \E_{Q^{(t)}} 
      [\log q(\bomega)] \label{eq:elboomega} \\
      & \,\,\,\,\, + \E_{Q^{(t)}} [\log \pi(\bbeta|\btau)] - \E_{Q^{(t)}} 
      [\log q(\bbeta)] \label{eq:elbobeta}\\
      & \,\,\,\,\, + \E_{Q^{(t)}} [\log \pi(\btau)] - \E_{Q^{(t)}} 
      [\log q(\btau)] \label{eq:elbotau}.
    \end{align}
  \end{subequations}

  We start with the computation of (\ref{eq:elboomega}). To this end we define:
  $$
  h(m_i, \omega_i) := \sum_{k=0}^{\infty} (-1)^k \frac{\Gamma(k+m_i}{k + 1}
  \frac{2k +m_i}{\sqrt{2 \pi \omega_i^3}}
  \exp\[-\frac{(2k + m_i)^2}{8 \omega_i} \].
  $$
  Let $\c = \begin{bmatrix} c_1 & \cdots & c_n \end{bmatrix} \tr$. Furthermore, 
  in the following, all expectations and variances are with respect to 
  $Q^{(t)}$. Next we write out the three terms in (\ref{eq:elboomega}):
  \begin{subequations} \label{eq:elboomega2}
    \begin{align}
      \E [\log \L (\y ; \bbeta)] & = \sum_{i=1}^n \log \binom{m_i}{y_i} + 
      \y \tr \X \E [\bbeta] - \m \tr \E \left\{ \log [1 + \exp(\x_i \tr \bbeta)]
      \right\}, \\
      \E [\log \pi(\bomega | \bbeta)] & = \m \tr \E 
      \left\{ \log [1 + \exp(\x_i \tr \bbeta)] \right\} - n \log 2 - 
      \frac{1}{2} \m \tr \X \E [\bbeta] \\
      & \,\,\,\,\, - \sum_{i=1}^n \log \Gamma (m_i) - \frac{1}{2} \sum_{i=1}^n 
      [ ( \x_i \tr \E [\bbeta] )^2 + \x_i \tr \V [\bbeta] \x_i ] 
      \E [\omega_i] \\
      & \,\,\,\,\,+ \sum_{i=1}^n \E 
      \left\{ \log [ h(m_i, \omega_i) ] \right\}, \\
      \E [\log q(\bomega)] & =  \m \tr \log [1 + \exp(\c)] - n \log 2 - 
      \frac{1}{2} \m \tr \c - \sum_{i=1}^n \log \Gamma (m_i) - \frac{1}{2} 
      \sum_{i=1}^n c_i^2 \E [\omega_i] \\
      & \,\,\,\,\, + \sum_{i=1}^n \E \left\{ \log [ h(m_i, \omega_i) ] \right\},
	  \end{align}
  \end{subequations}
  where all mathematical operations on vectors and matrices are element wise. 
  After combining the terms in (\ref{eq:elboomega2}) and substituting the 
  expectations and variances, several terms cancel to
  give:
  \begin{subequations} \label{eq:elboomega3}
	  \begin{align}
	    \E_{Q^{(t)}} & [\log \L (\y ; \bbeta)] + \E_{Q^{(t)}} 
	    [\log \pi(\bomega | \bbeta)] - \E_{Q^{(t)}} [\log q(\bomega)] \\ 
	    & = \sum_{i=1}^n \log \binom{m_i}{y_i} + ( \y - \frac{1}{2} \m) 
	    \tr \X \bmu^{(t)} + \m \tr \left\{ \frac{1}{2} \c^{(t)} - 
	    \log [1 + \exp(\c^{(t)})] \right\} \label{eq:elboomega4}\\
	    & \,\,\,\,\, + \frac{1}{4} \sum_{i=1}^n \frac{m_i}{c_i^{(t)}} 
	    \tanh \(\frac{c_i^{(t)}}{2} \)\left\{ (c_i^{(t)})^2 - 
	    ( \x_i \tr \bmu^{(t)} )^2 - \x_i \tr \bSigma^{(t)} \x_i \tr \right\}.
    \end{align}
  \end{subequations}

  Inspection of the updating equations (\ref{md-eq:VBupdateequations}) in the 
  MD, learns us that the 
  last term in the right-hand side of (\ref{eq:elboomega3}) equals zero, so that
  we are left with just (\ref{eq:elboomega4}). After a change of variables 
  $\psi_j = \tau_j - 1$, the two terms in (\ref{eq:elbobeta}) are as follows:
  \begin{subequations} \label{eq:elbobeta2}
	  \begin{align}
	    \E [\log \pi (\bbeta | \btau)] & = \frac{1}{2}\sum_{g=1}^G 
	    |\mathcal{G}(g)| \log \lambda'_g + \frac{p}{2} \log \lambda_2 - 
	    \frac{p}{2} \log (2 \pi) \\
	    & \,\,\,\,\, - \frac{\lambda_2}{2} \sum_{g=1}^G \lambda'_g \sum_{j \in 
	    \mathcal{G}(g)} \(1 + \frac{\lambda_1}{2 \sqrt{\lambda_2 \chi_j}}\) 
	    (\bmu_j^2 + \bSigma_{jj}) \\
	    & \,\,\,\,\, - \frac{1}{2} \sum_{j=1}^p \E [\log \psi_j] + \frac{1}{2} 
	    \sum_{j=1}^p \E [ \log(\psi_j + 1)], \\
	    \E [\log q (\bbeta )] & = -\frac{p}{2} - \frac{p}{2}\log (2 \pi) - 
	    \frac{p}{2} \log |\bSigma| .
	  \end{align}
  \end{subequations}
  We combine the terms in (\ref{eq:elbobeta2}) and arrive at the following for 
  (\ref{eq:elbobeta}):
  \begin{subequations} \label{eq:elbobeta3}
	   \begin{align}
	     \E_{Q^{(t)}} & [\log \pi (\bbeta | \btau)] - \E_{Q^{(t)}} 
	     [\log q (\bbeta )] \\ 
	     & = \frac{1}{2}\sum_{g=1}^G |\mathcal{G}(g)| \log \lambda'^{(t)}_g + 
	     \frac{p}{2} \log \lambda_2 + \frac{p}{2} \log |\bSigma^{(t)}| + 
	     \frac{p}{2} \\
	     & \,\,\,\,\, - \frac{\lambda_2}{2} \sum_{g=1}^G \lambda'^{(t)}_g 
	     \sum_{j \in \mathcal{G}(g)} \(1 + \frac{\lambda_1}{2 \sqrt{\lambda_2 
	     \chi_j^{(t)} } }\) \((\bmu^{(t)}_j)^2 + \bSigma_{jj}^{(t)}\) \\
	     & \,\,\,\,\, - \frac{1}{2} \sum_{j=1}^p \E_{Q^{(t)}} [\log \psi_j] + 
	     \frac{1}{2} \sum_{j=1}^p \E_{Q^{(t)}} [ \log(\psi_j + 1)].
	  \end{align}
  \end{subequations}

  Repeating the same exercise for (\ref{eq:elbotau}) gives:
  \begin{subequations} \label{eq:elbotau2}
	  \begin{align}
	    \E [\log \pi (\btau)] & = p \log \lambda_1 - \frac{p}{2} \log \lambda_2 - 
	    \frac{5 p }{2} \log 2 - \frac{p}{2} \log \pi - \frac{p}{2} - 
	    \frac{p \lambda_1^2}{8 \lambda_2} \\
	    & \,\,\,\,\, - p \log \Phi \( \frac{-\lambda_1}{2 \sqrt{ \lambda_2}} \) -
	    \frac{\lambda_1}{4 \sqrt{\lambda_2}} \sum_{j=1}^p \chi_j^{1/2} - 
	    \frac{1}{2} \sum_{j=1}^p \E [\log ( \psi_j + 1)], \\
	    \E [\log q (\btau)] & = p \log \lambda_1 - \frac{p}{2} \log \lambda_2 -
	    \frac{3p}{2} \log 2 -\frac{p}{2} \log \pi -\frac{p}{2} - 
	    \frac{\lambda_1}{2 \sqrt{\lambda_2}} \sum_{j=1}^p \chi_j^{1/2} \\
	    & \,\,\,\,\, + \frac{1}{4} \sum_{j=1}^p \log \chi_j - 
	    \frac{1}{2} \sum_{j=1}^p \E [\log \psi_j].
	  \end{align}
  \end{subequations}
  Again, we combine the terms in (\ref{eq:elbotau2}) to get:
  \begin{subequations} \label{eq:elbotau3}
	  \begin{align}
	    \E_{Q^{(t)}} & [\log \pi (\btau)] - \E_{Q^{(t)}} [\log q (\btau)] 
	    \\ & = p \log 2 - \frac{p \lambda_1^2}{8 \lambda_2} - 
	    p \log \Phi \( \frac{-\lambda_1}{2 \sqrt{ \lambda_2}} \) + 
	    \frac{\lambda_1}{4 \sqrt{\lambda_2}} \sum_{j=1}^p \sqrt{\chi_j^{(t)}} -
	    \frac{1}{4} \sum_{j=1}^p \log \chi_j^{(t)} \\
	    & \,\,\,\,\, + \frac{1}{2} \sum_{j=1}^p \E_{Q^{(t)}} [\log \psi_j] -
	    \frac{1}{2} \sum_{j=1}^p \E_{Q^{(t)}} [\log ( \psi_j + 1)].
	  \end{align}
  \end{subequations}

  Adding up the terms in (\ref{eq:elboomega3}), (\ref{eq:elbobeta3}), and
  (\ref{eq:elbotau3}) gives the ELBO, at iteration $t$:
  \begin{align*}
    \text{ELBO} (Q^{(t)}) & \propto (\y - \frac{1}{2} \m ) \tr \X \bmu^{(t)} + 
    \m \tr \{ \frac{1}{2} \c^{(t)} - \log [ \exp(\c^{(t)}) + 1 ]\} + \frac{1}{2}
    \sum_{g=1}^G |\G(g)| \log \lambda'^{(t)}_g \\ 
    & + \frac{1}{2}\log |\bSigma^{(t)}| - \frac{\lambda_2}{2} 
    \sum_{g=1}^G \lambda'^{(t)}_g \sum_{j \in \mathcal{G}(g)} 
    \(1 + \frac{\lambda_1}{2 \sqrt{\lambda_2 \chi_j^{(t)} } }\) 
    \((\bmu^{(t)}_j)^2 + \bSigma_{jj}^{(t)}\) \\
    & + \frac{\lambda_1}{4 \sqrt{\lambda_2}} \sum_{j=1}^p \sqrt{\chi_j^{(t)}} -
    \frac{1}{4} \sum_{j=1}^p \log \chi_j^{(t)}.
  \end{align*}
  Here, proportionality is with respect to the variational and penalty 
  parameters. 

  \section{The P\'{o}lya-Gamma parametrisation}\label{sec:polyagamma}
  In Section \ref{md-sec:estimation} of the MD latent P\'{o}lya-Gamma
  distributed variables
  are introduced. Likewise, in \cite{polson_bayesian_2013}, P\'{o}lya-Gamma
  variables are introduced into the logistic regression model under a Gaussian
  prior. Here, we explain how introduction of these latent variables simplifies
  the variational Bayes calculations. 

  In \cite{polson_bayesian_2013}, the central identity for this parametrisation
  is given in their Theorem 1:
  \begin{equation}\label{eq:intid}
    \frac{\exp(\psi)^{a}}{[1 + \exp(\psi)]^{b}} = 2^{-b} \exp(\kappa \psi)
    \int_0^{\infty} \exp(-\omega \psi^2 /2) \pi(\omega) d \omega,
  \end{equation}
  where $\kappa=a - b/2$ and $\omega$ is $\mathcal{PG}\(b,0\)$ distributed. This
  holds for all $b > 0$ and $a \in \mathbb{R}$. After setting $a=y_i$, $b=m_i$,
  and $\psi=\x_i \tr \bbeta$, we may consider the right-hand side of 
  (\ref{eq:intid}) as an un-normalised joint density in $(\bbeta, \omega)$. The
  likelihood contribution of observation $i$ for $\bbeta$ conditional on
  $\omega_i$ may now be written as:
  $$
  \L_i(\bbeta | \omega_i) \propto \exp[\kappa_i \x_i \tr \bbeta - 
  \omega_i (\x_i   \tr \bbeta)^2/2].
  $$

  This allows the conditional posterior of $\bbeta$ to be written as:
  $$
  p(\bbeta | \bomega, \y) \propto \pi(\bbeta) \prod_{i=1}^n 
  \L_i(\bbeta | \omega_i) = \pi(\bbeta) 
  \exp \[-\frac{1}{2}(\bOmega^{-1} \bkappa - \X \bbeta) \tr 
  \bOmega (\bOmega^{-1} \bkappa - \X \bbeta)\],
  $$
  where $\bOmega = \diag (\omega_i)$, for $i=1, \dots, n$. Consequently, with
  $\pi(\bbeta)$ a Gaussian prior, $p(\bbeta | \bomega, \y)$ is also in the
  Gaussian family.

  Furthermore, by treating the integrand in (\ref{eq:intid}) as an un-normalized
  density over $(\psi, \phi)$, we find the distribution of $\omega$ conditional
  on $\bbeta$,
  $$
  p(\omega | \psi) = \frac{\exp(-\omega \psi^2/2) \pi(\omega)}{\int_0^{\infty}
  \exp(-\omega \psi^2/2) \pi(\omega) d\omega},
  $$
  distributed as $\omega | \psi \sim \mathcal{PG}(b,\psi)$ 
  \cite[]{polson_bayesian_2013}, with expectation $\E[ \omega ] = 
  \frac{b}{2\psi} \tanh (\psi/2)$. We substitute again $b=m_i$ and $\psi=\x_i 
  \tr \bbeta$, and set the prior $\pi(\bbeta)$ to a Gaussian with mean $\m$ and
  covariance $\Vm$ to arrive at the following conditional posteriors:
  \begin{subequations}\label{eq:gibbssampler}
	  \begin{align}
	    \omega_i | \bbeta & \sim \mathcal{PG}(m_i, \x_i \tr \bbeta) \\
	    \bbeta | \y, \bomega & \sim \N(\bmu, \bSigma),
	  \end{align}
  \end{subequations}
  where
  \begin{align*}
    \bSigma & = (\X \tr \bOmega \X + \Vm^{-1})^{-1}, \\
    \bmu & = \Vm (\X \tr \bkappa + \Vm^{-1} \m),
  \end{align*}
  with $\bkappa = \begin{bmatrix} \kappa_1 \dots \kappa_n \end{bmatrix} \tr$.
  Equations (\ref{eq:gibbssampler}) constitute the Gibbs sampler used in
  \cite{polson_bayesian_2013}. However, from the corresponding variational Bayes
  derivations in Section \ref{sec:variationaldistributions}, it is obvious that
  the parametrisation above is also useful in the variational Bayes 
  approximation to the posterior distribution.

  \section{Variational posterior}\label{sec:variationaldistributions}
  The optimal distributions in our variational Bayes implementation are:
  \begin{align}
    q^*_{\bbeta} (\bbeta) & {\propto} \exp \{ \E_{\bomega, \btau} 
    [\log p (\bbeta, \bomega, \btau, \y)]\} {\propto} \exp 
    \{\log \L (\y ; \bbeta) + \E_{\bomega} [\log \pi (\bomega | \bbeta)] + 
    \E_{\btau} [ \log \pi (\bbeta | \btau)] \}, \label{eq:qbeta} \\
    q^*_{\bomega} (\bomega) & {\propto} \exp \{ \E_{\bbeta, \btau} 
    [\log p (\bbeta, \bomega, \btau, \y)] \} {\propto} \exp \{ \E_{\bbeta} 
    [\log \pi (\bomega | \bbeta)] \},\label{eq:qomega} \\
    q^*_{\tau} (\btau) & {\propto} \exp \{ \E_{\bbeta, \bomega} 
    [\log p (\bbeta, \bomega, \btau, \y)] \} {\propto} \exp \{ \E_{\bbeta} 
    [\log \pi ( \bbeta | \btau)] + \log \pi (\btau) \}. \label{eq:qtau}
  \end{align}

  For convenience we work on the log-scale and occasionally do a change of
  variables $\psi_j = \tau_j - 1$. Starting with $\bbeta$ we have for the three
  terms on the right-hand side of (\ref{eq:qbeta}):
  \begin{align*}
    \log \L (\y ; \bbeta) & = \sum_{i=1}^n \log f (y_i ; \bbeta) \propto 
    \sum_{i=1}^n \log \left\{ \frac{\exp(\x_i \tr \bbeta)^{y_i}}{[1 + 
    \exp(\x_i \tr \bbeta)]^{m_i}} \right\} \\
    & = \y \tr \X \bbeta - \sum_{i=1}^n m_i \log [1 + \exp(\x_i \tr \bbeta)], \\
    \E_{\bomega} [\log \pi (\bomega | \bbeta)] & \propto \sum_{i=1}^n 
    \E_{\omega_i} [\log \pi (\omega_i | \bbeta)] \\ & \propto \sum_{i=1}^n
    \E_{\omega_i} \( \log \left\{ \cosh\( \frac{\x_i \tr \bbeta}{2}\)^{m_i} 
    \exp \[-\frac{(\x_i \tr \bbeta)^2}{2} \omega_i \]\right\} \) \\
    & \propto \sum_{i=1}^n m_i \log \cosh \( \frac{\x_i \tr \bbeta }{2} \) -
    \frac{1}{2} \bbeta \tr \X \tr \bOmega \X \bbeta \\ & \propto 
    \sum_{i=1}^n m_i \log [1 + \exp(\x_i \tr \bbeta)] - \frac{1}{2} \m \tr 
    \X \bbeta - \frac{1}{2} \bbeta \tr \X \tr \bOmega \X \bbeta, \\
    \E_{\btau} [ \log \pi (\bbeta | \btau)] & = \sum_{j=1}^p \E_{\tau_j} 
    [ \log \pi (\beta_j | \tau_j)] \propto \frac{\lambda_2}{2} 
    \sum_{j=1}^p \beta_j^2 \lambda'_{g(j)} \E \( \frac{\tau_j}{\tau_j - 1} \) \\
    & = \sum_{j=1}^p \beta_j^2 \lambda'_{g(j)} [1 + \E ( \psi_j^{-1} )] = -
    \frac{1}{2} \lambda_2 \bbeta \tr \bLambda' (\I + \Z) \bbeta,
  \end{align*}
  where proportionality is with respect to $\bbeta$. Furthermore, $\bOmega =
  \diag [\E(\omega_i)]]$, $\Z = \diag [ \E (\psi_j^{-1})]$, and $\bLambda' =
  \diag (\lambda'_{g(j)})$. 
  Since the difficult $\sum_{i=1}^n m_i \log[1 + \exp(\x_i \tr \bbeta)]$ term
  appears in both the $\log \L (\y ; \bbeta)$ and $\pi (\omega_i | \bbeta)$ 
  part, it cancels, thereby justifying the introduction of the $\omega_i$.
  Finally, we have: 
  $$
  \log q^*_{\bbeta} (\bbeta) = \bkappa \tr \X \bbeta - \frac{1}{2} \bbeta \tr 
  (\X \tr \bOmega \X + \lambda_2 \bLambda' + \lambda_2 \bLambda' \Z) \bbeta,
  $$
  which is quadratic in $\bbeta$ and thus recognised as the log-kernel of a 
  Gaussian distribution with covariance $\bSigma = (\X \tr \bOmega \X + 
  \lambda_2 \bLambda' + \lambda_2 \bLambda' \Z)^{-1}$ and mean $\bmu=\bSigma 
  \X \tr \bkappa$.

  Next we derive the variational posterior for $\bomega$ in (\ref{eq:qomega}):
  \begin{align*}
    \log q^*_{\bomega} (\bomega) & = \E_{\bbeta} [\log \pi (\bomega | \bbeta)] =
    \sum_{i=1}^n \E_{\bbeta} [\log \pi (\omega_i | \bbeta)] \\ & = \sum_{i=1}^n
    \E_{\bbeta} \( m_i \log \left\{ \cosh \( \frac{\x_i \tr \bbeta}{2}\) 
    \exp\[-\frac{(\x_i \tr \bbeta)^2}{2} \omega_i\] \cdot f(\omega_i | m_i, 0) 
    \right\} \) \\ 
    & = \sum_{i=1}^n \E_{\bbeta} \[ m_i \log \cosh 
    \( \frac{\x_i \tr \bbeta}{2}\) \] - \frac{1}{2} \sum_{i=1}^n \omega_i 
    \E[ (\x_i \tr \bbeta)^2] + \sum_{i=1}^n \log f(\omega_i | m_i, 0) \\ 
    & \propto \sum_{i=1}^n m_i \log \cosh \left\{ \frac{\sqrt{[\x_i \tr 
    \E(\bbeta)]^2 + \x_i \tr \V[\bbeta]}}{2}\right\} - 
    \frac{1}{2} \sum_{i=1}^n \omega_i \left\{ [\x_i \tr \E(\bbeta)]^2 + 
    \x_i \tr \V[\bbeta] \right\} \\ 
    & \,\,\, + \sum_{i=1}^n \log f(\omega_i | m_i, 0),
  \end{align*}
  where proportionality is with respect to $\bomega$. If exponentiated, this is 
  a product of tilted and renormalised $\mathcal{PG}(b_i, c_i)$ variables
  \cite[]{polson_bayesian_2013}, with $b_i=m_i$ and $c_i = \sqrt{[\x_i \tr 
  \E(\bbeta)]^2 + \x_i \tr \V[\bbeta] \x_i }$. 

  The variational posterior terms of $\btau$ in (\ref{eq:qtau}) are calculated 
  as:
  \begin{align*}
    \E_{\bbeta} [\log \pi ( \bbeta | \btau)] & = \sum_{j=1}^p \E_{\bbeta_j} 
    [\log \pi(\beta_j | \tau_j )] \propto \frac{1}{2} \sum_{j=1}^p \log 
    \(\frac{\tau_j}{\tau_j - 1} \) - \frac{1}{2} \sum_{j=1}^p \E (\beta_j^2)
    \lambda_2 \lambda'_{g(j)} \frac{\tau_j}{\tau_j - 1}, \\
    \log \pi (\btau) & = \sum_{j=1}^p \log \pi (\tau_j) \propto - 
    \frac{1}{2} \sum_{j=1}^p \log \tau_j - 
    \sum_{j=1}^p \frac{\lambda_1^2}{8 \lambda_2} \tau_j. 
  \end{align*}
  Combining the terms we find the log-kernel of the variational posterior as:
  \begin{align*}
    \log q^*_{\tau} (\btau) & \propto \frac{1}{2} \sum_{j=1}^p 
    \log \(\frac{\tau_j}{\tau_j - 1} \) - \frac{1}{2} \sum_{j=1}^p 
    \E (\beta_j^2) \lambda_2 \lambda'_{g(j)} \frac{\tau_j}{\tau_j - 1}  - 
    \frac{1}{2} \sum_{j=1}^p \log \tau_j - \sum_{j=1}^p 
    \frac{\lambda_1^2}{8 \lambda_2} \tau_j \\
    & = \frac{1}{2} \sum_{j=1}^p \log [(\tau_j - 1)^{-1}] - 
    \frac{1}{2} \sum_{j=1}^p \(\frac{\lambda_1^2}{4 \lambda_2}\tau_j + 
    [\E (\beta_j)^2 + \V(\beta_j)] \lambda_2 \lambda'_{g(j)} 
    \frac{\tau_j}{\tau_j - 1}\),
  \end{align*}
  where proportionality is with respect to $\btau$. Again applying the change of
  variables $\psi_j = \tau_j - 1$, we have:
  $$
  \log q^*_{\psi} (\psi) \propto \frac{1}{2} \sum_{j=1}^p \log \psi_j^{-1} -
  \frac{1}{2} \sum_{j=1}^p \(\frac{\lambda_1^2}{4 \lambda_2} \psi_j + 
  [\E (\beta_j)^2 + \V(\beta_j)] \lambda_2 \lambda'_{g(j)} \psi_j^{-1}\),
  $$
  which is the log-kernel of a $\GIG(1/2, \lambda_1^2/(4 \lambda_2), \chi_j)$
  random variable with $\chi_j=\lambda_2 \lambda'_{g(j)} [\E (\beta_j)^2 + 
  \V(\beta_j)]$.

  \section{Computational complexity}\label{sec:computationalcomplexity}
  Calculation of $\bSigma$ involves the inversion of a 
  $(p \times p)$-dimensional matrix, which occurs at every iteration of the
  algorithm. If naively done, every inversion is of computational complexity
  $\mathcal{O}(p^3)$. With $p$ large, this is a serious computational bottleneck
  of the algorithm, especially with possibly many iterations. However, 
  inspection of the updating equations in (\ref{md-eq:VBupdateequations})
  in the MD, reveals
  that we only require the inner products $\x_i \tr \bSigma \x_i$, $\x_i 
  \tr \bmu$, and $\bmu$, plus the diagonal of $\bSigma$ to directly update the
  $c_i$ and $\chi_j$. In Section \ref{md-sec:estimation} of the MD we show that 
  for the 
  EM updates of $\blambda'$, we only require the $c_i$, the $\chi_j$, $\bmu$, 
  and the diagonal of $\bSigma$. Here we provide details on how to compute these
  with complexity $\mathcal{O}(n^2 p)$, thereby considerably reducing the
  computational cost of the algorithm. If necessary, we may calculate the full
  $\bSigma$ once after the algorithm has converged.

  In the following we slightly change notation and let $\diag(\A)$, with $\A$ a
  $p \times p$ square matrix, be the operator that extracts the diagonal of $\A$
  to a $p$-dimensional column vector, e.g., $\diag(\A) = \begin{bmatrix} 
  \A_{11} & \cdots & \A_{pp} \end{bmatrix} \tr$. We start by calculating the
  complexity of $\diag(\bSigma)$. To this end, let $\H = \lambda_2 \bLambda' +
  \lambda_2 \bLambda' \Z$. Now, by the Woodbury identity, we may decompose 
  $\diag(\bSigma)$ as:
  \begin{subequations}\label{eq:diagsigma}
	  \begin{align}
	    \diag(\bSigma) ~& = \diag(\H^{-1}) - \diag[\H^{-1} \X \tr \bOmega 
	    (\I + \X \H^{-1} \X \tr \bOmega)^{-1} \X  \H^{-1}] \\
	    & = \diag(\H^{-1}) - [(\H^{-1} \X \tr \bOmega (\I + \X \H^{-1} \X \tr 
	    \bOmega)^{-1}) \circ \H^{-1} \X \tr] \cdot \ones_{n \times 1},
	  \end{align}
  \end{subequations}
  where $\circ$ denotes the Hadamard matrix product and $\ones_{p \times n}$ is
  a $p \times n$-dimensional matrix of ones. By recognising that $\H$ is a
  diagonal matrix, we can post-multiply it by a $p \times n$-dimensional matrix
  with computational complexity $\mathcal{O}(np)$, which leads to a total
  complexity of $\mathcal{O}(n^2p)$ for $\diag(\bSigma)$. Next we consider the
  calculation of $\bmu$. Again using the Woodbury identity, we may write $\bmu$
  as:
  \begin{align}\label{eq:mu}
    \bmu = \bSigma \X \tr \bkappa = [\H^{-1} \X \tr - \H^{-1} \X \tr \bOmega 
    (\I + \X \H^{-1} \X \tr \bOmega)^{-1} \X  \H^{-1} \X \tr] \bkappa,
  \end{align}
  which is, by a similar argument as before, again an operation of complexity
  $\mathcal{O}(n^2p)$.

  Now we rewrite the calculation of $\c = \begin{bmatrix} c_1 & \cdots & c_n
  \end{bmatrix} \tr$ as:
  \begin{align*}
    \c & = \begin{bmatrix} \sqrt{\x_1 \bSigma \x_1 + (\x_1 \tr \bmu)^2} & 
    \cdots & \sqrt{\x_n \bSigma \x_n + (\x_n \tr \bmu)^2} \end{bmatrix} \tr \\
    & = \sqrt{\diag \[\X \bSigma \X \tr\] +  
    \diag \[\X \bmu \bmu \tr \X \tr\]} \\
    & = \sqrt{ [(\X \bSigma) \circ \X ] \cdot \ones_{p \times 1} + 
    (\X \bmu) \circ (\X \bmu)}.
  \end{align*}
  The calculation requires matrix products of complexity $\mathcal{O}(n^2 p)$,
  Hadamard products of $\mathcal{O}(np)$ and matrix inversions of $\mathcal{O}
  (n^3)$, such that the total complexity is of $\mathcal{O}(n^2p)$.

  The parameters $\bm{\chi} = \begin{bmatrix} \chi_1 & \cdots & \chi_p 
  \end{bmatrix} \tr$ are rewritten as:
  \begin{subequations}\label{eq:chicomp}
	  \begin{align}
	    \bm{\chi} & = \begin{bmatrix} \lambda'_{g(1)} \lambda_2 (\bSigma_{11} +
	    \bmu_1^2) & \cdots & \lambda'_{g(p)} \lambda_2 (\bSigma_{pp} + \bmu_p^2)
	    \end{bmatrix} \tr\\
	    & = \lambda_2 \bLambda' \[ \diag (\bSigma ) + \diag (\bmu \bmu \tr )\] \\
	    & = \lambda_2 \blambda' \circ \[ \diag (\bSigma) + \bmu \circ \bmu \].
	  \end{align}
  \end{subequations}
  By inserting (\ref{eq:diagsigma}) and (\ref{eq:mu}) into (\ref{eq:chicomp}) we
  see that the computation of $\bm{\chi}$ consists of one matrix diagonal of
  $\mathcal{O}(n^2p)$, matrix products of complexity $\mathcal{O}(n^2p)$ and two
  Hadamard products of $\mathcal{O}(p)$. Computation of $\bm{\chi}$ is therefore
  of complexity of $\mathcal{O}(n^2p)$.

  \section{Variational Bayes expected joint log likelihood}
  \label{sec:expectedloglikelihood}
  Here, let $\propto$ denote proportionality with respect to $\blambda'$. Then
  the variational Bayes expected joint log likelihood is derived as follows:
  \begin{align*}
    \E_{Q} [ \log \L_{\blambda'}(\y, \bomega, \bbeta, \btau) | \blambda'^{(k)} ]
    &\propto \E_Q [\log \pi_{\blambda'}(\bbeta | \btau) | \blambda'^{(k)}]
    \propto \sum_{g=1}^G \sum_{j \in \mathcal{G}(g)} \E_Q [\log \pi_{\blambda'}
    (\beta_j | \tau_j) | \blambda'^{(k)}] \\
    & \propto \frac{1}{2} \sum_{g=1}^G \sum_{j \in \mathcal{G}(g)}  
    \log \lambda'_g - \frac{(1 - \alpha) \lambda}{4} \sum_{g=1}^G 
    \sum_{j \in \mathcal{G}(g)} \lambda'_g \E_Q \( \frac{\tau_j}{\tau_j - 1}
    \beta_j^2 | \blambda'^{(k)} \) \\ 
    & \propto \frac{1}{2} \sum_{g=1}^G |\G(g)|  \log \lambda'_g - 
    \frac{(1 - \alpha) \lambda}{4} \sum_{g=1}^G \lambda'_g d_g^{(k)},
  \end{align*}
  with the coefficients $d_g^{(k)}$ calculated as:
  \begin{align*}
    d_g^{(k)} & = \sum_{j \in \mathcal{G}(g)} \E_Q \( \frac{\tau_j}{\tau_j - 1}
    \beta_j^2 | \blambda'^{(k)} \) \\
    & = \sum_{j \in \mathcal{G}(g)} \[1 + \E_{q_{\psi_j}} 
    ( \psi_j^{-1} | \blambda'^{(k)} ) \]  \[ \E_{q_{\beta_j}} 
    ( \beta_j | \blambda'^{(k)} )^2 + \V_{q_{\beta_j}} 
    ( \beta_j | \blambda'^{(k)} ) \] \\
    & = \sum_{j \in \mathcal{G}(g)} \[\bSigma^{(k)}_{jj} + 
    (\bmu^{(k)}_j)^2\]\(1 + \alpha \lambda^{1.5} 
    \sqrt{\frac{1 - \alpha}{8 \chi_j^{(k)}}}\).
  \end{align*}

  \section{Full estimation procedure}\label{sec:fullestimationprocedure}
  The expectation and maximisation steps introduced in the Main Document Section
  \ref{md-sec:estimation} result
  in a procedure as outlined in Algorithm \ref{alg:summaryalg}:
  \begin{algorithm}
    \caption{Group-regularized empirical Bayes elastic net}
    \label{alg:summaryalg}
	  \begin{algorithmic}[h]
		  \Require $\X, \y, \G, \alpha, \epsilon_1, \epsilon_2$
		  \Ensure $\lambda, \blambda', \bSigma, \bmu$
		  \State{Estimate $\lambda$ by cross-validation of the regular elastic net
		  model}
		  \While{$|\frac{\blambda'^{(k + 1)} - \blambda'^{(k)}}{\blambda'^{(k)}}| >
		  \epsilon_1$}
		  \While{$\underset{ij}{\max} \, |\frac{\bSigma_{ij}^{(k+1)} - 
		  \bSigma_{ij}^{(k)}}{\bSigma_{ij}^{(k)}}| > \epsilon_2$ or 
		  $\underset{i}{\max} \, |\frac{\bmu_{i}^{(k+1)} - 
		  \bmu_{i}^{(k)}}{\bmu_{i}^{(k)}}| > \epsilon_2$}
		  \State{Update $\bSigma$, $\bmu$, $c_i$ for $i=1, \dots, n$ and $\chi_j$ 
		  for $j=1, \dots, p$ using (\ref{md-eq:VBupdateequations}) in the MD}
		  \EndWhile
		  \State{Update $\blambda'$ by (\ref{md-eq:mmlupdateequation}) in the MD}
		  \EndWhile
	  \end{algorithmic}
  \end{algorithm}

  \section{Extensions}\label{sec:extensions}
  \subsection{Unpenalized covariates}
  Inclusion of an intercept $\beta_0$ is achieved by appending the data matrix
  $\X$ with a column of ones. Penalization of such an intercept parameter is not
  desirable \cite[]{Hastie_Elements_2009}. Additionally, it is often desirable 
  to include unpenalized covariates in the model. Common examples of such 
  covariates in clinical research are patient characteristics, such as age, BMI,
  and sex. In the following we assume the intercept to be included in the
  unpenalized covariates.

  To include unpenalized covariates, we divide the model matrix into two parts
  $\X = \begin{bmatrix}\X_u & \X_r\end{bmatrix}$, where $\X_u$ are the $u$
  unpenalized variables and $\X_r$ are the $r$ penalized variables. Let
  $\bLambda'_*$ and $\Z_*$ be the matrices $\bLambda'$ and $\Z$ prepended with
  $u$ zero columns and $u$ zero rows. Then we have for the current estimate of
  the covariance matrix $\bSigma$:
  \begin{align}\label{eq:unpencov}
    \bSigma &=\(\X \tr \bOmega \X + \lambda_2 \bLambda'_* + 
    \frac{\lambda_1 \sqrt{\lambda_2}}{2} \bLambda'_* \Z_*\)^{-1} \nonumber \\
    & = 
    \begin{bmatrix} 
      \X_u \tr \bOmega \X_u & \X_u \tr \bOmega \X_r \\
      \X_r \tr \bOmega \X_u & \X_r \tr \bOmega \X_r + \lambda_2 \bLambda' +
      \frac{\lambda_1 \sqrt{\lambda_2}}{2} \bLambda' \Z
    \end{bmatrix}^{-1}.
  \end{align}
  With the choice of blocks as in (\ref{eq:unpencov}), blockwise inversion
  renders the largest required matrix inverse $(\X_r \tr \bOmega \X_r +
  \lambda_2 \bLambda' + 
  \frac{\lambda_1 \sqrt{\lambda_2}}{2} \bLambda' \Z)^{-1}$, of dimension 
  $p \times p$. Inversion of this matrix is done efficiently by applying the
  Woodbury identity.

  \subsection{Monotonicity of the penalty parameters}
  Enforcing monotonicity of the penalty multipliers is desirable in some
  settings. An example of such a setting is if we have $p$-values from a
  previous, related study available. A $p$-values-based partitioning of the
  variables may, \textit{a priori}, be expected to render penalty multipliers
  that increase monotonically with $p$-value. That is, larger $p$-values may be
  expected to yield at least as large penalty multipliers as variables with
  smaller $p$-values. We propose to include this \textit{a priori} assumption by
  requiring the penalty multipliers to increase monotonically with $p$-value,
  thereby also stabilizing their estimates.

  A natural way of enforcing monotonicity is to extend the constraint in (10) in
  the Main Document with $\lambda'_1 \leq \dots \leq \lambda'_G$. The problem is
  still convex and may be numerically solved. From experience, however, we note
  that in combination with this constrained optimisation, the EM algorithm
  described in Section \ref{md-sec:estimation} in the MD often converges to a
  local optimum
  close to the initialisation. We therefore enforce monotonicity through a
  \textit{post hoc} isotonic regression on the penalty multipliers after every
  optimisation step.

  \subsection{Multiple partitions}
  In many cases the features may be partitioned in more than one way. For
  example, we may have both information on annotation and $p$-values from a
  previous study available. A naive way of incorporating multiple partitions is
  to cross-tabulate the partitions and create a separate group for every
  combination. This poses two problems: (i) The number of penalty parameters
  increases exponentially with the number of partitions and (ii) some of these
  combinations may contain only few features, so that the estimation procedure
  becomes unstable. We propose to stabilise the procedure and keep the number of
  parameters to estimate manageable by modelling the penalty parameters
  multiplicatively. 

  We describe our implementation here for two partitions of the features. To
  this end, let $(\G_1(1), \dots,\allowbreak \G_1(G_1))$ and $(\G_2(1), \dots,
  \G_2(G_2))$ denote the two partitions, containing $G_1$ and $G_2$ groups
  respectively. Furthermore, in the following we assume that the empty sum and
  empty product evaluate to 0 and 1, respectively. In this two-partition setting
  we have two penalty multipliers per feature, represented by $\lambda'_{g_1}$
  and $\lambda''_{g_2}$, respectively. The generalised frequentist elastic net
  estimator and corresponding conditional prior are now:
  \begin{subequations}\label{eq:multpart}
	  \begin{align}
	    \hat{\bbeta} &:= \underset{\bbeta}{\argmax} \ell(\y ; \bbeta) -  
	    \frac{\lambda_1}{2} \sum_{g_1=1}^{G_1} \sum_{g_2=1}^{G_2} 
	    \sqrt{\lambda'_{g_1} \lambda''_{g_2}} 
	    \sum_{\mathclap{\substack{j \in \G_1(g_1) \\ \cap \G_2(g_2)}}} | \beta_j|
	    - \frac{\lambda_2}{2} \sum_{g_1=1}^{G_1} \sum_{g_2=1}^{G_2} \lambda'_{g_1}
	    \lambda''_{g_2} \sum_{\mathclap{\substack{j \in \G_1(g_1) \\ 
	    \cap \G_2(g_2)}}} \beta_j^2, \\
	    \bbeta | \btau & \sim \prod_{g_1=1}^{G_1} \prod_{g_2=1}^{G_2} 
	    \prod_{\substack{j \in \G_1(g_1) \\ \cap \G_2(g_2)}} 
	    \mathcal{N} \left(0,\frac{1}{ \lambda'_{g_1} \lambda''_{g_2} \lambda_2}
	    \frac{\tau_j - 1}{\tau_j} \right).
	  \end{align}
  \end{subequations}
  After switching to the parametrisation in \cite{friedman_regularization_2010},
  the new penalty multiplier estimates $\blambda'= \begin{bmatrix} \lambda'_1 &
  \cdots & \lambda'_{G_1} \end{bmatrix} \tr, \blambda''= \begin{bmatrix}
  \lambda''_1 & \cdots & \lambda''_{G_2} \end{bmatrix} \tr$ are given by:
  \begin{subequations}\label{eq:estmultpart}
	  \begin{alignat}{2}
	    \blambda'^{(k+1)}, \blambda''^{(k+1)} &:= \underset{\blambda', \blambda''}
	    {\argmax} && \Bigg\{ \frac{1}{2} \sum_{g_1=1}^{G_1} |\G_1(g_1)| 
	    \log (\lambda'_{g_1}) + \frac{1}{2} \sum_{g_2=1}^{G_2} |\G_2(g_2)| 
	    \log (\lambda''_{g_2}) \\ & && - \frac{(1 - \alpha) \lambda}{4} 
	    \sum_{g_1=1}^{G_1} \sum_{g_2=1}^{G_2} \lambda'_{g_1} \lambda''_{g_2} 
	    d^{(k)}_{g_1 g_2} \Bigg\} \nonumber \\       
	    & \text{subject to } && \prod_{g_1=1}^{G_1} \prod_{g_2=1}^{G_2} 
	    (\lambda'_{g_1} \lambda''_{g_2})^{|\G_1(g_1) \cap \G_2(g_2)|} = 1,
	  \end{alignat}
  \end{subequations}
  with the $d_{g_1 g_2}$ terms calculated as $\sum_{\substack{j \in \G_1(g_1) \\
  \cap \G_2(g_2)}} \[\bSigma^{(k)}_{jj} + (\bmu^{(k)}_j)^2\]\(1 + \alpha
  \lambda^{1.5} \sqrt{\frac{1 - \alpha}{8 \chi_j^{(k)}}}\)$. The optimisation in
  (\ref{eq:estmultpart}) is again a convex problem that is easily solved by some
  numerical optimisation routine. This method naturally generalises to more than
  two partitions of the data.

  In (\ref{eq:multpart}) we model the two partition-specific penalty multipliers
  in a multiplicative way, mainly for convenience. First, there are 
  computational reasons: The separation of the square root in the $L_1$-norm
  penalty term into two terms facilitates the numerical estimation. 
  Additionally, multiplicative modelling of the partitions allows for more
  flexibility in the estimates. To see this consider the obvious alternative:
  additive partition-specific multipliers. Since the penalty multipliers are
  strictly positive, the constraint on the geometric mean makes one large
  additive penalty estimate difficult to compensate with a smaller estimate,
  thereby impairing flexibility of the model. Large multiplicative penalty
  multipliers are easier to compensate for by smaller penalty multipliers,
  rendering this the less restrictive option for modelling the partitions.

  \section{Simulations}\label{sec:simulations}
  The quality of feature selection is assessed by Cohen's kappa 
  \cite[]{cohen_coefficient_1960}. This kappa is measures similarity between two
  sets of categorical values. One set contains the feature selection indicators:
  $s(j) := \mathbbm{1}\{\hat{\beta}_j \neq 0\}$, while the other set contains
  the true non-zero feature indicators: $t(j) := \mathbbm{1}\{\beta_j \neq 0\}$.
  We calculate Cohen's $\kappa$ as:
	\begin{align*}
	  \kappa & = \frac{f_o - f_e}{1 - f_e}, \text{ where } f_o = p^{-1}
	  \sum_{j=1}^p \mathbbm{1}\{s(j)=t(j)\} \\ & \text{ and } f_e = p^{-2} 
	  \left\{ \(\sum_{j=1}^p s(j)\) \cdot \(\sum_{j=1}^p t(j)\) + 
	  \[\sum_{j=1}^p (1 - s(j)) \] \cdot \[\sum_{j=1}^p (1 - t(j)) \]\right\}.
	\end{align*}
	Here, $f_o$ and $f_e$ denote the frequencies of correctly identified and
	expected correctly identified features, respectively. A positive or negative
	kappa indicate better and worse feature selection than expected by chance,
	respectively. In addition, we evaluate estimation accuracy by mean squared
	error: $\text{MSE} = p^{-1} \sum_{j=1}^p (\beta_j - \hat{\beta}_j)^2$. 
	
  \subsection{Scenario 1}
	In this sparse scenario we assume differential signal between 5 groups of
	features. The average signal in a group increases by a factor 1.6 with group
	number:
	\begin{align*}
	  \bbeta & =  \begin{bmatrix} \smash[b]{\underbrace{ \begin{matrix} 
	  \mathbf{0} & \cdots & \mathbf{0} \end{matrix}}_{150}} 
	  & \smash[b]{\underbrace{ \begin{matrix} \beta_{1}^1 & \cdots & 
	  \beta_{50}^1 \end{matrix}}_{50}} & \cdots & 
	  \smash[b]{\underbrace{ \begin{matrix} \mathbf{0} & \cdots & \mathbf{0}
	  \end{matrix}}_{150}} & \smash[b]{\underbrace{ \begin{matrix} 
	  \beta_{1}^5 & \cdots & \beta^5_{50} \end{matrix}}_{50}} \end{bmatrix} \tr,
	  \\
	  & \, \\
	  \beta_{j}^g & \sim \mathcal{U} (0, b_1 \cdot1.6^{g-1}) \text{, } g=1, \dots,
	  5  \text{, } j=1, \dots, 50,
	\end{align*}
	where $\mathcal{U} (a, b)$ denotes the uniform distribution on $(a,b)$. We set
	$b_1 \approx 0.18$ such that the mean expected signal 
	$\overline{\E(\beta_j^g)}=0.07$. Furthermore, we
	introduce correlation between the features by setting the covariances
	$\bSigma_{ij} = 0.5^{|i-j|}$.The
	model parameters clearly do not follow the relatively heavy-tailed elastic net
	distribution. Because of this mismatch between the assumed and true 
	distributions, we expect poor performance by \texttt{gren}. 
	
	\subsection{Scenario 2}
	For this scenario we loosely follow the simulation setup in 
	\cite{meier_group_2008}. We create $G=300$ groups of features and $p=900$
	features in total, such that we
	have 3 features per group. There are 50 active groups, with 2 active 
	parameters with the same value and one inactive parameter per group, such 
	that in total we have 100 non-zero model parameters. The active model 
	parameters are fixed and decrease linearly from $30/51$ to $3/510$, such that
	the mean parameter $\overline{\beta}_j=1/30$. In this example, we included
	an intercept; $\beta_0=10$. 
	
	The predictors are sampled in such a way that they represent dummy variables
	for categorical variables. We start with sampling 300 latent variables:
	$\mathbf{z} \sim \mathcal{N} (\mathbf{0}, \bSigma)$, with 
	$\bSigma_{ij} = 0.5^{|i-j|}$, $i,j=1, \dots, G$. These latent latent 
	variables are then transformed to dummies by:
	$$
	x_{ij} = 
	\begin{cases}
	  1 & \text{if } z_g \in (q_i, q_{i+1}), \\
	  0 & \text{otherwise},
	\end{cases}
	$$
	where the $q_i$ are the $0$, $1/3$, $2/3$, and $1$th quantiles of the normal
	distribution.
	
	The second scenario contains many groups of features and is geared towards the
	group lasso and its extensions. These methods are able to incorporate
	the grouping of the features, and were originally designed for small 
	interpretable groups, such as dummies of a categorical variable. \texttt{gren}
	on the other hand, relies on large groups of features to accurately estimate
	group-specific penalty weights. The estimation of many penalty weights in
	small groups is inefficient and computationally challenging. We therefore
	expect \texttt{gren} to under-perform compared to the other methods.
	
	\subsection{Scenario 3}
	In this scenario we sample the 1000 model parameters in $G=5$ groups of 200.
	For each group we generate the parameters in two steps. First we draw 200
	parameters from the elastic net distribution, parametrised as in 
	\cite{friedman_regularization_2010}, where $\alpha=0.5$ and $\lambda=100$.
	The second step consists of setting the 
	(in absolute value) smallest 100 parameters in a group to zero. This results
	in a total of 500 zero and 500 non-zero parameters, evenly distributed over
	the five groups. We sample the features from a multivariate normal 
	distribution with all variances 1 and all covariances 0.7.
	
	This third scenario is modelled by considering a situation in which there is no
	differential signal between the groups, but the correlation within groups is
	very high. We conjecture that this leads to unstable estimation of the 
	penalty multipliers by \texttt{gren} and consequently, performance will suffer.
	
	\subsection{Scenario 4}
	This scenario is similar to Scenario 3, except that we apply differential 
	shrinkage over the groups with penalty multipliers: 
	$\lambda' \in \{ 0.14, 0.51, 1.95, 7.39 \}$. 
	Furthermore, we change the covariance matrix of the predictors to a block
	diagonal, with block sizes $25
	\times 25$, such that each group of 200 features consists of 8 blocks of 
	correlated features. The covariance off-diagonals and diagonal in each block 
	are set to $\rho=0.7$ and $\sigma^2 = 1$, respectively.
	We expect \texttt{gren} to pick up this differential signal and outperform the
	other methods. 
	
	\subsection{Scenario 5}
	In this last scenario, we create $G=10$ groups of 100 features.
	Of the 10 groups 8 do not contain any signal. In the two active groups, we set 
	85 model parameters to zero and 15 to 0.2 and 0.5, respectively, such that the
	mean signal $\overline{\beta}_j=21/2000$. The features are simulated from the
	same distribution as in Scenario 1.
	
	This sparse truth with differential signal between the groups is expected to
	yield a somewhat better performance by \texttt{gren} than the regular elastic
	net and \texttt{GRridge}. The group lasso extensions are expected to give a 
	similar performance.
	
	\subsection{Additional results}
	Additional results to MD Section \ref{md-sec:simulations} are given in 
	Figures \ref{fig:lines2_simulations_res1_auc_briers}-
	\ref{fig:lines2_simulations_res5_auc_briers}.
	
<<lines2_simulations_res1_auc_briers, fig.cap="Estimated (a) AUC and (b) Brier skill score in simulation setting 1.", out.width="100%", fig.asp=1/2>>=
@

<<lines2_simulations_res2_auc_briers, fig.cap="Estimated (a) AUC and (b) Brier skill score in simulation setting 2.", out.width="100%", fig.asp=1/2>>=
@

<<lines2_simulations_res3_auc_briers, fig.cap="Estimated (a) AUC and (b) Brier skill score in simulation setting 3.", out.width="100%", fig.asp=1/2>>=
@

<<lines2_simulations_res4_auc_briers, fig.cap="Estimated (a) AUC and (b) Brier skill score in simulation setting 4.", out.width="100%", fig.asp=1/2>>=
@

<<lines2_simulations_res5_auc_briers, fig.cap="Estimated (a) AUC and (b) Brier skill score in simulation setting 5.", out.width="100%", fig.asp=1/2>>=
@

  \section{Application to MicroRNAs in colorectal cancer}\label{sec:colorectal}
  \subsection{Additional comparisons}
  We compared \texttt{gren} to two methods in addition to the ones in the MD
  Section \ref{md-sec:colorectal}: 
  the sparse group lasso (\texttt{SGL}) by \cite{simon_sparse-group_2013} and 
  the group exponential lasso (\texttt{gel}) by \cite{breheny_group_2015}. For
  all methods with a tuning parameter $\alpha$, we set 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$ and present the AUC and Brier skill score 
  on the test set in Figures 
  \ref{fig:lines_micrornaseq_colorectal_cancer_res1_auc}--
  \ref{fig:lines_micrornaseq_colorectal_cancer_res1_briers}.
<<lines_micrornaseq_colorectal_cancer_res1_auc, fig.cap="Estimated AUC for the colorectal cancer example with (a) $\\alpha=0.05$, (b) $\\alpha=0.5$, and (c) $\\alpha=0.95$.", fig.asp=1>>=
@
  
<<lines_micrornaseq_colorectal_cancer_res1_briers, fig.cap="Estimated Brier skill score for the colorectal cancer example with (a) $\\alpha=0.05$, (b) $\\alpha=0.5$, and (c) $\\alpha=0.95$.", out.width="100%", fig.asp=1>>=
@
  
  \subsection{Random groups}\label{sec:randomgroups}
  Considering that the features may be partitioned into many groups, with one 
  parameter per group, we have to be aware of overfitting risks. We investigated
  this using the data introduced in Section \ref{md-sec:colorectal} of the MD, 
  randomly dividing the features into three groups. We fixed the group sizes to 
  the group sizes used in the MD. Under this random 
  partitioning of the features, we expect all penalty multipliers to be 
  estimated as one if no overfitting occurs.
  
<<>>=
res <- read.table("results/micrornaseq_colorectal_cancer_res2.csv", 
                  stringsAsFactors=FALSE)
medians <- round(apply(res, 2, median), 2)
@
  We compared the estimated multipliers to the estimates by \texttt{GRridge}. 
  Since these results depend on one specific randomisation of the groups, we 
  repeated the procedure 100 times and present the results in Figure 
  \ref{fig:boxplots_micrornaseq_colorectal_cancer_res2_multipliers}. 
  From this figure we see that the estimates for \texttt{gren} are relatively 
  close to one.
  The estimates by \texttt{GRridge} show more variation. We found no strong
  indication of bias, with median estimated penalty parameters 
  \Sexpr{medians[c(1:3)]} for
  \texttt{GRridge} and \Sexpr{medians[c(4:6)]}; \Sexpr{medians[c(7:9)]},
  and \Sexpr{medians[c(10:12)]} for \texttt{gren} with 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$, respectively.
<<boxplots_micrornaseq_colorectal_cancer_res2_multipliers, fig.cap="Results for the colorectal cancer example with 100 realisations of 3 random groups of features.", out.width="50%", fig.asp=1>>=
@

<<>>=
res <- read.table("results/micrornaseq_colorectal_cancer_res3.csv", 
                  stringsAsFactors=FALSE)
medians <- round(sapply(c("grridge", paste0("gren", 1:3)), function(s) {
  median(as.matrix(res[, grepl(s, colnames(res))]))}), 2)
@
  We repeated the simulation with ten evenly sized groups. Figure 
  \ref{fig:boxplots_micrornaseq_colorectal_cancer_res3_multipliers} presents 
  the estimated penalty multipliers for 100 random 
  splits of the microRNA features into 10 evenly sized groups for 
  \texttt{GRridge} and \texttt{gren} with $\alpha \in \{ 0.05, 0.5, 0.95 \}$. 
  From the Figure we see that \texttt{GRridge} shows more variation around one 
  then the other models. There is no indication of bias: the median penalty 
  multiplier for 
  \texttt{GRridge} is \Sexpr{medians[1]}, while the median multipliers in
  \texttt{gren} are \Sexpr{medians[c(2:4)]} for 
  $\alpha \in \{ 0.05, 0.5, 0.95 \}$, respectively.
<<boxplots_micrornaseq_colorectal_cancer_res3_multipliers, fig.cap="Results for the colorectal cancer example with 100 realisations of 10 random groups of features, plotted (a) with outliers, and (b) without outliers.", out.width="100%", fig.asp=1/2>>=
@

  \subsection{Stability of selection}
  One common criticism on the elastic net, and especially the lasso, is the 
  instability of feature selection. That is, different data sets yield different
  sets of selected features. To investigate the stability of feature selection 
  by \texttt{gren} we created 50 stratified bootstrap samples from the microRNA 
  data \cite[]{neerincx_combination_2018}, where we stratified by the target of 
  prediction, treatment response. 
  For each of these bootstrap samples we estimated the regular elastic
  net and \texttt{gren} for $\alpha \in \{0.05, 0.5, 0.95 \}$, where we selected
  25 features (including the 5 unpenalized covariates). We calculated the size 
  of all $\genfrac(){0pt}{2}{50}{2}$ intersections of selected features and
  present the results in Figure 
  \ref{fig:hist_micrornaseq_colorectal_cancer_res4_overlap}.
<<hist_micrornaseq_colorectal_cancer_res4_overlap, fig.cap="Stability of feature selection in colorectal cancer example for (a) $\\alpha=0.05$, (b) $\\alpha=0.5$, and (c) $\\alpha=0.95$.", out.width="100%", fig.asp=2/3>>=
@
  
  \section{Application to RNAseq in oral cancer}
  \label{sec:oral}
  In addition to the results presented in Section \ref{md-sec:oral} of the 
  MD, we compared \texttt{gren} to the sparse group lasso (\texttt{SGL}) by 
  \cite{simon_sparse-group_2013} and the group exponential lasso (\texttt{gel}) 
  by \cite{breheny_group_2015}. The original group lasso methods are not 
  designed to deal with overlapping groups, so in addition to using the versions
  that allow for overlapping groups, we 
  cross-tabulated the two co-data partitions to create one paritioning with 25 
  groups for \texttt{cMCP} and \texttt{gel}.
  We use $\alpha \in \{0.05, 0.5, 0.95 \}$ for the
  methods that require this extra tuning parameter. The AUCs and Brier skill 
  scores calculated on the
  validation set are presented in Figures 
  \ref{fig:lines_rnaseq_oral_cancer_res1_auc}--
  \ref{fig:lines_rnaseq_oral_cancer_res1_briers}
<<lines_rnaseq_oral_cancer_res1_auc, fig.cap="Estimated AUC for the oral cancer example with (a) $\\alpha=0.05$, (b) $\\alpha=0.5$, and (c) $\\alpha=0.95$.", out.width="100%", fig.asp=1>>=
@
  
<<lines_rnaseq_oral_cancer_res1_briers, fig.cap="Estimated Briers skill score for the oral cancer example with (a) $\\alpha=0.05$, (b) $\\alpha=0.5$, and (c) $\\alpha=0.95$.", out.width="100%", fig.asp=1>>=
@
  
  \section{Application to metabolomics in alzheimer}\label{sec:alzheimer}
	We applied our method to an Alzheimer's dataset from 
	\cite{de_leeuw_blood-based_2017}. The
	data consist of 87 diagnosed Alzheimer's patients with the $\epsilon$4
	allele of the APOE gene and 87 subjects with subjective un-diagnosed cognitive
	decline, without the $\epsilon$4 allele. The aim of the study was to diagnose 
	subjects with Alzheimer's disease based on their metabolic profile. After 
	pre-processing, 230 metabolite expression levels were obtained. To enhance
	classification performance, we included two sources of co-data: (a) the 
	relative standard devitation of the measurements (RSD), a commonly 
	used quality score (smaller is better) for metabolites, binned into five 
	groups, and (b) the node degree of the metabolites, based on a differential 
	network analysis of the metabolites, binned into three categories: 0 degree, 
	positive, lower than average degree, and postive, higher than average degree. 
	We expect a lower RSD and a higher node degree to be indicative of an 
	important metabolite and therefore receive less penalty.
	
	To estimate performance measures AUC and Briers skill score, we split the data
	into 120 training instances and 54 test instances. 
	The training data is used to fit the models, while the test data
	is used to estimate the performance measures on. This resulted in the 
	estimated multipliers and performance measures presented in Figures 
	\ref{fig:barplot_metabolomics_alzheimer_res1}--
	\ref{fig:lines_metabolomics_alzheimer_res1_briers}.
<<barplot_metabolomics_alzheimer_res1, fig.cap="Estimated penalty multiplies for the (a) quality score and the (b) network degree in the alzheimer example.", out.width="100%", fig.asp=1/2>>=
@

<<lines_metabolomics_alzheimer_res1_auc, fig.cap="Estimated AUC for the alzheimer example.", out.width="100%", fig.asp=1>>=
@

<<lines_metabolomics_alzheimer_res1_briers, fig.cap="Estimated Brier skill score for the alzheimer example.", out.width="100%", fig.asp=1>>=
@

  In this example, \texttt{GRridge} and \texttt{gren} underperform due to:
  (i) a small number of features to learn the penalty parameters from and (ii)
  Large negative correlations between the metabolites (see Figure 
  \ref{fig:heatmap_metabolomics_alzheimer_res1_cor}). Both (i) and (ii) make 
  estimation of the penalty multipliers difficult. The other 
  methods estimate one global penalty parameter, so do not suffer from the 
  lower dimensionality of the problem and the large negative correlation.
  
<<heatmap_metabolomics_alzheimer_res1_cor, fig.cap="Correlations between the metabolites.", out.width="50%", fig.asp=1>>=
library(Biobase)
library(lattice)
load("data/ESetMbolCSFPR2.Rdata")
pheno <- pData(ESetMbolCSFPR2)
metabol <- t(exprs(ESetMbolCSFPR2))
metabol.apoe <- metabol[(pheno$D_diag_name=="Probable AD" & 
                           pheno$APOE=="E4YES") |
                          (pheno$D_diag_name=="Subjectieve klachten" & 
                             pheno$APOE=="E4NO"), ]
cor.mat <- cor(scale(metabol.apoe))
levelplot(cor.mat, xlab=NULL, ylab=NULL, scales=list(draw=FALSE))
@
  
  \section{Application to microRNAs in cervical cancer}\label{sec:cervical}
  A deep sequencing analysis on small non-coding ribonucleic acid (microRNAseq) 
  was performed on 24 women with high-grade cervical intraepithelial 
  neoplasia (CIN3) and 32 healthy women for the purpose of finding relevant 
  screening markers for cervical cancer screening. The next generation 
  sequencing analysis resulted in 2576 transcripts. The data was normalized 
  and pre-processed, rendering 752 transcripts. More details of the
  data sets and the pre-processing steps are found in the supplementary   
  material of \cite{novianti_better_2017}.
  
  The 752 microRNAs were divided in three classes based on their conservation 
  status. Conservation status refers to whether the microRNAs are only found in
  humans or are conserved across different species. They come in three classes:
  (a) non-conserved (535), (b) conserved in mammals (70), and (c) 
  conserved across most vertebrates (147). We expect (c) to contain the most 
  important microRNAs, followed by (b) and (a), because these microRNAs have not
  been lost during evolution and might therefore regulate important processes.
  
  The estimated multipliers, AUC, and Brier skill score
  are in Figures \ref{fig:barplot_micrornaseq_cervical_cancer_res1}--
  \ref{fig:lines_micrornaseq_cervical_cancer_res1_briers}, respectively. In this
  example the group regularized methods outperform the regular methods with
  respect to discrimination (AUC) and calibration (Brier skill score).

<<barplot_micrornaseq_cervical_cancer_res1, fig.cap="Estimated penalty multiplies for conservation status in cervical cancer example.", out.width="50%", fig.asp=1>>=
@

<<lines_micrornaseq_cervical_cancer_res1_auc, fig.cap="Cross-validated AUC in cervical cancer example", out.width="100%", fig.asp=1>>=
@

<<lines_micrornaseq_cervical_cancer_res1_briers, fig.cap="Cross-validated Brier skill score in cervical cancer example", out.width="100%", fig.asp=1>>=
@
  
  \section{Empirical-variational Bayes for the ridge and lasso}
  \label{sec:evbridgelasso}
  \subsection{Ridge regression}
  Ridge regression is a special case of the elastic net, that does not include
  an $L_1$-norm on the parameters. Its estimator is given by:
  $$
  \hat{\bbeta} := \underset{\bbeta}{\argmax} \ell(\y ; \bbeta) - 
  \frac{\lambda_2}{2} \norm{\bbeta}_2.
  $$
  The Bayesian version of ridge regression is regression under a Gaussian prior.
  The generalised version of this prior is:
  $$
  \bbeta \sim \prod_{g=1}^G \prod_{j \in \G(g)} 
  \N(0,(\lambda_2 \lambda'_g)^{-1}).
  $$
  Although the posterior is analytically calculated in the linear case, in the
  logistic case we must approximate it iteratively. The variational 
  distributions in this case are:
  \begin{align*}
    q^*_{\bbeta} (\bbeta) & \sim \mathcal{N} (\bmu, \bSigma), \\ 
    q^*_{\bomega} (\bomega) & \sim \prod_{i=1}^n \mathcal{PG} (m_i, c_i). \\
  \end{align*}
  The parameters $c_i$ and $\bmu$ remain unchanged compared to the elastic net
  and are updated, together with $\bSigma$, until convergence:
  $$
  \bSigma^{t+1} = (\X \tr \bOmega^{(t)} \X + \lambda_2 \bLambda')^{-1}
  \text{, with } \bOmega^{(t)} = \diag \[ \frac{m_i}{2 c^{(t)}_i} 
  \tanh \( \frac{c^{(t)}_i}{2} \) \].
  $$

  Updating the penalty parameters is done by solving the following (convex)
  constraint optimisation problem:
  \begin{align*}
    \blambda'^{(k+1)} & = \underset{\blambda'}{\argmax} \frac{1}{2} \sum_{g=1}^G
    |\G(g)| \log (\lambda'_g) - \frac{\lambda_2}{2} \sum_{g=1}^G 
    \lambda'_g d^{(k)}_g \\
    & \text{subject to } \prod_{g=1}^G (\lambda'_g)^{|\G(g)|} = 1,
  \end{align*}
  with the terms $d^{(k)}_g$ given by:
  $$
  d^{(k)}_g = \sum_{j \in \G(g)} ((\bmu^{(k)}_j)^2 + \bSigma^{(k)}_{jj}).
  $$ 

  \subsection{Lasso regression}
  In lasso regression, another special case of the elastic net, we have the
  following estimator:
  $$
  \hat{\bbeta} := \underset{\bbeta}{\argmax} \ell(\y ; \bbeta) - 
  \frac{\lambda_1}{2} \norm{\bbeta}_1.
  $$
  The Bayesian counterpart is regression under a Laplace prior on the model
  parameters. The generalised Bayesian prior may decomposed as:
  \begin{align*}
    \bbeta | \btau & \sim \prod_{j=1}^p \mathcal{N} \left(0,\tau_j \right), \\
    \btau & \sim \prod_{g=1}^G \prod_{j \in \G(g)} \text{Exp} 
    \left( \frac{\sqrt{\lambda'_g}\lambda_1^2}{8} \right).
  \end{align*}

  The variational distributions are similar as in the elastic net, except that
  we do not re-parametrise the $\tau_j$:
  \begin{align*}
    q^*_{\bbeta} (\bbeta) & \sim \mathcal{N} (\bmu, \bSigma), \\ 
    q^*_{\bomega} (\bomega) & \sim \prod_{i=1}^n \mathcal{PG} (m_i, c_i), \\
    q^*_{\btau}(\btau) & \sim \prod_{g=1}^G \prod_{j \in \G(g)} 
    \mathcal{GIG} \(\frac{1}{2}, \phi_g, \chi_j\).
  \end{align*}
  The parameters $c_i$ and $\bmu$ and remain unchanged compared to the elastic
  net, while the $\phi_g$ are given by:
  $$
  \phi_g = \lambda_{g} \frac{\lambda_1^2}{4}.
  $$
  The following parameters are updated, together with the unchanged $c_i$ and
  $\bmu$, until convergence:
  \begin{align*}
    \bSigma^{(t + 1)} &= ( \X \tr \bOmega^{(t)} \X + \Z^{(t)})^{-1}, \\ 
    & \text{with } \bOmega^{(t)} = \diag\[ \(\frac{m_i}{2 c_i^{(t)}}\) 
    \tanh \( \frac{c_i^{(t)}}{2} \)\] \text{ and } 
    \Z^{(t)}=\diag\(\sqrt{\frac{\phi_g(j)}{\chi_j^{(t)}}}\), \\
    \chi_j^{(t + 1)} &= \bSigma^{(t + 1)}_{jj} + (\bmu^{(t + 1)}_j)^2 
    \text{, for } j=1, \dots, p.
  \end{align*}

  Updating the penalty parameters is done by solving the following (convex)
  constraint problem:
  \begin{align*}
    \blambda'^{(k+1)} & = \underset{\blambda'}{\argmax} \sum_{g=1}^G |\G(g)|
    \log (\lambda'_g) - \frac{\lambda_1^2}{8} \sum_{g=1}^G 
    \lambda'_g d^{(k)}_g \\
    & \text{subject to } \prod_{g=1}^G (\lambda'_g)^{|\G(g)|} = 1,
  \end{align*}
  with the terms $d^{(k)}_g$ given by:
  $$
  d^{(k)}_g = \sum_{j \in \G(g)} \( \frac{1}{\phi_g^{(k)}} + 
  \frac{\chi^{(k)}_j}{\phi_g^{(k)}} \).
  $$ 
  Note that the $\phi_g$ are updated after every penalty multiplier update  
  iteration as well, in contrast to the elastic net.

  % \section{Gibbs sampler}
  %   
  % \begin{align*}
  %   \bbeta | \bomega, \bpsi & \sim \mathcal{N} (\bSigma, \bmu) \\
  %     & \text{with } \bSigma = [\X \tr \diag (\bomega_i) \X + 
  %     \lambda_2 \bLambda' + \lambda_2 \bLambda' \diag (\psi_j^{-1})]^{-1} 
  %     \text{ and } \bmu = \bSigma \X \tr (\y - \frac{1}{2} \mathbf{m}), \\
  %   \omega_i | \bbeta & \sim \mathcal{PG} (m_i, |\x_i \tr \bbeta|), \\
  %   \psi_j^{-1} | \beta_j & \sim \mathcal{IG} (\lambda_1/ (2 \lambda_2 
  %   \sqrt{\lambda'_{g(j)}} |\beta_j|), \lambda_1^2/(4 \lambda_2)).
  % \end{align*}
    
  \bibliographystyle{author_short3.bst} 
  \bibliography{refs}

  \section*{Session info}
<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}




