\documentclass{letter}

\newenvironment{itquote}
{\begin{quote}\itshape}
{\end{quote}}

\signature{Magnus M\"unch}
\begin{document}
  \begin{letter}{}
    \opening{Dear editor,}
    
    Thank you for the opportunity to resubmit our manuscript. We realise it has
    taken us an extroardinarily long time to respond, but given the extensive 
    nature of the revision, we hope that you will still consider this 
    resubmission.
    
    Below we formulate our response to your and the reviewers' comments
    point by point. We hope that these responses and the accompanying changes in
    the manuscript and supplementary material address the raised issues, such 
    that the manuscript is deemed appropriate for publication in Biostatistics.
    
    \textbf{Reply to editor}
    \begin{itquote}
      I would be willing to consider a revision of your work if you 
      made a substantial effort to rework the simulation examples and practical 
      case studies to demonstrate clearly unique cases where your method works 
      and others don't (as well as the reverse case!).
    \end{itquote} 
    The simulations were extended to five different scenarios. In three
    scenarios, we expect all methods to underperform. In one scenario, we expect
    \texttt{gren} to underperform compared to its competitors and in one 
    scenario we expect \texttt{gren} to outperform its competitors. We have 
    included 4/5 applications in the manuscript and supplementary material. In
    one application \texttt{gren} underperforms compared to the others. We argue
    as for why this happens and deduce certain scenarios in which \texttt{gren}
    should not be used.

    \begin{itquote}
      This comparison should include, not just the EN and Grouped 
      Lasso, but other methods that approach this same problem as well. There 
      would need to be clear empirical demonstration on real data of a unique 
      advantage of this approach for it to be a good fit at Biostatistics.
    \end{itquote}
    We included comparisons to the sparse group lasso (\texttt{SGL}) by 
    Simon et al. (2013), the group mimimax concave penalty
    (\texttt{grMCP}) by Breheny and Huang (2009), and the group exponential
    lasso (\texttt{gel}) by Breheny (2015). \texttt{gren} outperforms these in 
    a number of comparisons. Our approach is based on a different philosophy
    than the group lasso and its extensions. Group weights are adaptively 
    estimated from the data and not based on one common penalty, as is the case
    in the group lasso and its extensions.

    \textbf{Reply to reviewer 1}
    \begin{itquote}
      However, in my opinion the scientific contribution and novelty 
      of the methodology is just incremental. Elastic net itself is designed to 
      do the grouping. So, I am not sure why putting group penalty elastic net
      will be better than other group penalty methods like group lasso
    \end{itquote}
    The grouping property of elastic net refers to a theoretical advantage over
    the lasso in dealing with correlated features. It does not refer to the
    explicit inclusion of external information in the form of groups of 
    features and is therefore unrelated to the topic of the manuscript.
    
    \begin{itquote}
      Also Bayesian logistic regression can be formulated based on 
      Polya-Gamma latent variables (Polson et al.) and that need to be 
      incorporated in your modeling strategy.
    \end{itquote}
    The Polya-Gamma parametrization is actually an important part of our method.
    It is introduced in equation (6) in the Section 3.2 and extensively covered 
    in Section 4 of the supplementary material.

    \begin{itquote}
      Why no comparison between the proposed method and at least the 
      group lasso which assigns the group penalty.
    \end{itquote}
    We included comparisons to the group lasso and several of its extensions in
    the simulations and data applications.
    
    \begin{itquote}
      The model contains several tuning/prior parameters. Sensitivity 
      analysis need to be provided for that.
    \end{itquote}
    We show in one of our applications that randomly assigning the features
    to the groups does not lead to bias in the tuning parameter estimates, 
    suggesting that the method is robust (at least in this application).
      
    \begin{itquote}
      How much your model is scalable? EM is notoriously slow.
    \end{itquote}
    We included a comparison to other methods in terms of computation time on 
    one of the applications in the discussion. The comparison shows that our 
    method is competitive and outperforms some commonly used methods in terms of
    computation time.
    
    \begin{itquote}
      The authors have also missed several relevant references and 
      should put some more focus on the literature review.
    \end{itquote}
    We have included one of the suggested references in the introduction. 
    However, we feel that the other suggested references are not relevant and
    have thus not included them.

    \textbf{Reply to reviewer 2}
    \begin{itquote}
      This paper considered the use of group elastic net method. 
      Compared to Meier et al (2008, JRSS B) who used group lasso penalty, you 
      proposed to use the group elastic net. This extension is fine, but the 
      novelty is rather little in this sense.
    \end{itquote}
    The group lasso is built on an eniterely different philosophy.
    Estimation is very different and not adaptive. Group lasso was designed for
    small interpretable groups of features, such as dummy variables. Our method
    is designed for larger groups of features, where the grouping is based on 
    external information that may informative for the prediction problem at 
    hand. The group lasso has one penalty parameter for all groups and estimates
    it by cross-validation. Our method uses a different penalty parameter per 
    group of features and estimates them by empirical Bayes.
      
    \begin{itquote}
      In (2.2), the penalization parameters $\lambda_1$ , $\lambda_2$ 
      and $\lambda_g$ appears in the forms of $\lambda_1 \sqrt{\lambda_g'}$ and 
      $\lambda_2 \lambda_g'$. A question is, is there is an identifiability 
      issue here? If not, why not?
    \end{itquote}
    There is not, since we fix $\lambda_1$ and $\lambda_2$ beforehand through 
    cross-validation. This leaves $G$ penalties to estimate for $G$ groups with
    empirical Bayes, where each group consists of many features.
    
    \begin{itquote}
      The computational load and time: As commented in the Discussion,
      the proposed method uses the double EM loop, which not only increases the
      chance of ending up in a local optimum but also can be very time consuming
      in computation. It would be helpful by commenting on the computation time,
      for example, in the real data analysis.
    \end{itquote}
    We included comparisons in terms of computation time in the discussion that 
    show that our method is competitive in terms of computation time. In 
    addition, we investigated different starting values for our algorithm in the
    applications presented in the manuscript and supplementary material. In none
    of the applications did this lead to different optimisation results.

    \closing{Yours Faithfully,}
    \encl{manuscript.pdf, supplement.pdf}

  \end{letter}
\end{document}